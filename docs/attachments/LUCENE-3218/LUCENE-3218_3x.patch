
Property changes on: lucene
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/trunk/lucene:r1138063

Index: lucene/CHANGES.txt
===================================================================
--- lucene/CHANGES.txt	(revision 1138332)
+++ lucene/CHANGES.txt	(working copy)
@@ -90,6 +90,13 @@
 
 * LUCENE-1736: DateTools.java general improvements. 
   (David Smiley via Steve Rowe)
+  
+* LUCENE-3201, LUCENE-3218: CompoundFileSystem code has been consolidated 
+  into a Directory implementation. Reading is optimized for MMapDirectory,
+  NIOFSDirectory and SimpleFSDirectory to only map requested parts of the
+  CFS into an IndexInput. Writing to a CFS now tries to append to the CF
+  directly if possible and merges separately written files on the fly instead
+  of during close. (Simon Willnauer, Robert Muir)
 
 New Features
 

Property changes on: lucene/backwards
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/trunk/lucene/backwards:r1138063


Property changes on: lucene/backwards/src/test
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/trunk/lucene/backwards/src/test:r1138063


Property changes on: lucene/backwards/src/test-framework
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/trunk/lucene/backwards/src/test-framework:r1138063

Index: lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1138332)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -954,7 +954,7 @@
     w3.addIndexes(readers);
     w3.close();
     
-    assertEquals("Only one compound segment should exist", 3, dir.listAll().length);
+    assertEquals("Only one compound segment should exist", 4, dir.listAll().length);
   }
  
   // LUCENE-2996: tests that addIndexes(IndexReader) applies existing deletes correctly.
Index: lucene/backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 1138332)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -526,87 +526,6 @@
     return indexDir;
   }
 
-  /* Verifies that the expected file names were produced */
-
-  public void testExactFileNames() throws IOException {
-
-    String outputDirName = "lucene.backwardscompat0.index";
-    File outputDir = _TestUtil.getTempDir(outputDirName);
-    _TestUtil.rmDir(outputDir);
-
-    try {
-      Directory dir = newFSDirectory(outputDir);
-
-      LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);
-      mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS
-      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(-1).setRAMBufferSizeMB(16.0)
-        .setMergePolicy(mergePolicy);
-      IndexWriter writer = new IndexWriter(dir, conf);
-      for(int i=0;i<35;i++) {
-        addDoc(writer, i);
-      }
-      assertEquals("wrong doc count", 35, writer.maxDoc());
-      writer.close();
-
-      // Delete one doc so we get a .del file:
-      IndexReader reader = IndexReader.open(dir, false);
-      Term searchTerm = new Term("id", "7");
-      int delCount = reader.deleteDocuments(searchTerm);
-      assertEquals("didn't delete the right number of documents", 1, delCount);
-
-      // Set one norm so we get a .s0 file:
-      reader.setNorm(21, "content", (float) 1.5);
-      reader.close();
-
-      // The numbering of fields can vary depending on which
-      // JRE is in use.  On some JREs we see content bound to
-      // field 0; on others, field 1.  So, here we have to
-      // figure out which field number corresponds to
-      // "content", and then set our expected file names below
-      // accordingly:
-      CompoundFileReader cfsReader = new CompoundFileReader(dir, "_0.cfs");
-      FieldInfos fieldInfos = new FieldInfos(cfsReader, "_0.fnm");
-      int contentFieldIndex = -1;
-      for(int i=0;i<fieldInfos.size();i++) {
-        FieldInfo fi = fieldInfos.fieldInfo(i);
-        if (fi.name.equals("content")) {
-          contentFieldIndex = i;
-          break;
-        }
-      }
-      cfsReader.close();
-      assertTrue("could not locate the 'content' field number in the _2.cfs segment", contentFieldIndex != -1);
-
-      // Now verify file names:
-      String[] expected = new String[] {"_0.cfs",
-                               "_0_1.del",
-                               "_0_1.s" + contentFieldIndex,
-                               "segments_2",
-                               "segments.gen"};
-
-      String[] actual = dir.listAll();
-      Arrays.sort(expected);
-      Arrays.sort(actual);
-      if (!Arrays.equals(expected, actual)) {
-        fail("incorrect filenames in index: expected:\n    " + asString(expected) + "\n  actual:\n    " + asString(actual));
-      }
-      dir.close();
-    } finally {
-      _TestUtil.rmDir(outputDir);
-    }
-  }
-
-  private String asString(String[] l) {
-    String s = "";
-    for(int i=0;i<l.length;i++) {
-      if (i > 0) {
-        s += "\n    ";
-      }
-      s += l[i];
-    }
-    return s;
-  }
-
   private void addDoc(IndexWriter writer, int id) throws IOException
   {
     Document doc = new Document();
Index: lucene/backwards/src/test/org/apache/lucene/index/TestCompoundFile.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestCompoundFile.java	(revision 1138332)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestCompoundFile.java	(working copy)
@@ -1,672 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.File;
-
-import org.apache.lucene.util.LuceneTestCase;
-import junit.framework.TestSuite;
-import junit.textui.TestRunner;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.store._TestHelper;
-import org.apache.lucene.util._TestUtil;
-
-
-public class TestCompoundFile extends LuceneTestCase
-{
-    /** Main for running test case by itself. */
-    public static void main(String args[]) {
-        TestRunner.run (new TestSuite(TestCompoundFile.class));
-//        TestRunner.run (new TestCompoundFile("testSingleFile"));
-//        TestRunner.run (new TestCompoundFile("testTwoFiles"));
-//        TestRunner.run (new TestCompoundFile("testRandomFiles"));
-//        TestRunner.run (new TestCompoundFile("testClonedStreamsClosing"));
-//        TestRunner.run (new TestCompoundFile("testReadAfterClose"));
-//        TestRunner.run (new TestCompoundFile("testRandomAccess"));
-//        TestRunner.run (new TestCompoundFile("testRandomAccessClones"));
-//        TestRunner.run (new TestCompoundFile("testFileNotFound"));
-//        TestRunner.run (new TestCompoundFile("testReadPastEOF"));
-
-//        TestRunner.run (new TestCompoundFile("testIWCreate"));
-
-    }
-
-
-    private Directory dir;
-
-
-    @Override
-    public void setUp() throws Exception {
-       super.setUp();
-       File file = _TestUtil.getTempDir("testIndex");
-       // use a simple FSDir here, to be sure to have SimpleFSInputs
-       dir = new SimpleFSDirectory(file,null);
-    }
-
-    @Override
-    public void tearDown() throws Exception {
-       dir.close();
-       super.tearDown();
-    }
-
-    /** Creates a file of the specified size with random data. */
-    private void createRandomFile(Directory dir, String name, int size)
-    throws IOException
-    {
-        IndexOutput os = dir.createOutput(name);
-        for (int i=0; i<size; i++) {
-            byte b = (byte) (Math.random() * 256);
-            os.writeByte(b);
-        }
-        os.close();
-    }
-
-    /** Creates a file of the specified size with sequential data. The first
-     *  byte is written as the start byte provided. All subsequent bytes are
-     *  computed as start + offset where offset is the number of the byte.
-     */
-    private void createSequenceFile(Directory dir,
-                                    String name,
-                                    byte start,
-                                    int size)
-    throws IOException
-    {
-        IndexOutput os = dir.createOutput(name);
-        for (int i=0; i < size; i++) {
-            os.writeByte(start);
-            start ++;
-        }
-        os.close();
-    }
-
-
-    private void assertSameStreams(String msg,
-                                   IndexInput expected,
-                                   IndexInput test)
-    throws IOException
-    {
-        assertNotNull(msg + " null expected", expected);
-        assertNotNull(msg + " null test", test);
-        assertEquals(msg + " length", expected.length(), test.length());
-        assertEquals(msg + " position", expected.getFilePointer(),
-                                        test.getFilePointer());
-
-        byte expectedBuffer[] = new byte[512];
-        byte testBuffer[] = new byte[expectedBuffer.length];
-
-        long remainder = expected.length() - expected.getFilePointer();
-        while(remainder > 0) {
-            int readLen = (int) Math.min(remainder, expectedBuffer.length);
-            expected.readBytes(expectedBuffer, 0, readLen);
-            test.readBytes(testBuffer, 0, readLen);
-            assertEqualArrays(msg + ", remainder " + remainder, expectedBuffer,
-                testBuffer, 0, readLen);
-            remainder -= readLen;
-        }
-    }
-
-
-    private void assertSameStreams(String msg,
-                                   IndexInput expected,
-                                   IndexInput actual,
-                                   long seekTo)
-    throws IOException
-    {
-        if(seekTo >= 0 && seekTo < expected.length())
-        {
-            expected.seek(seekTo);
-            actual.seek(seekTo);
-            assertSameStreams(msg + ", seek(mid)", expected, actual);
-        }
-    }
-
-
-
-    private void assertSameSeekBehavior(String msg,
-                                        IndexInput expected,
-                                        IndexInput actual)
-    throws IOException
-    {
-        // seek to 0
-        long point = 0;
-        assertSameStreams(msg + ", seek(0)", expected, actual, point);
-
-        // seek to middle
-        point = expected.length() / 2l;
-        assertSameStreams(msg + ", seek(mid)", expected, actual, point);
-
-        // seek to end - 2
-        point = expected.length() - 2;
-        assertSameStreams(msg + ", seek(end-2)", expected, actual, point);
-
-        // seek to end - 1
-        point = expected.length() - 1;
-        assertSameStreams(msg + ", seek(end-1)", expected, actual, point);
-
-        // seek to the end
-        point = expected.length();
-        assertSameStreams(msg + ", seek(end)", expected, actual, point);
-
-        // seek past end
-        point = expected.length() + 1;
-        assertSameStreams(msg + ", seek(end+1)", expected, actual, point);
-    }
-
-
-    private void assertEqualArrays(String msg,
-                                   byte[] expected,
-                                   byte[] test,
-                                   int start,
-                                   int len)
-    {
-        assertNotNull(msg + " null expected", expected);
-        assertNotNull(msg + " null test", test);
-
-        for (int i=start; i<len; i++) {
-            assertEquals(msg + " " + i, expected[i], test[i]);
-        }
-    }
-
-
-    // ===========================================================
-    //  Tests of the basic CompoundFile functionality
-    // ===========================================================
-
-
-    /** This test creates compound file based on a single file.
-     *  Files of different sizes are tested: 0, 1, 10, 100 bytes.
-     */
-    public void testSingleFile() throws IOException {
-        int data[] = new int[] { 0, 1, 10, 100 };
-        for (int i=0; i<data.length; i++) {
-            String name = "t" + data[i];
-            createSequenceFile(dir, name, (byte) 0, data[i]);
-            CompoundFileWriter csw = new CompoundFileWriter(dir, name + ".cfs");
-            csw.addFile(name);
-            csw.close();
-
-            CompoundFileReader csr = new CompoundFileReader(dir, name + ".cfs");
-            IndexInput expected = dir.openInput(name);
-            IndexInput actual = csr.openInput(name);
-            assertSameStreams(name, expected, actual);
-            assertSameSeekBehavior(name, expected, actual);
-            expected.close();
-            actual.close();
-            csr.close();
-        }
-    }
-
-
-    /** This test creates compound file based on two files.
-     *
-     */
-    public void testTwoFiles() throws IOException {
-        createSequenceFile(dir, "d1", (byte) 0, 15);
-        createSequenceFile(dir, "d2", (byte) 0, 114);
-
-        CompoundFileWriter csw = new CompoundFileWriter(dir, "d.csf");
-        csw.addFile("d1");
-        csw.addFile("d2");
-        csw.close();
-
-        CompoundFileReader csr = new CompoundFileReader(dir, "d.csf");
-        IndexInput expected = dir.openInput("d1");
-        IndexInput actual = csr.openInput("d1");
-        assertSameStreams("d1", expected, actual);
-        assertSameSeekBehavior("d1", expected, actual);
-        expected.close();
-        actual.close();
-
-        expected = dir.openInput("d2");
-        actual = csr.openInput("d2");
-        assertSameStreams("d2", expected, actual);
-        assertSameSeekBehavior("d2", expected, actual);
-        expected.close();
-        actual.close();
-        csr.close();
-    }
-
-    /** This test creates a compound file based on a large number of files of
-     *  various length. The file content is generated randomly. The sizes range
-     *  from 0 to 1Mb. Some of the sizes are selected to test the buffering
-     *  logic in the file reading code. For this the chunk variable is set to
-     *  the length of the buffer used internally by the compound file logic.
-     */
-    public void testRandomFiles() throws IOException {
-        // Setup the test segment
-        String segment = "test";
-        int chunk = 1024; // internal buffer size used by the stream
-        createRandomFile(dir, segment + ".zero", 0);
-        createRandomFile(dir, segment + ".one", 1);
-        createRandomFile(dir, segment + ".ten", 10);
-        createRandomFile(dir, segment + ".hundred", 100);
-        createRandomFile(dir, segment + ".big1", chunk);
-        createRandomFile(dir, segment + ".big2", chunk - 1);
-        createRandomFile(dir, segment + ".big3", chunk + 1);
-        createRandomFile(dir, segment + ".big4", 3 * chunk);
-        createRandomFile(dir, segment + ".big5", 3 * chunk - 1);
-        createRandomFile(dir, segment + ".big6", 3 * chunk + 1);
-        createRandomFile(dir, segment + ".big7", 1000 * chunk);
-
-        // Setup extraneous files
-        createRandomFile(dir, "onetwothree", 100);
-        createRandomFile(dir, segment + ".notIn", 50);
-        createRandomFile(dir, segment + ".notIn2", 51);
-
-        // Now test
-        CompoundFileWriter csw = new CompoundFileWriter(dir, "test.cfs");
-        final String data[] = new String[] {
-            ".zero", ".one", ".ten", ".hundred", ".big1", ".big2", ".big3",
-            ".big4", ".big5", ".big6", ".big7"
-        };
-        for (int i=0; i<data.length; i++) {
-            csw.addFile(segment + data[i]);
-        }
-        csw.close();
-
-        CompoundFileReader csr = new CompoundFileReader(dir, "test.cfs");
-        for (int i=0; i<data.length; i++) {
-            IndexInput check = dir.openInput(segment + data[i]);
-            IndexInput test = csr.openInput(segment + data[i]);
-            assertSameStreams(data[i], check, test);
-            assertSameSeekBehavior(data[i], check, test);
-            test.close();
-            check.close();
-        }
-        csr.close();
-    }
-
-
-    /** Setup a larger compound file with a number of components, each of
-     *  which is a sequential file (so that we can easily tell that we are
-     *  reading in the right byte). The methods sets up 20 files - f0 to f19,
-     *  the size of each file is 1000 bytes.
-     */
-    private void setUp_2() throws IOException {
-        CompoundFileWriter cw = new CompoundFileWriter(dir, "f.comp");
-        for (int i=0; i<20; i++) {
-            createSequenceFile(dir, "f" + i, (byte) 0, 2000);
-            cw.addFile("f" + i);
-        }
-        cw.close();
-    }
-
-
-    public void testReadAfterClose() throws IOException {
-        demo_FSIndexInputBug(dir, "test");
-    }
-
-    private void demo_FSIndexInputBug(Directory fsdir, String file)
-    throws IOException
-    {
-        // Setup the test file - we need more than 1024 bytes
-        IndexOutput os = fsdir.createOutput(file);
-        for(int i=0; i<2000; i++) {
-            os.writeByte((byte) i);
-        }
-        os.close();
-
-        IndexInput in = fsdir.openInput(file);
-
-        // This read primes the buffer in IndexInput
-        in.readByte();
-
-        // Close the file
-        in.close();
-
-        // ERROR: this call should fail, but succeeds because the buffer
-        // is still filled
-        in.readByte();
-
-        // ERROR: this call should fail, but succeeds for some reason as well
-        in.seek(1099);
-
-        try {
-            // OK: this call correctly fails. We are now past the 1024 internal
-            // buffer, so an actual IO is attempted, which fails
-            in.readByte();
-            fail("expected readByte() to throw exception");
-        } catch (IOException e) {
-          // expected exception
-        }
-    }
-
-
-    static boolean isCSIndexInput(IndexInput is) {
-        return is instanceof CompoundFileReader.CSIndexInput;
-    }
-
-    static boolean isCSIndexInputOpen(IndexInput is) throws IOException {
-        if (isCSIndexInput(is)) {
-            CompoundFileReader.CSIndexInput cis =
-            (CompoundFileReader.CSIndexInput) is;
-
-            return _TestHelper.isSimpleFSIndexInputOpen(cis.base);
-        } else {
-            return false;
-        }
-    }
-
-
-    public void testClonedStreamsClosing() throws IOException {
-        setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
-
-        // basic clone
-        IndexInput expected = dir.openInput("f11");
-
-        // this test only works for FSIndexInput
-        assertTrue(_TestHelper.isSimpleFSIndexInput(expected));
-        assertTrue(_TestHelper.isSimpleFSIndexInputOpen(expected));
-
-        IndexInput one = cr.openInput("f11");
-        assertTrue(isCSIndexInputOpen(one));
-
-        IndexInput two = (IndexInput) one.clone();
-        assertTrue(isCSIndexInputOpen(two));
-
-        assertSameStreams("basic clone one", expected, one);
-        expected.seek(0);
-        assertSameStreams("basic clone two", expected, two);
-
-        // Now close the first stream
-        one.close();
-        assertTrue("Only close when cr is closed", isCSIndexInputOpen(one));
-
-        // The following should really fail since we couldn't expect to
-        // access a file once close has been called on it (regardless of
-        // buffering and/or clone magic)
-        expected.seek(0);
-        two.seek(0);
-        assertSameStreams("basic clone two/2", expected, two);
-
-
-        // Now close the compound reader
-        cr.close();
-        assertFalse("Now closed one", isCSIndexInputOpen(one));
-        assertFalse("Now closed two", isCSIndexInputOpen(two));
-
-        // The following may also fail since the compound stream is closed
-        expected.seek(0);
-        two.seek(0);
-        //assertSameStreams("basic clone two/3", expected, two);
-
-
-        // Now close the second clone
-        two.close();
-        expected.seek(0);
-        two.seek(0);
-        //assertSameStreams("basic clone two/4", expected, two);
-
-        expected.close();
-    }
-
-
-    /** This test opens two files from a compound stream and verifies that
-     *  their file positions are independent of each other.
-     */
-    public void testRandomAccess() throws IOException {
-        setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
-
-        // Open two files
-        IndexInput e1 = dir.openInput("f11");
-        IndexInput e2 = dir.openInput("f3");
-
-        IndexInput a1 = cr.openInput("f11");
-        IndexInput a2 = dir.openInput("f3");
-
-        // Seek the first pair
-        e1.seek(100);
-        a1.seek(100);
-        assertEquals(100, e1.getFilePointer());
-        assertEquals(100, a1.getFilePointer());
-        byte be1 = e1.readByte();
-        byte ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now seek the second pair
-        e2.seek(1027);
-        a2.seek(1027);
-        assertEquals(1027, e2.getFilePointer());
-        assertEquals(1027, a2.getFilePointer());
-        byte be2 = e2.readByte();
-        byte ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Now make sure the first one didn't move
-        assertEquals(101, e1.getFilePointer());
-        assertEquals(101, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now more the first one again, past the buffer length
-        e1.seek(1910);
-        a1.seek(1910);
-        assertEquals(1910, e1.getFilePointer());
-        assertEquals(1910, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now make sure the second set didn't move
-        assertEquals(1028, e2.getFilePointer());
-        assertEquals(1028, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Move the second set back, again cross the buffer size
-        e2.seek(17);
-        a2.seek(17);
-        assertEquals(17, e2.getFilePointer());
-        assertEquals(17, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Finally, make sure the first set didn't move
-        // Now make sure the first one didn't move
-        assertEquals(1911, e1.getFilePointer());
-        assertEquals(1911, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        e1.close();
-        e2.close();
-        a1.close();
-        a2.close();
-        cr.close();
-    }
-
-    /** This test opens two files from a compound stream and verifies that
-     *  their file positions are independent of each other.
-     */
-    public void testRandomAccessClones() throws IOException {
-        setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
-
-        // Open two files
-        IndexInput e1 = cr.openInput("f11");
-        IndexInput e2 = cr.openInput("f3");
-
-        IndexInput a1 = (IndexInput) e1.clone();
-        IndexInput a2 = (IndexInput) e2.clone();
-
-        // Seek the first pair
-        e1.seek(100);
-        a1.seek(100);
-        assertEquals(100, e1.getFilePointer());
-        assertEquals(100, a1.getFilePointer());
-        byte be1 = e1.readByte();
-        byte ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now seek the second pair
-        e2.seek(1027);
-        a2.seek(1027);
-        assertEquals(1027, e2.getFilePointer());
-        assertEquals(1027, a2.getFilePointer());
-        byte be2 = e2.readByte();
-        byte ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Now make sure the first one didn't move
-        assertEquals(101, e1.getFilePointer());
-        assertEquals(101, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now more the first one again, past the buffer length
-        e1.seek(1910);
-        a1.seek(1910);
-        assertEquals(1910, e1.getFilePointer());
-        assertEquals(1910, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now make sure the second set didn't move
-        assertEquals(1028, e2.getFilePointer());
-        assertEquals(1028, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Move the second set back, again cross the buffer size
-        e2.seek(17);
-        a2.seek(17);
-        assertEquals(17, e2.getFilePointer());
-        assertEquals(17, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Finally, make sure the first set didn't move
-        // Now make sure the first one didn't move
-        assertEquals(1911, e1.getFilePointer());
-        assertEquals(1911, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        e1.close();
-        e2.close();
-        a1.close();
-        a2.close();
-        cr.close();
-    }
-
-
-    public void testFileNotFound() throws IOException {
-        setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
-
-        // Open two files
-        try {
-            cr.openInput("bogus");
-            fail("File not found");
-
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: File Not Found: " + e);
-        }
-
-        cr.close();
-    }
-
-
-    public void testReadPastEOF() throws IOException {
-        setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
-        IndexInput is = cr.openInput("f2");
-        is.seek(is.length() - 10);
-        byte b[] = new byte[100];
-        is.readBytes(b, 0, 10);
-
-        try {
-            is.readByte();
-            fail("Single byte read past end of file");
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: single byte read past end of file: " + e);
-        }
-
-        is.seek(is.length() - 10);
-        try {
-            is.readBytes(b, 0, 50);
-            fail("Block read past end of file");
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: block read past end of file: " + e);
-        }
-
-        is.close();
-        cr.close();
-    }
-
-    /** This test that writes larger than the size of the buffer output
-     * will correctly increment the file pointer.
-     */
-    public void testLargeWrites() throws IOException {
-        IndexOutput os = dir.createOutput("testBufferStart.txt");
-
-        byte[] largeBuf = new byte[2048];
-        for (int i=0; i<largeBuf.length; i++) {
-            largeBuf[i] = (byte) (Math.random() * 256);
-        }
-
-        long currentPos = os.getFilePointer();
-        os.writeBytes(largeBuf, largeBuf.length);
-
-        try {
-            assertEquals(currentPos + largeBuf.length, os.getFilePointer());
-        } finally {
-            os.close();
-        }
-
-    }
-    
-   public void testAddExternalFile() throws IOException {
-       createSequenceFile(dir, "d1", (byte) 0, 15);
-
-       Directory newDir = newDirectory();
-       CompoundFileWriter csw = new CompoundFileWriter(newDir, "d.csf");
-       csw.addFile("d1", dir);
-       csw.close();
-
-       CompoundFileReader csr = new CompoundFileReader(newDir, "d.csf");
-       IndexInput expected = dir.openInput("d1");
-       IndexInput actual = csr.openInput("d1");
-       assertSameStreams("d1", expected, actual);
-       assertSameSeekBehavior("d1", expected, actual);
-       expected.close();
-       actual.close();
-       csr.close();
-       
-       newDir.close();
-   }
-
-}
Index: lucene/backwards/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 1138332)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -1,238 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-
-import java.io.*;
-import java.util.*;
-
-/*
-  Verify we can read the pre-2.1 file format, do searches
-  against it, and add documents to it.
-*/
-
-public class TestIndexFileDeleter extends LuceneTestCase {
-  
-  public void testDeleteLeftoverFiles() throws IOException {
-    MockDirectoryWrapper dir = newDirectory();
-    dir.setPreventDoubleWrite(false);
-    IndexWriterConfig conf = newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))
-        .setMaxBufferedDocs(10);
-    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);
-    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS
-    conf.setMergePolicy(mergePolicy);
-
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(10).
-            setMergePolicy(mergePolicy)
-    );
-
-    int i;
-    for(i=0;i<35;i++) {
-      addDoc(writer, i);
-    }
-    mergePolicy.setUseCompoundFile(false);
-    for(;i<45;i++) {
-      addDoc(writer, i);
-    }
-    writer.close();
-
-    // Delete one doc so we get a .del file:
-    IndexReader reader = IndexReader.open(dir, false);
-    Term searchTerm = new Term("id", "7");
-    int delCount = reader.deleteDocuments(searchTerm);
-    assertEquals("didn't delete the right number of documents", 1, delCount);
-
-    // Set one norm so we get a .s0 file:
-    reader.setNorm(21, "content", (float) 1.5);
-    reader.close();
-
-    // Now, artificially create an extra .del file & extra
-    // .s0 file:
-    String[] files = dir.listAll();
-
-    /*
-    for(int j=0;j<files.length;j++) {
-      System.out.println(j + ": " + files[j]);
-    }
-    */
-
-    // The numbering of fields can vary depending on which
-    // JRE is in use.  On some JREs we see content bound to
-    // field 0; on others, field 1.  So, here we have to
-    // figure out which field number corresponds to
-    // "content", and then set our expected file names below
-    // accordingly:
-    CompoundFileReader cfsReader = new CompoundFileReader(dir, "_2.cfs");
-    FieldInfos fieldInfos = new FieldInfos(cfsReader, "_2.fnm");
-    int contentFieldIndex = -1;
-    for(i=0;i<fieldInfos.size();i++) {
-      FieldInfo fi = fieldInfos.fieldInfo(i);
-      if (fi.name.equals("content")) {
-        contentFieldIndex = i;
-        break;
-      }
-    }
-    cfsReader.close();
-    assertTrue("could not locate the 'content' field number in the _2.cfs segment", contentFieldIndex != -1);
-
-    String normSuffix = "s" + contentFieldIndex;
-
-    // Create a bogus separate norms file for a
-    // segment/field that actually has a separate norms file
-    // already:
-    copyFile(dir, "_2_1." + normSuffix, "_2_2." + normSuffix);
-
-    // Create a bogus separate norms file for a
-    // segment/field that actually has a separate norms file
-    // already, using the "not compound file" extension:
-    copyFile(dir, "_2_1." + normSuffix, "_2_2.f" + contentFieldIndex);
-
-    // Create a bogus separate norms file for a
-    // segment/field that does not have a separate norms
-    // file already:
-    copyFile(dir, "_2_1." + normSuffix, "_1_1." + normSuffix);
-
-    // Create a bogus separate norms file for a
-    // segment/field that does not have a separate norms
-    // file already using the "not compound file" extension:
-    copyFile(dir, "_2_1." + normSuffix, "_1_1.f" + contentFieldIndex);
-
-    // Create a bogus separate del file for a
-    // segment that already has a separate del file: 
-    copyFile(dir, "_0_1.del", "_0_2.del");
-
-    // Create a bogus separate del file for a
-    // segment that does not yet have a separate del file:
-    copyFile(dir, "_0_1.del", "_1_1.del");
-
-    // Create a bogus separate del file for a
-    // non-existent segment:
-    copyFile(dir, "_0_1.del", "_188_1.del");
-
-    // Create a bogus segment file:
-    copyFile(dir, "_0.cfs", "_188.cfs");
-
-    // Create a bogus fnm file when the CFS already exists:
-    copyFile(dir, "_0.cfs", "_0.fnm");
-    
-    // Create a deletable file:
-    copyFile(dir, "_0.cfs", "deletable");
-
-    // Create some old segments file:
-    copyFile(dir, "segments_2", "segments");
-    copyFile(dir, "segments_2", "segments_1");
-
-    // Create a bogus cfs file shadowing a non-cfs segment:
-    copyFile(dir, "_1.cfs", "_2.cfs");
-    
-    String[] filesPre = dir.listAll();
-
-    // Open & close a writer: it should delete the above 4
-    // files and nothing more:
-    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
-    writer.close();
-
-    String[] files2 = dir.listAll();
-    dir.close();
-
-    Arrays.sort(files);
-    Arrays.sort(files2);
-    
-    Set<String> dif = difFiles(files, files2);
-    
-    if (!Arrays.equals(files, files2)) {
-      fail("IndexFileDeleter failed to delete unreferenced extra files: should have deleted " + (filesPre.length-files.length) + " files but only deleted " + (filesPre.length - files2.length) + "; expected files:\n    " + asString(files) + "\n  actual files:\n    " + asString(files2)+"\ndif: "+dif);
-    }
-  }
-
-  private static Set<String> difFiles(String[] files1, String[] files2) {
-    Set<String> set1 = new HashSet<String>();
-    Set<String> set2 = new HashSet<String>();
-    Set<String> extra = new HashSet<String>();
-    
-    for (int x=0; x < files1.length; x++) {
-      set1.add(files1[x]);
-    }
-    for (int x=0; x < files2.length; x++) {
-      set2.add(files2[x]);
-    }
-    Iterator<String> i1 = set1.iterator();
-    while (i1.hasNext()) {
-      String o = i1.next();
-      if (!set2.contains(o)) {
-        extra.add(o);
-      }
-    }
-    Iterator<String> i2 = set2.iterator();
-    while (i2.hasNext()) {
-      String o = i2.next();
-      if (!set1.contains(o)) {
-        extra.add(o);
-      }
-    }
-    return extra;
-  }
-  
-  private String asString(String[] l) {
-    String s = "";
-    for(int i=0;i<l.length;i++) {
-      if (i > 0) {
-        s += "\n    ";
-      }
-      s += l[i];
-    }
-    return s;
-  }
-
-  public void copyFile(Directory dir, String src, String dest) throws IOException {
-    IndexInput in = dir.openInput(src);
-    IndexOutput out = dir.createOutput(dest);
-    byte[] b = new byte[1024];
-    long remainder = in.length();
-    while(remainder > 0) {
-      int len = (int) Math.min(b.length, remainder);
-      in.readBytes(b, 0, len);
-      out.writeBytes(b, len);
-      remainder -= len;
-    }
-    in.close();
-    out.close();
-  }
-
-  private void addDoc(IndexWriter writer, int id) throws IOException
-  {
-    Document doc = new Document();
-    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(newField("id", Integer.toString(id), Field.Store.YES, Field.Index.NOT_ANALYZED));
-    writer.addDocument(doc);
-  }
-}
Index: lucene/contrib/misc/src/java/org/apache/lucene/store/NRTCachingDirectory.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/store/NRTCachingDirectory.java	(revision 1138332)
+++ lucene/contrib/misc/src/java/org/apache/lucene/store/NRTCachingDirectory.java	(working copy)
@@ -246,6 +246,25 @@
   }
 
   @Override
+  public synchronized CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    if (cache.fileExists(name)) {
+      return cache.openCompoundInput(name, bufferSize);
+    } else {
+      return delegate.openCompoundInput(name, bufferSize);
+    }
+  }
+  
+  @Override
+  public synchronized CompoundFileDirectory createCompoundOutput(String name)
+      throws IOException {
+    if (cache.fileExists(name)) {
+      throw new IOException("File " + name + "already exists");
+    } else {
+      return delegate.createCompoundOutput(name);
+    }
+  }
+
+  @Override
   public synchronized IndexInput openInput(String name, int bufferSize) throws IOException {
     if (cache.fileExists(name)) {
       return cache.openInput(name, bufferSize);
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(revision 1138332)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(working copy)
@@ -79,7 +79,7 @@
     _TestUtil.rmDir(destDir2);
     destDir2.mkdirs();
     IndexSplitter.main(new String[] {dir.getAbsolutePath(), destDir2.getAbsolutePath(), splitSegName});
-    assertEquals(3, destDir2.listFiles().length);
+    assertEquals(4, destDir2.listFiles().length);
     Directory fsDirDest2 = newFSDirectory(destDir2);
     r = IndexReader.open(fsDirDest2, true);
     assertEquals(50, r.maxDoc());
Index: lucene/src/java/org/apache/lucene/index/CompoundFileReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompoundFileReader.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/CompoundFileReader.java	(working copy)
@@ -1,310 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Lock;
-
-import java.util.HashMap;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-/**
- * Class for accessing a compound stream.
- * This class implements a directory, but is limited to only read operations.
- * Directory methods that would normally modify data throw an exception.
- */
-class CompoundFileReader extends Directory {
-  
-  private int readBufferSize;
-  
-  private static final class FileEntry {
-    long offset;
-    long length;
-  }
-    
-  // Base info
-  private Directory directory;
-  private String fileName;
-  
-  private IndexInput stream;
-  private HashMap<String,FileEntry> entries = new HashMap<String,FileEntry>();
-  
-  public CompoundFileReader(Directory dir, String name) throws IOException {
-    this(dir, name, BufferedIndexInput.BUFFER_SIZE);
-  }
-  
-  public CompoundFileReader(Directory dir, String name, int readBufferSize) throws IOException {
-    assert !(dir instanceof CompoundFileReader) : "compound file inside of compound file: " + name;
-    directory = dir;
-    fileName = name;
-    this.readBufferSize = readBufferSize;
-    
-    boolean success = false;
-    
-    try {
-      stream = dir.openInput(name, readBufferSize);
-      
-      // read the first VInt. If it is negative, it's the version number
-      // otherwise it's the count (pre-3.1 indexes)
-      int firstInt = stream.readVInt();
-      
-      final int count;
-      final boolean stripSegmentName;
-      if (firstInt < CompoundFileWriter.FORMAT_PRE_VERSION) {
-        if (firstInt < CompoundFileWriter.FORMAT_CURRENT) {
-          throw new CorruptIndexException("Incompatible format version: "
-              + firstInt + " expected " + CompoundFileWriter.FORMAT_CURRENT);
-        }
-        // It's a post-3.1 index, read the count.
-        count = stream.readVInt();
-        stripSegmentName = false;
-      } else {
-        count = firstInt;
-        stripSegmentName = true;
-      }
-      
-      // read the directory and init files
-      FileEntry entry = null;
-      for (int i=0; i<count; i++) {
-        long offset = stream.readLong();
-        String id = stream.readString();
-        
-        if (stripSegmentName) {
-          // Fix the id to not include the segment names. This is relevant for
-          // pre-3.1 indexes.
-          id = IndexFileNames.stripSegmentName(id);
-        }
-        
-        if (entry != null) {
-          // set length of the previous entry
-          entry.length = offset - entry.offset;
-        }
-        
-        entry = new FileEntry();
-        entry.offset = offset;
-        entries.put(id, entry);
-      }
-      
-      // set the length of the final entry
-      if (entry != null) {
-        entry.length = stream.length() - entry.offset;
-      }
-      
-      success = true;
-      
-    } finally {
-      if (!success && (stream != null)) {
-        try {
-          stream.close();
-        } catch (IOException e) { }
-      }
-    }
-  }
-  
-  public Directory getDirectory() {
-    return directory;
-  }
-  
-  public String getName() {
-    return fileName;
-  }
-  
-  @Override
-  public synchronized void close() throws IOException {
-    if (stream == null)
-      throw new IOException("Already closed");
-    
-    entries.clear();
-    stream.close();
-    stream = null;
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String id) throws IOException {
-    // Default to readBufferSize passed in when we were opened
-    return openInput(id, readBufferSize);
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String id, int readBufferSize) throws IOException {
-    if (stream == null)
-      throw new IOException("Stream closed");
-    
-    id = IndexFileNames.stripSegmentName(id);
-    FileEntry entry = entries.get(id);
-    if (entry == null)
-      throw new IOException("No sub-file with id " + id + " found (files: " + entries.keySet() + ")");
-    
-    return new CSIndexInput(stream, entry.offset, entry.length, readBufferSize);
-  }
-  
-  /** Returns an array of strings, one for each file in the directory. */
-  @Override
-  public String[] listAll() {
-    String[] res = entries.keySet().toArray(new String[entries.size()]);
-    // Add the segment name
-    String seg = fileName.substring(0, fileName.indexOf('.'));
-    for (int i = 0; i < res.length; i++) {
-      res[i] = seg + res[i];
-    }
-    return res;
-  }
-  
-  /** Returns true iff a file with the given name exists. */
-  @Override
-  public boolean fileExists(String name) {
-    return entries.containsKey(IndexFileNames.stripSegmentName(name));
-  }
-  
-  /** Returns the time the compound file was last modified. */
-  @Override
-  public long fileModified(String name) throws IOException {
-    return directory.fileModified(fileName);
-  }
-  
-  /** Set the modified time of the compound file to now.
-   *  @deprecated Lucene never uses this API; it will be
-   *  removed in 4.0. */
-  @Override
-  @Deprecated
-  public void touchFile(String name) throws IOException {
-    directory.touchFile(fileName);
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public void deleteFile(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  public void renameFile(String from, String to) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Returns the length of a file in the directory.
-   * @throws IOException if the file does not exist */
-  @Override
-  public long fileLength(String name) throws IOException {
-    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
-    if (e == null)
-      throw new FileNotFoundException(name);
-    return e.length;
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public IndexOutput createOutput(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public Lock makeLock(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Implementation of an IndexInput that reads from a portion of the
-   *  compound file. The visibility is left as "package" *only* because
-   *  this helps with testing since JUnit test cases in a different class
-   *  can then access package fields of this class.
-   */
-  static final class CSIndexInput extends BufferedIndexInput {
-    IndexInput base;
-    long fileOffset;
-    long length;
-    
-    CSIndexInput(final IndexInput base, final long fileOffset, final long length) {
-      this(base, fileOffset, length, BufferedIndexInput.BUFFER_SIZE);
-    }
-    
-    CSIndexInput(final IndexInput base, final long fileOffset, final long length, int readBufferSize) {
-      super(readBufferSize);
-      this.base = (IndexInput)base.clone();
-      this.fileOffset = fileOffset;
-      this.length = length;
-    }
-    
-    @Override
-    public Object clone() {
-      CSIndexInput clone = (CSIndexInput)super.clone();
-      clone.base = (IndexInput)base.clone();
-      clone.fileOffset = fileOffset;
-      clone.length = length;
-      return clone;
-    }
-    
-    /** Expert: implements buffer refill.  Reads bytes from the current
-     *  position in the input.
-     * @param b the array to read bytes into
-     * @param offset the offset in the array to start storing bytes
-     * @param len the number of bytes to read
-     */
-    @Override
-    protected void readInternal(byte[] b, int offset, int len) throws IOException {
-      long start = getFilePointer();
-      if(start + len > length)
-        throw new IOException("read past EOF");
-      base.seek(fileOffset + start);
-      base.readBytes(b, offset, len, false);
-    }
-    
-    /** Expert: implements seek.  Sets current position in this file, where
-     *  the next {@link #readInternal(byte[],int,int)} will occur.
-     * @see #readInternal(byte[],int,int)
-     */
-    @Override
-    protected void seekInternal(long pos) {}
-    
-    /** Closes the stream to further operations. */
-    @Override
-    public void close() throws IOException {
-      base.close();
-    }
-    
-    @Override
-    public long length() {
-      return length;
-    }
-    
-    @Override
-    public void copyBytes(IndexOutput out, long numBytes) throws IOException {
-      // Copy first whatever is in the buffer
-      numBytes -= flushBuffer(out, numBytes);
-      
-      // If there are more bytes left to copy, delegate the copy task to the
-      // base IndexInput, in case it can do an optimized copy.
-      if (numBytes > 0) {
-        long start = getFilePointer();
-        if (start + numBytes > length) {
-          throw new IOException("read past EOF");
-        }
-        base.seek(fileOffset + start);
-        base.copyBytes(out, numBytes);
-      }
-    }
-  }
-}
Index: lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java	(working copy)
@@ -1,252 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-import java.util.LinkedList;
-import java.util.HashSet;
-
-import java.io.IOException;
-
-/**
- * Combines multiple files into a single compound file.
- * The file format:<br>
- * <ul>
- *     <li>VInt fileCount</li>
- *     <li>{Directory}
- *         fileCount entries with the following structure:</li>
- *         <ul>
- *             <li>long dataOffset</li>
- *             <li>String fileName</li>
- *         </ul>
- *     <li>{File Data}
- *         fileCount entries with the raw data of the corresponding file</li>
- * </ul>
- *
- * The fileCount integer indicates how many files are contained in this compound
- * file. The {directory} that follows has that many entries. Each directory entry
- * contains a long pointer to the start of this file's data section, and a String
- * with that file's name.
- * 
- * @lucene.internal
- */
-public final class CompoundFileWriter {
-
-    private static final class FileEntry {
-        /** source file */
-        String file;
-
-        /** temporary holder for the start of directory entry for this file */
-        long directoryOffset;
-
-        /** temporary holder for the start of this file's data section */
-        long dataOffset;
-        
-        /** the directory which contains the file. */
-        Directory dir;
-    }
-
-    // Before versioning started.
-    static final int FORMAT_PRE_VERSION = 0;
-    
-    // Segment name is not written in the file names.
-    static final int FORMAT_NO_SEGMENT_PREFIX = -1;
-
-    // NOTE: if you introduce a new format, make it 1 lower
-    // than the current one, and always change this if you
-    // switch to a new format!
-    static final int FORMAT_CURRENT = FORMAT_NO_SEGMENT_PREFIX;
-
-    private Directory directory;
-    private String fileName;
-    private HashSet<String> ids;
-    private LinkedList<FileEntry> entries;
-    private boolean merged = false;
-    private SegmentMerger.CheckAbort checkAbort;
-
-    /** Create the compound stream in the specified file. The file name is the
-     *  entire name (no extensions are added).
-     *  @throws NullPointerException if <code>dir</code> or <code>name</code> is null
-     */
-    public CompoundFileWriter(Directory dir, String name) {
-      this(dir, name, null);
-    }
-
-    CompoundFileWriter(Directory dir, String name, SegmentMerger.CheckAbort checkAbort) {
-        if (dir == null)
-            throw new NullPointerException("directory cannot be null");
-        if (name == null)
-            throw new NullPointerException("name cannot be null");
-        this.checkAbort = checkAbort;
-        directory = dir;
-        fileName = name;
-        ids = new HashSet<String>();
-        entries = new LinkedList<FileEntry>();
-    }
-
-    /** Returns the directory of the compound file. */
-    public Directory getDirectory() {
-        return directory;
-    }
-
-    /** Returns the name of the compound file. */
-    public String getName() {
-        return fileName;
-    }
-
-    /** Add a source stream. <code>file</code> is the string by which the 
-     *  sub-stream will be known in the compound stream.
-     * 
-     *  @throws IllegalStateException if this writer is closed
-     *  @throws NullPointerException if <code>file</code> is null
-     *  @throws IllegalArgumentException if a file with the same name
-     *   has been added already
-     */
-    public void addFile(String file) {
-      addFile(file, directory);
-    }
-
-    /**
-     * Same as {@link #addFile(String)}, only for files that are found in an
-     * external {@link Directory}.
-     */
-    public void addFile(String file, Directory dir) {
-        if (merged)
-            throw new IllegalStateException(
-                "Can't add extensions after merge has been called");
-
-        if (file == null)
-            throw new NullPointerException(
-                "file cannot be null");
-
-        if (! ids.add(file))
-            throw new IllegalArgumentException(
-                "File " + file + " already added");
-
-        FileEntry entry = new FileEntry();
-        entry.file = file;
-        entry.dir = dir;
-        entries.add(entry);
-    }
-
-    /** Merge files with the extensions added up to now.
-     *  All files with these extensions are combined sequentially into the
-     *  compound stream.
-     *  @throws IllegalStateException if close() had been called before or
-     *   if no file has been added to this object
-     */
-    public void close() throws IOException {
-        if (merged)
-            throw new IllegalStateException("Merge already performed");
-
-        if (entries.isEmpty())
-            throw new IllegalStateException("No entries to merge have been defined");
-
-        merged = true;
-
-        // open the compound stream
-        IndexOutput os = directory.createOutput(fileName);
-        IOException priorException = null;
-        try {
-            // Write the Version info - must be a VInt because CFR reads a VInt
-            // in older versions!
-            os.writeVInt(FORMAT_CURRENT);
-            
-            // Write the number of entries
-            os.writeVInt(entries.size());
-
-            // Write the directory with all offsets at 0.
-            // Remember the positions of directory entries so that we can
-            // adjust the offsets later
-            long totalSize = 0;
-            for (FileEntry fe : entries) {
-                fe.directoryOffset = os.getFilePointer();
-                os.writeLong(0);    // for now
-                os.writeString(IndexFileNames.stripSegmentName(fe.file));
-                totalSize += fe.dir.fileLength(fe.file);
-            }
-
-            // Pre-allocate size of file as optimization --
-            // this can potentially help IO performance as
-            // we write the file and also later during
-            // searching.  It also uncovers a disk-full
-            // situation earlier and hopefully without
-            // actually filling disk to 100%:
-            final long finalLength = totalSize+os.getFilePointer();
-            os.setLength(finalLength);
-
-            // Open the files and copy their data into the stream.
-            // Remember the locations of each file's data section.
-            for (FileEntry fe : entries) {
-                fe.dataOffset = os.getFilePointer();
-                copyFile(fe, os);
-            }
-
-            // Write the data offsets into the directory of the compound stream
-            for (FileEntry fe : entries) {
-                os.seek(fe.directoryOffset);
-                os.writeLong(fe.dataOffset);
-            }
-
-            assert finalLength == os.length();
-
-            // Close the output stream. Set the os to null before trying to
-            // close so that if an exception occurs during the close, the
-            // finally clause below will not attempt to close the stream
-            // the second time.
-            IndexOutput tmp = os;
-            os = null;
-            tmp.close();
-        } catch (IOException e) {
-          priorException = e;
-        } finally {
-          IOUtils.closeSafely(priorException, os);
-        }
-    }
-
-  /**
-   * Copy the contents of the file with specified extension into the provided
-   * output stream.
-   */
-  private void copyFile(FileEntry source, IndexOutput os) throws IOException {
-    IndexInput is = source.dir.openInput(source.file);
-    try {
-      long startPtr = os.getFilePointer();
-      long length = is.length();
-      os.copyBytes(is, length);
-
-      if (checkAbort != null) {
-        checkAbort.work(length);
-      }
-
-      // Verify that the output length diff is equal to original file
-      long endPtr = os.getFilePointer();
-      long diff = endPtr - startPtr;
-      if (diff != length)
-        throw new IOException("Difference in the output file offsets " + diff
-            + " does not match the original file length " + length);
-
-    } finally {
-      is.close();
-    }
-  }
-}
Index: lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.lucene.store.RAMFile;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BitVector;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util.ThreadInterruptedException;
 
@@ -597,11 +598,17 @@
           message("flush: create compound file \"" + cfsFileName + "\"");
         }
 
-        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);
-        for(String fileName : newSegment.files()) {
-          cfsWriter.addFile(fileName);
+        final Directory cfsDir = directory.createCompoundOutput(cfsFileName);
+        IOException prior = null;
+        try {
+          for (String fileName : newSegment.files()) {
+            directory.copy(cfsDir, fileName, fileName);
+          }
+        } catch (IOException ex) {
+          prior = ex;
+        } finally {
+          IOUtils.closeSafely(prior, cfsDir);
         }
-        cfsWriter.close();
         deleter.deleteNewFiles(newSegment.files());
         newSegment.setUseCompoundFile(true);
       }
Index: lucene/src/java/org/apache/lucene/index/IndexFileNames.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexFileNames.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/IndexFileNames.java	(working copy)
@@ -74,6 +74,9 @@
 
   /** Extension of compound file */
   public static final String COMPOUND_FILE_EXTENSION = "cfs";
+  
+  /** Extension of compound file entries */
+  public static final String COMPOUND_FILE_ENTRIES_EXTENSION = "cfe";
 
   /** Extension of compound file for doc store files*/
   public static final String COMPOUND_FILE_STORE_EXTENSION = "cfx";
@@ -103,6 +106,7 @@
    */
   public static final String INDEX_EXTENSIONS[] = new String[] {
     COMPOUND_FILE_EXTENSION,
+    COMPOUND_FILE_ENTRIES_EXTENSION,
     FIELD_INFOS_EXTENSION,
     FIELDS_INDEX_EXTENSION,
     FIELDS_EXTENSION,
@@ -262,7 +266,15 @@
     }
     return filename;
   }
-
+  
+  public static String stripExtension(String filename) {
+    int idx = filename.indexOf('.');
+    if (idx != -1) {
+      filename = filename.substring(0, idx);
+    }
+    return filename;
+  }
+  
   /**
    * Returns true if the given filename ends with the separate norms file
    * pattern: {@code SEPARATE_NORMS_EXTENSION + "[0-9]+"}.
Index: lucene/src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexReader.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -1261,14 +1261,14 @@
     }
 
     Directory dir = null;
-    CompoundFileReader cfr = null;
+    CompoundFileDirectory cfr = null;
 
     try {
       File file = new File(filename);
       String dirname = file.getAbsoluteFile().getParent();
       filename = file.getName();
       dir = FSDirectory.open(new File(dirname));
-      cfr = new CompoundFileReader(dir, filename);
+      cfr = dir.openCompoundInput(filename, BufferedIndexInput.BUFFER_SIZE);
 
       String [] files = cfr.listAll();
       ArrayUtil.mergeSort(files);   // sort the array of filename so that the output is more readable
Index: lucene/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -48,6 +48,7 @@
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.Version;
@@ -1208,7 +1209,7 @@
     Directory cfsDir = null;
     try {
       if (info.getUseCompoundFile()) {
-        cfsDir = new CompoundFileReader(directory, IndexFileNames.segmentFileName(info.name, IndexFileNames.COMPOUND_FILE_EXTENSION));
+        cfsDir = directory.openCompoundInput(IndexFileNames.segmentFileName(info.name, IndexFileNames.COMPOUND_FILE_EXTENSION), 1024);
       } else {
         cfsDir = directory;
       }
@@ -3235,21 +3236,24 @@
   private void copySegmentIntoCFS(SegmentInfo info, String segName) throws IOException {
     String segFileName = IndexFileNames.segmentFileName(segName, IndexFileNames.COMPOUND_FILE_EXTENSION);
     Collection<String> files = info.files();
-    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, segFileName);
-    for (String file : files) {
-      String newFileName = segName + IndexFileNames.stripSegmentName(file);
-      if (!IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION)
-          && !IndexFileNames.isSeparateNormsFile(file)) {
-        cfsWriter.addFile(file, info.dir);
-      } else {
-        assert !directory.fileExists(newFileName): "file \"" + newFileName + "\" already exists";
-        info.dir.copy(directory, file, newFileName);
+    final Directory cfsDir = directory.createCompoundOutput(segFileName);
+    IOException prior = null;
+    try {
+      for (String file : files) {
+        String newFileName = segName + IndexFileNames.stripSegmentName(file);
+        if (!IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION)
+            && !IndexFileNames.isSeparateNormsFile(file)) {
+          info.dir.copy(cfsDir, file, file);
+        } else {
+          assert !directory.fileExists(newFileName): "file \"" + newFileName + "\" already exists";
+          info.dir.copy(directory, file, newFileName);
+        }
       }
+    } catch(IOException ex) {
+      prior = ex;
+    } finally {
+      IOUtils.closeSafely(prior, cfsDir);
     }
-    
-    // Create the .cfs
-    cfsWriter.close();
-    
     info.dir = directory;
     info.name = segName;
     info.setUseCompoundFile(true);
Index: lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 
@@ -51,8 +52,8 @@
   TermInfosReader tis;
   FieldsReader fieldsReaderOrig;
   TermVectorsReader termVectorsReaderOrig;
-  CompoundFileReader cfsReader;
-  CompoundFileReader storeCFSReader;
+  CompoundFileDirectory cfsReader;
+  CompoundFileDirectory storeCFSReader;
 
   SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentInfo si, int readBufferSize, int termsIndexDivisor) throws IOException {
     segment = si.name;
@@ -64,7 +65,7 @@
     try {
       Directory dir0 = dir;
       if (si.getUseCompoundFile()) {
-        cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
+        cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
         dir0 = cfsReader;
       }
       cfsDir = dir0;
@@ -145,7 +146,7 @@
         // terms reader with index, the segment has switched
         // to CFS
         if (cfsReader == null) {
-          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
+          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
         }
         dir0 = cfsReader;
       } else {
@@ -211,7 +212,7 @@
       if (si.getDocStoreOffset() != -1) {
         if (si.getDocStoreIsCompoundFile()) {
           assert storeCFSReader == null;
-          storeCFSReader = new CompoundFileReader(dir,
+          storeCFSReader = dir.openCompoundInput(
               IndexFileNames.segmentFileName(si.getDocStoreSegment(), IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),
                                                   readBufferSize);
           storeDir = storeCFSReader;
@@ -225,7 +226,7 @@
         // was not used, but then we are asked to open doc
         // stores after the segment has switched to CFS
         if (cfsReader == null) {
-          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
+          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
         }
         storeDir = cfsReader;
         assert storeDir != null;
Index: lucene/src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentInfo.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.StringHelper;
 
 import java.io.IOException;
 import java.util.HashSet;
@@ -237,7 +238,7 @@
         }
         final Directory dirToTest;
         if (isCompoundFile) {
-          dirToTest = new CompoundFileReader(dir, IndexFileNames.segmentFileName(storesSegment, ext));
+          dirToTest = dir.openCompoundInput(IndexFileNames.segmentFileName(storesSegment, ext), 1024);
         } else {
           dirToTest = dir;
         }
@@ -657,6 +658,10 @@
 
     if (useCompoundFile) {
       filesSet.add(IndexFileNames.segmentFileName(name, IndexFileNames.COMPOUND_FILE_EXTENSION));
+      if (version != null && StringHelper.getVersionComparator().compare("3.3", version) <= 0) {
+        filesSet.add(IndexFileNames.segmentFileName(name,
+            IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+      }
     } else {
       for (String ext : IndexFileNames.NON_STORE_INDEX_EXTENSIONS)
         addIfExists(filesSet, IndexFileNames.segmentFileName(name, ext));
Index: lucene/src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentInfos.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -285,12 +285,12 @@
           Directory dir = directory;
           if (si.getDocStoreOffset() != -1) {
             if (si.getDocStoreIsCompoundFile()) {
-              dir = new CompoundFileReader(dir, IndexFileNames.segmentFileName(
+              dir = dir.openCompoundInput(IndexFileNames.segmentFileName(
                   si.getDocStoreSegment(),
                   IndexFileNames.COMPOUND_FILE_STORE_EXTENSION), 1024);
             }
           } else if (si.getUseCompoundFile()) {
-            dir = new CompoundFileReader(dir, IndexFileNames.segmentFileName(
+            dir = dir.openCompoundInput(IndexFileNames.segmentFileName(
                 si.name, IndexFileNames.COMPOUND_FILE_EXTENSION), 1024);
           }
 
Index: lucene/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.index.MergePolicy.MergeAbortedException;
 import org.apache.lucene.index.PayloadProcessorProvider.PayloadProcessor;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -124,18 +125,19 @@
           throws IOException {
     // Now merge all added files
     Collection<String> files = info.files();
-    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, fileName, checkAbort);
-    for (String file : files) {
-      assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) 
-                : ".del file is not allowed in .cfs: " + file;
-      assert !IndexFileNames.isSeparateNormsFile(file)
-                : "separate norms file (.s[0-9]+) is not allowed in .cfs: " + file;
-      cfsWriter.addFile(file);
+    CompoundFileDirectory cfsDir = directory.createCompoundOutput(fileName);
+    try {
+      for (String file : files) {
+        assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) 
+                  : ".del file is not allowed in .cfs: " + file;
+        assert !IndexFileNames.isSeparateNormsFile(file) 
+                  : "separate norms file (.s[0-9]+) is not allowed in .cfs: " + file;
+        directory.copy(cfsDir, file, file);
+        checkAbort.work(directory.fileLength(file));
+      }
+    } finally {
+      cfsDir.close();
     }
-    
-    // Perform the merge
-    cfsWriter.close();
-   
     return files;
   }
 
Index: lucene/src/java/org/apache/lucene/store/CompoundFileDirectory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/CompoundFileDirectory.java	(revision 1138061)
+++ lucene/src/java/org/apache/lucene/store/CompoundFileDirectory.java	(working copy)
@@ -88,7 +88,7 @@
     if (firstInt == CompoundFileWriter.FORMAT_CURRENT) {
       IndexInput input = null;
       try {
-        input = dir.openInput(IndexFileNames.segmentFileName(IndexFileNames.stripExtension(name), "",
+        input = dir.openInput(IndexFileNames.segmentFileName(IndexFileNames.stripExtension(name),
             IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
         final int readInt = input.readInt(); // unused right now
         assert readInt == CompoundFileWriter.ENTRY_FORMAT_CURRENT;
@@ -309,5 +309,10 @@
       writer = new CompoundFileWriter(directory, fileName);
     }
   }
+
+  @Override
+  public void touchFile(String name) throws IOException {
+  }
+
  
 }
Index: lucene/src/java/org/apache/lucene/store/CompoundFileWriter.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/CompoundFileWriter.java	(revision 1138061)
+++ lucene/src/java/org/apache/lucene/store/CompoundFileWriter.java	(working copy)
@@ -106,7 +106,7 @@
       throw new NullPointerException("name cannot be null");
     directory = dir;
     entryTableName = IndexFileNames.segmentFileName(
-        IndexFileNames.stripExtension(name), "",
+        IndexFileNames.stripExtension(name),
         IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);
     dataFileName = name;
   }
Index: lucene/src/java/org/apache/lucene/store/DataOutput.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/DataOutput.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/DataOutput.java	(working copy)
@@ -60,6 +60,14 @@
     writeByte((byte)(i >>  8));
     writeByte((byte) i);
   }
+  
+  /** Writes a short as two bytes.
+   * @see DataInput#readShort()
+   */
+  public void writeShort(short i) throws IOException {
+    writeByte((byte)(i >>  8));
+    writeByte((byte) i);
+  }
 
   /** Writes an int in a variable-length format.  Writes between one and
    * five bytes.  Smaller values take fewer bytes.  Negative numbers are not
Index: lucene/src/java/org/apache/lucene/store/Directory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/Directory.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/Directory.java	(working copy)
@@ -137,11 +137,35 @@
    * implementation may ignore the buffer size.  Currently
    * the only Directory implementations that respect this
    * parameter are {@link FSDirectory} and {@link
-   * org.apache.lucene.index.CompoundFileReader}.
+   * CompoundFileDirectory}.
   */
   public IndexInput openInput(String name, int bufferSize) throws IOException {
     return openInput(name);
   }
+  
+  /** 
+   * Returns a {@link CompoundFileDirectory} capable of
+   * reading the Lucene compound file format.  
+   * <p>
+   * The default implementation returns 
+   * {@link DefaultCompoundFileDirectory}.
+   * @lucene.experimental
+   */
+  public CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    return new DefaultCompoundFileDirectory(this, name, bufferSize, false);
+  }
+  
+  /** 
+   * Returns a {@link CompoundFileDirectory} capable of
+   * writing the Lucene compound file format.  
+   * <p>
+   * The default implementation returns 
+   * {@link DefaultCompoundFileDirectory}.
+   * @lucene.experimental
+   */
+  public CompoundFileDirectory createCompoundOutput(String name) throws IOException {
+    return new DefaultCompoundFileDirectory(this, name, 1024, true);
+  }
 
   /** Construct a {@link Lock}.
    * @param name the name of the lock file
Index: lucene/src/java/org/apache/lucene/store/FileSwitchDirectory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/FileSwitchDirectory.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/FileSwitchDirectory.java	(working copy)
@@ -163,4 +163,14 @@
   public IndexInput openInput(String name) throws IOException {
     return getDirectory(name).openInput(name);
   }
+
+  @Override
+  public CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    return getDirectory(name).openCompoundInput(name, bufferSize);
+  }
+  
+  @Override
+  public CompoundFileDirectory createCompoundOutput(String name) throws IOException {
+    return getDirectory(name).createCompoundOutput(name);
+  }
 }
Index: lucene/src/java/org/apache/lucene/store/IndexOutput.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/IndexOutput.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/IndexOutput.java	(working copy)
@@ -58,4 +58,5 @@
    * @param length file length
    */
   public void setLength(long length) throws IOException {}
+
 }
Index: lucene/src/java/org/apache/lucene/store/MMapDirectory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/MMapDirectory.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/MMapDirectory.java	(working copy)
@@ -32,6 +32,7 @@
 import java.lang.reflect.Method;
 
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.IOUtils;
 
 /** File-based {@link Directory} implementation that uses
  *  mmap for reading, and {@link
@@ -213,12 +214,50 @@
     File f = new File(getDirectory(), name);
     RandomAccessFile raf = new RandomAccessFile(f, "r");
     try {
-      return new MMapIndexInput(raf, chunkSizePower);
+      return new MMapIndexInput(raf, 0, raf.length(), chunkSizePower);
     } finally {
       raf.close();
     }
   }
+  
+  @Override
+  public CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    return new MMapCompoundFileDirectory(name, bufferSize);
+  }
+  
+  private final class MMapCompoundFileDirectory extends CompoundFileDirectory {
+    private RandomAccessFile raf = null;
 
+    public MMapCompoundFileDirectory(String fileName, int readBufferSize) throws IOException {
+      super(MMapDirectory.this, fileName, readBufferSize);
+      IndexInput stream = null;
+      try {
+        File f = new File(MMapDirectory.this.getDirectory(), fileName);
+        raf = new RandomAccessFile(f, "r");
+        stream = new MMapIndexInput(raf, 0, raf.length(), chunkSizePower);
+        initForRead(CompoundFileDirectory.readEntries(stream, MMapDirectory.this, fileName));
+        stream.close();
+      } catch (IOException e) {
+        // throw our original exception
+        IOUtils.closeSafely(e, raf, stream);
+      }
+    }
+
+    @Override
+    public IndexInput openInputSlice(String id, long offset, long length, int readBufferSize) throws IOException {
+      return new MMapIndexInput(raf, offset, length, chunkSizePower);
+    }
+
+    @Override
+    public synchronized void close() throws IOException {
+      try {
+        raf.close();
+      } finally {
+        super.close();
+      }
+    }
+  }
+
   // Because Java's ByteBuffer uses an int to address the
   // values, it's necessary to access a file >
   // Integer.MAX_VALUE in size using multiple byte buffers.
@@ -235,8 +274,8 @@
   
     private boolean isClone = false;
     
-    MMapIndexInput(RandomAccessFile raf, int chunkSizePower) throws IOException {
-      this.length = raf.length();
+    MMapIndexInput(RandomAccessFile raf, long offset, long length, int chunkSizePower) throws IOException {
+      this.length = length;
       this.chunkSizePower = chunkSizePower;
       this.chunkSize = 1L << chunkSizePower;
       this.chunkSizeMask = chunkSize - 1L;
@@ -261,7 +300,7 @@
           ? chunkSize
           : (length - bufferStart)
         );
-        this.buffers[bufNr] = rafc.map(MapMode.READ_ONLY, bufferStart, bufSize);
+        this.buffers[bufNr] = rafc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
         bufferStart += bufSize;
       }
       seek(0L);
Index: lucene/src/java/org/apache/lucene/store/NIOFSDirectory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/NIOFSDirectory.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/NIOFSDirectory.java	(working copy)
@@ -24,6 +24,9 @@
 import java.nio.channels.FileChannel;
 import java.util.concurrent.Future; // javadoc
 
+import org.apache.lucene.store.SimpleFSDirectory.SimpleFSIndexInput;
+import org.apache.lucene.util.IOUtils;
+
 /**
  * An {@link FSDirectory} implementation that uses java.nio's FileChannel's
  * positional read, which allows multiple threads to read from the same file
@@ -77,7 +80,48 @@
     ensureOpen();
     return new NIOFSIndexInput(new File(getDirectory(), name), bufferSize, getReadChunkSize());
   }
+  
+  @Override
+  public CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    return new NIOFSCompoundFileDirectory(name, bufferSize);
+  }
 
+  private final class NIOFSCompoundFileDirectory extends CompoundFileDirectory {
+    private SimpleFSIndexInput.Descriptor fd;
+    private FileChannel fc;
+
+    public NIOFSCompoundFileDirectory(String fileName, int readBufferSize) throws IOException {
+      super(NIOFSDirectory.this, fileName, readBufferSize);
+      IndexInput stream = null;
+      try {
+        File f = new File(NIOFSDirectory.this.getDirectory(), fileName);
+        fd = new SimpleFSIndexInput.Descriptor(f, "r");
+        fc = fd.getChannel();
+        stream = new NIOFSIndexInput(fd, fc, 0, fd.length, readBufferSize,
+            getReadChunkSize());
+        initForRead(CompoundFileDirectory.readEntries(stream, NIOFSDirectory.this, fileName));
+        stream.close();
+      } catch (IOException e) {
+        // throw our original exception
+        IOUtils.closeSafely(e, fc, fd, stream);
+      }
+    }
+    
+    @Override
+    public IndexInput openInputSlice(String id, long offset, long length, int readBufferSize) throws IOException {
+      return new NIOFSIndexInput(fd, fc, offset, length, readBufferSize, getReadChunkSize());
+    }
+
+    @Override
+    public synchronized void close() throws IOException {
+      try {
+        IOUtils.closeSafely(false, fc, fd);
+      } finally {
+        super.close();
+      }
+    }
+  }
+
   protected static class NIOFSIndexInput extends SimpleFSDirectory.SimpleFSIndexInput {
 
     private ByteBuffer byteBuf; // wraps the buffer for NIO
@@ -91,6 +135,12 @@
       super(path, bufferSize, chunkSize);
       channel = file.getChannel();
     }
+    
+    public NIOFSIndexInput(Descriptor file, FileChannel fc, long off, long length, int bufferSize, int chunkSize) throws IOException {
+      super(file, off, length, bufferSize, chunkSize);
+      channel = fc;
+      isClone = true;
+    }
 
     @Override
     protected void newBuffer(byte[] newBuffer) {
@@ -145,7 +195,11 @@
       int readLength = bb.limit() - readOffset;
       assert readLength == len;
 
-      long pos = getFilePointer();
+      long pos = getFilePointer() + off;
+      
+      if (pos + len > end) {
+        throw new IOException("read past EOF");
+      }
 
       try {
         while (readLength > 0) {
@@ -159,9 +213,6 @@
           }
           bb.limit(limit);
           int i = channel.read(bb, pos);
-          if (i == -1) {
-            throw new IOException("read past EOF");
-          }
           pos += i;
           readOffset += i;
           readLength -= i;
Index: lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.java
===================================================================
--- lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.java	(revision 1138332)
+++ lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.java	(working copy)
@@ -21,6 +21,8 @@
 import java.io.IOException;
 import java.io.RandomAccessFile;
 
+import org.apache.lucene.util.IOUtils;
+
 /** A straightforward implementation of {@link FSDirectory}
  *  using java.io.RandomAccessFile.  However, this class has
  *  poor concurrent performance (multiple threads will
@@ -55,7 +57,46 @@
     ensureOpen();
     return new SimpleFSIndexInput(new File(directory, name), bufferSize, getReadChunkSize());
   }
+  
+  @Override
+  public CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    return new SimpleFSCompoundFileDirectory(name, bufferSize);
+  }
 
+  private final class SimpleFSCompoundFileDirectory extends CompoundFileDirectory {
+    private SimpleFSIndexInput.Descriptor fd;
+
+    public SimpleFSCompoundFileDirectory(String fileName, int readBufferSize) throws IOException {
+      super(SimpleFSDirectory.this, fileName, readBufferSize);
+      IndexInput stream = null;
+      try {
+        final File f = new File(SimpleFSDirectory.this.getDirectory(), fileName);
+        fd = new SimpleFSIndexInput.Descriptor(f, "r");
+        stream = new SimpleFSIndexInput(fd, 0, fd.length, readBufferSize,
+            getReadChunkSize());
+        initForRead(CompoundFileDirectory.readEntries(stream, SimpleFSDirectory.this, fileName));
+        stream.close();
+      } catch (IOException e) {
+        // throw our original exception
+        IOUtils.closeSafely(e, fd, stream);
+      }
+    }
+
+    @Override
+    public IndexInput openInputSlice(String id, long offset, long length, int readBufferSize) throws IOException {
+      return new SimpleFSIndexInput(fd, offset, length, readBufferSize, getReadChunkSize());
+    }
+
+    @Override
+    public synchronized void close() throws IOException {
+      try {
+        fd.close();
+      } finally {
+        super.close();
+      }
+    }
+  }
+
   protected static class SimpleFSIndexInput extends BufferedIndexInput {
   
     protected static class Descriptor extends RandomAccessFile {
@@ -84,25 +125,42 @@
     boolean isClone;
     //  LUCENE-1566 - maximum read length on a 32bit JVM to prevent incorrect OOM 
     protected final int chunkSize;
+    protected final long off;
+    protected final long end;
     
     public SimpleFSIndexInput(File path, int bufferSize, int chunkSize) throws IOException {
       super(bufferSize);
-      file = new Descriptor(path, "r");
+      this.file = new Descriptor(path, "r"); 
       this.chunkSize = chunkSize;
+      this.off = 0L;
+      this.end = file.length;
     }
+    
+    public SimpleFSIndexInput(Descriptor file, long off, long length, int bufferSize, int chunkSize) throws IOException {
+      super(bufferSize);
+      this.file = file;
+      this.chunkSize = chunkSize;
+      this.off = off;
+      this.end = off + length;
+      this.isClone = true; // well, we are sorta?
+    }
   
     /** IndexInput methods */
     @Override
     protected void readInternal(byte[] b, int offset, int len)
          throws IOException {
       synchronized (file) {
-        long position = getFilePointer();
+        long position = off + getFilePointer();
         if (position != file.position) {
           file.seek(position);
           file.position = position;
         }
         int total = 0;
 
+        if (position + len > end) {
+          throw new IOException("read past EOF");
+        }
+
         try {
           do {
             final int readLength;
@@ -113,9 +171,6 @@
               readLength = chunkSize;
             }
             final int i = file.read(b, offset + total, readLength);
-            if (i == -1) {
-              throw new IOException("read past EOF");
-            }
             file.position += i;
             total += i;
           } while (total < len);
@@ -144,7 +199,7 @@
   
     @Override
     public long length() {
-      return file.length;
+      return end - off;
     }
   
     @Override
Index: lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java	(revision 1138332)
+++ lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java	(working copy)
@@ -398,7 +398,7 @@
     }
   }
 
-  private void addFileHandle(Closeable c, String name, boolean input) {
+  void addFileHandle(Closeable c, String name, boolean input) {
     Integer v = openFiles.get(name);
     if (v != null) {
       v = Integer.valueOf(v.intValue()+1);
@@ -426,7 +426,13 @@
     addFileHandle(ii, name, true);
     return ii;
   }
-
+  
+  @Override
+  public synchronized CompoundFileDirectory openCompoundInput(String name, int bufferSize) throws IOException {
+    maybeYield();
+    return new MockCompoundFileDirectoryWrapper(name, this, delegate.openCompoundInput(name, bufferSize));
+  }
+  
   /** Provided for testing purposes.  Use sizeInBytes() instead. */
   public synchronized final long getRecomputedSizeInBytes() throws IOException {
     if (!(delegate instanceof RAMDirectory))
@@ -483,7 +489,7 @@
     delegate.close();
   }
 
-  private synchronized void removeOpenFile(Closeable c, String name) {
+  synchronized void removeOpenFile(Closeable c, String name) {
     Integer v = openFiles.get(name);
     // Could be null when crash() was called
     if (v != null) {
Index: lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1138332)
+++ lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -949,7 +949,7 @@
     w3.addIndexes(readers);
     w3.close();
     
-    assertEquals("Only one compound segment should exist", 3, dir.listAll().length);
+    assertEquals("Only one compound segment should exist", 4, dir.listAll().length);
   }
  
   // LUCENE-2996: tests that addIndexes(IndexReader) applies existing deletes correctly.
Index: lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 1138332)
+++ lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -24,7 +24,6 @@
 import java.util.Arrays;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.ArrayList;
 import java.util.Random;
 
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
@@ -558,7 +557,7 @@
       // figure out which field number corresponds to
       // "content", and then set our expected file names below
       // accordingly:
-      CompoundFileReader cfsReader = new CompoundFileReader(dir, "_0.cfs");
+      Directory cfsReader = dir.openCompoundInput("_0.cfs", 1024);
       FieldInfos fieldInfos = new FieldInfos(cfsReader, "_0.fnm");
       int contentFieldIndex = -1;
       for(int i=0;i<fieldInfos.size();i++) {
@@ -573,6 +572,7 @@
 
       // Now verify file names:
       String[] expected = new String[] {"_0.cfs",
+                               "_0.cfe",
                                "_0_1.del",
                                "_0_1.s" + contentFieldIndex,
                                "segments_2",
Index: lucene/src/test/org/apache/lucene/index/TestCompoundFile.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCompoundFile.java	(revision 1138332)
+++ lucene/src/test/org/apache/lucene/index/TestCompoundFile.java	(working copy)
@@ -23,6 +23,8 @@
 import org.apache.lucene.util.LuceneTestCase;
 import junit.framework.TestSuite;
 import junit.textui.TestRunner;
+
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
@@ -200,11 +202,11 @@
         for (int i=0; i<data.length; i++) {
             String name = "t" + data[i];
             createSequenceFile(dir, name, (byte) 0, data[i]);
-            CompoundFileWriter csw = new CompoundFileWriter(dir, name + ".cfs");
-            csw.addFile(name);
+            CompoundFileDirectory csw = dir.createCompoundOutput(name + ".cfs");
+            dir.copy(csw, name, name);
             csw.close();
 
-            CompoundFileReader csr = new CompoundFileReader(dir, name + ".cfs");
+            CompoundFileDirectory csr = dir.openCompoundInput(name + ".cfs", 1024);
             IndexInput expected = dir.openInput(name);
             IndexInput actual = csr.openInput(name);
             assertSameStreams(name, expected, actual);
@@ -223,12 +225,12 @@
         createSequenceFile(dir, "d1", (byte) 0, 15);
         createSequenceFile(dir, "d2", (byte) 0, 114);
 
-        CompoundFileWriter csw = new CompoundFileWriter(dir, "d.csf");
-        csw.addFile("d1");
-        csw.addFile("d2");
+        CompoundFileDirectory csw = dir.createCompoundOutput("d.cfs");
+        dir.copy(csw, "d1", "d1");
+        dir.copy(csw, "d2", "d2");
         csw.close();
 
-        CompoundFileReader csr = new CompoundFileReader(dir, "d.csf");
+        CompoundFileDirectory csr = dir.openCompoundInput("d.cfs", 1024);
         IndexInput expected = dir.openInput("d1");
         IndexInput actual = csr.openInput("d1");
         assertSameStreams("d1", expected, actual);
@@ -273,17 +275,18 @@
         createRandomFile(dir, segment + ".notIn2", 51);
 
         // Now test
-        CompoundFileWriter csw = new CompoundFileWriter(dir, "test.cfs");
+        CompoundFileDirectory csw = dir.createCompoundOutput("test.cfs");
         final String data[] = new String[] {
             ".zero", ".one", ".ten", ".hundred", ".big1", ".big2", ".big3",
             ".big4", ".big5", ".big6", ".big7"
         };
         for (int i=0; i<data.length; i++) {
-            csw.addFile(segment + data[i]);
+            String fileName = segment + data[i];
+            dir.copy(csw, fileName, fileName);
         }
         csw.close();
 
-        CompoundFileReader csr = new CompoundFileReader(dir, "test.cfs");
+        CompoundFileDirectory csr = dir.openCompoundInput("test.cfs", 1024);
         for (int i=0; i<data.length; i++) {
             IndexInput check = dir.openInput(segment + data[i]);
             IndexInput test = csr.openInput(segment + data[i]);
@@ -302,10 +305,11 @@
      *  the size of each file is 1000 bytes.
      */
     private void setUp_2() throws IOException {
-        CompoundFileWriter cw = new CompoundFileWriter(dir, "f.comp");
+        CompoundFileDirectory cw = dir.createCompoundOutput("f.comp");
         for (int i=0; i<20; i++) {
             createSequenceFile(dir, "f" + i, (byte) 0, 2000);
-            cw.addFile("f" + i);
+            String fileName = "f" + i;
+            dir.copy(cw, fileName, fileName);
         }
         cw.close();
     }
@@ -350,26 +354,9 @@
         }
     }
 
-
-    static boolean isCSIndexInput(IndexInput is) {
-        return is instanceof CompoundFileReader.CSIndexInput;
-    }
-
-    static boolean isCSIndexInputOpen(IndexInput is) throws IOException {
-        if (isCSIndexInput(is)) {
-            CompoundFileReader.CSIndexInput cis =
-            (CompoundFileReader.CSIndexInput) is;
-
-            return _TestHelper.isSimpleFSIndexInputOpen(cis.base);
-        } else {
-            return false;
-        }
-    }
-
-
     public void testClonedStreamsClosing() throws IOException {
         setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
+        CompoundFileDirectory cr = dir.openCompoundInput("f.comp", 1024);
 
         // basic clone
         IndexInput expected = dir.openInput("f11");
@@ -379,10 +366,8 @@
         assertTrue(_TestHelper.isSimpleFSIndexInputOpen(expected));
 
         IndexInput one = cr.openInput("f11");
-        assertTrue(isCSIndexInputOpen(one));
 
         IndexInput two = (IndexInput) one.clone();
-        assertTrue(isCSIndexInputOpen(two));
 
         assertSameStreams("basic clone one", expected, one);
         expected.seek(0);
@@ -390,7 +375,6 @@
 
         // Now close the first stream
         one.close();
-        assertTrue("Only close when cr is closed", isCSIndexInputOpen(one));
 
         // The following should really fail since we couldn't expect to
         // access a file once close has been called on it (regardless of
@@ -402,8 +386,6 @@
 
         // Now close the compound reader
         cr.close();
-        assertFalse("Now closed one", isCSIndexInputOpen(one));
-        assertFalse("Now closed two", isCSIndexInputOpen(two));
 
         // The following may also fail since the compound stream is closed
         expected.seek(0);
@@ -426,7 +408,7 @@
      */
     public void testRandomAccess() throws IOException {
         setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
+        CompoundFileDirectory cr = dir.openCompoundInput("f.comp", 1024);
 
         // Open two files
         IndexInput e1 = dir.openInput("f11");
@@ -505,7 +487,7 @@
      */
     public void testRandomAccessClones() throws IOException {
         setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
+        CompoundFileDirectory cr = dir.openCompoundInput("f.comp", 1024);
 
         // Open two files
         IndexInput e1 = cr.openInput("f11");
@@ -582,7 +564,7 @@
 
     public void testFileNotFound() throws IOException {
         setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
+        CompoundFileDirectory cr = dir.openCompoundInput("f.comp", 1024);
 
         // Open two files
         try {
@@ -600,7 +582,7 @@
 
     public void testReadPastEOF() throws IOException {
         setUp_2();
-        CompoundFileReader cr = new CompoundFileReader(dir, "f.comp");
+        CompoundFileDirectory cr = dir.openCompoundInput("f.comp", 1024);
         IndexInput is = cr.openInput("f2");
         is.seek(is.length() - 10);
         byte b[] = new byte[100];
@@ -653,11 +635,11 @@
        createSequenceFile(dir, "d1", (byte) 0, 15);
 
        Directory newDir = newDirectory();
-       CompoundFileWriter csw = new CompoundFileWriter(newDir, "d.csf");
-       csw.addFile("d1", dir);
+       CompoundFileDirectory csw = newDir.createCompoundOutput("d.cfs");
+       dir.copy(csw, "d1", "d1");
        csw.close();
 
-       CompoundFileReader csr = new CompoundFileReader(newDir, "d.csf");
+       CompoundFileDirectory csr = newDir.openCompoundInput("d.cfs", 1024);
        IndexInput expected = dir.openInput("d1");
        IndexInput actual = csr.openInput("d1");
        assertSameStreams("d1", expected, actual);
@@ -668,5 +650,72 @@
        
        newDir.close();
    }
+   
+   
+  public void testAppend() throws IOException {
+    Directory newDir = newDirectory();
+    CompoundFileDirectory csw = newDir.createCompoundOutput("d.cfs");
+    int size = 5 + random.nextInt(128);
+    for (int j = 0; j < 2; j++) {
+      IndexOutput os = csw.createOutput("seg" + j + "_foo.txt");
+      for (int i = 0; i < size; i++) {
+        os.writeInt(i);
+      }
+      os.close();
+      String[] listAll = newDir.listAll();
+      assertEquals(1, listAll.length);
+      assertEquals("d.cfs", listAll[0]);
+    }
+    createSequenceFile(dir, "d1", (byte) 0, 15);
+    dir.copy(csw, "d1", "d1");
+    String[] listAll = newDir.listAll();
+    assertEquals(1, listAll.length);
+    assertEquals("d.cfs", listAll[0]);
+    csw.close();
+    CompoundFileDirectory csr = newDir.openCompoundInput("d.cfs", 1024);
+    for (int j = 0; j < 2; j++) {
+      IndexInput openInput = csr.openInput("seg" + j + "_foo.txt");
+      assertEquals(size * 4, openInput.length());
+      for (int i = 0; i < size; i++) {
+        assertEquals(i, openInput.readInt());
+      }
 
+      openInput.close();
+
+    }
+    IndexInput expected = dir.openInput("d1");
+    IndexInput actual = csr.openInput("d1");
+    assertSameStreams("d1", expected, actual);
+    assertSameSeekBehavior("d1", expected, actual);
+    expected.close();
+    actual.close();
+    csr.close();
+    newDir.close();
+  }
+  
+  public void testAppendTwice() throws IOException {
+    Directory newDir = newDirectory();
+    CompoundFileDirectory csw = newDir.createCompoundOutput("d.cfs");
+    createSequenceFile(newDir, "d1", (byte) 0, 15);
+    IndexOutput out = csw.createOutput("d.xyz");
+    out.writeInt(0);
+    try {
+      newDir.copy(csw, "d1", "d1");
+      fail("file does already exist");
+    } catch (IOException e) {
+      //
+    }
+    out.close();
+    assertEquals(1, csw.listAll().length);
+    assertEquals("d.xyz", csw.listAll()[0]);
+   
+    csw.close();
+
+    CompoundFileDirectory cfr = newDir.openCompoundInput("d.cfs", 1024);
+    assertEquals(1, cfr.listAll().length);
+    assertEquals("d.xyz", cfr.listAll()[0]);
+    cfr.close();
+    newDir.close();
+  }
+
 }
Index: lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 1138332)
+++ lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -93,7 +94,7 @@
     // figure out which field number corresponds to
     // "content", and then set our expected file names below
     // accordingly:
-    CompoundFileReader cfsReader = new CompoundFileReader(dir, "_2.cfs");
+    CompoundFileDirectory cfsReader = dir.openCompoundInput("_2.cfs", 1024);
     FieldInfos fieldInfos = new FieldInfos(cfsReader, "_2.fnm");
     int contentFieldIndex = -1;
     for(i=0;i<fieldInfos.size();i++) {
