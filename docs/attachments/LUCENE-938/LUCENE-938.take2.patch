Index: CHANGES.txt
===================================================================
--- CHANGES.txt	(revision 553298)
+++ CHANGES.txt	(working copy)
@@ -20,6 +20,10 @@
  1. LUCENE-933: QueryParser fixed to not produce empty sub 
     BooleanQueries "()" even if the Analyzer proudced no 
     tokens for input. (Doron Cohen)
+
+ 2. LUCENE-938: Fixed cases where an unhandled exception in
+    IndexWriter's methods could cause deletes to be lost.
+    (Steven Parkes via Mike McCandless)
     
 New features
 
Index: src/test/org/apache/lucene/store/MockRAMDirectory.java
===================================================================
--- src/test/org/apache/lucene/store/MockRAMDirectory.java	(revision 553298)
+++ src/test/org/apache/lucene/store/MockRAMDirectory.java	(working copy)
@@ -24,6 +24,7 @@
 import java.util.Random;
 import java.util.Map;
 import java.util.HashMap;
+import java.util.ArrayList;
 
 /**
  * This is a subclass of RAMDirectory that adds methods
@@ -116,6 +117,7 @@
   }
 
   void maybeThrowIOException() throws IOException {
+    maybeThrowDeterministicException();
     if (randomIOExceptionRate > 0.0) {
       int number = Math.abs(randomState.nextInt() % 1000);
       if (number < randomIOExceptionRate*1000) {
@@ -213,4 +215,59 @@
       }
     }
   }
+
+  /**
+   * Objects that represent failable conditions. Objects of a derived
+   * class are created and registered with the mock directory. After
+   * register, each object will be invoked once for each first write
+   * of a file, giving the object a chance to throw an IOException.
+   */
+  public static class Failure {
+    /**
+     * eval is called on the first write of every new file.
+     */
+    public void eval(MockRAMDirectory dir) throws IOException { }
+
+    /**
+     * reset should set the state of the failure to its default
+     * (freshly constructued) state. Reset is convenient for tests
+     * that want to create one failure object and then reuse it in
+     * multiple cases. This, combineded with the fact that Failure
+     * subclasses are often anonymous classes makes reset difficult to
+     * do otherwise.
+     *
+     * A typical example of use is
+     * Failure failure = new Failure() { ... };
+     * ...
+     * mock.failOn(failure.reset())
+     */
+    public Failure reset() { return this; }
+  }
+
+  ArrayList failures;
+
+  /**
+   * add a Failure object to the list of objects to be evaluated
+   * at every potential failure point
+   */
+  public void failOn(Failure fail) {
+    if (failures == null) {
+      failures = new ArrayList();
+    }
+    failures.add(fail);
+  }
+
+  /**
+   * Itterate through the failures list, giving each object a
+   * chance to throw an IOE
+   */
+  void maybeThrowDeterministicException() throws IOException {
+    if (failures != null) {
+      for(int i = 0; i < failures.size(); i++) {
+        ((Failure)failures.get(i)).eval(this);
+      }
+    }
+  }
+
+
 }
Index: src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 553298)
+++ src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.lang.StackTraceElement;
 
 import junit.framework.TestCase;
 
@@ -534,6 +535,306 @@
     }
   }
 
+  // This test tests that buffered deletes are not lost due to i/o errors occuring after the buffered deletes have
+  // been flushed but before the segmentInfos have been successfully written
+
+  public void testErrorAfterApplyDeletes() throws IOException {
+    
+    MockRAMDirectory.Failure failure = new MockRAMDirectory.Failure() {
+        boolean sawMaybe = false;
+        boolean failed = false;
+        public MockRAMDirectory.Failure reset() {
+          sawMaybe = false;
+          failed = false;
+          return this;
+        }
+        public void eval(MockRAMDirectory dir)  throws IOException {
+          if (sawMaybe && !failed) {
+            failed = true;
+            throw new IOException("fail after applyDeletes");
+          }
+          if (!failed) {
+            StackTraceElement[] trace = new Exception().getStackTrace();
+            for (int i = 0; i < trace.length; i++) {
+              if ("applyDeletes".equals(trace[i].getMethodName())) {
+                sawMaybe = true;
+                break;
+              }
+            }
+          }
+        }
+      };
+
+    // create a couple of files
+
+    String[] keywords = { "1", "2" };
+    String[] unindexed = { "Netherlands", "Italy" };
+    String[] unstored = { "Amsterdam has lots of bridges",
+        "Venice has lots of canals" };
+    String[] text = { "Amsterdam", "Venice" };
+
+    for(int pass=0;pass<2;pass++) {
+      boolean autoCommit = (0==pass);
+      Directory ramDir = new RAMDirectory();
+      MockRAMDirectory dir = new MockRAMDirectory(ramDir);
+      IndexWriter modifier = new IndexWriter(dir, autoCommit,
+                                             new WhitespaceAnalyzer(), true);
+      modifier.setUseCompoundFile(true);
+      modifier.setMaxBufferedDeleteTerms(2);
+
+      dir.failOn(failure.reset());
+
+      for (int i = 0; i < keywords.length; i++) {
+        Document doc = new Document();
+        doc.add(new Field("id", keywords[i], Field.Store.YES,
+                          Field.Index.UN_TOKENIZED));
+        doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                          Field.Index.NO));
+        doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                          Field.Index.TOKENIZED));
+        doc.add(new Field("city", text[i], Field.Store.YES,
+                          Field.Index.TOKENIZED));
+        modifier.addDocument(doc);
+      }
+      // flush (and commit if ac)
+
+      modifier.optimize();
+
+      // commit if !ac
+
+      if (!autoCommit) {
+        modifier.close();
+      }
+      // one of the two files hits
+
+      Term term = new Term("city", "Amsterdam");
+      int hitCount = getHitCount(dir, term);
+      assertEquals(1, hitCount);
+
+      // open the writer again (closed above)
+
+      if (!autoCommit) {
+        modifier = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer());
+        modifier.setUseCompoundFile(true);
+      }
+
+      // delete the doc
+      // max buf del terms is two, so this is buffered
+
+      modifier.deleteDocuments(term);
+
+      // add a doc (needed for the !ac case; see below)
+      // doc remains buffered
+
+      Document doc = new Document();
+      modifier.addDocument(doc);
+
+      // flush the changes
+      // the buffered deletes
+      // and the new doc
+
+      // The failure object will fail on the first write after the del file gets created
+      // when processing the buffered delete
+
+      // in the ac case, this will be when writing the new segments files
+      // so we really don't need the new doc, but it's harmless
+
+      // in the !ac case, a new segments file won't be created
+      // but in this case, creation of the cfs file happens next
+      // so we need the doc (to test that it's okay that we don't lose deletes if failing
+      // while creating the cfs file
+
+      boolean failed = false;
+      try {
+        modifier.flush();
+      } catch (IOException ioe) {
+        failed = true;
+      }
+
+      assertTrue(failed);
+
+      // The flush above failed, so we need to retry it (which will succeed, because the failure is a one-shot
+
+      if (!autoCommit) {
+        modifier.close();
+      } else {
+        modifier.flush();
+      }
+
+      hitCount = getHitCount(dir, term);
+
+      // If we haven't lost the delete the hit count will be zero
+
+      assertEquals(0, hitCount);
+
+      if (autoCommit) {
+        modifier.close();
+      }
+
+      dir.close();
+    }
+  }
+
+  // This test tests that buffered deletes are lost during exception that occur either within or when creating
+  // a transaction
+
+  public void testErrorDuringTransaction() throws IOException {
+
+    MockRAMDirectory.Failure failure = new MockRAMDirectory.Failure() {
+
+        boolean sawMaybe = false;
+        boolean failed = false;
+
+        public MockRAMDirectory.Failure reset() {
+          sawMaybe = false;
+          failed = false;
+          return this;
+        }
+        public void eval(MockRAMDirectory dir)  throws IOException {
+          if (sawMaybe && !failed) {
+            failed = true;
+            throw new IOException("fail after applyDeletes");
+          }
+          if (!failed) {
+            StackTraceElement[] trace = new Exception().getStackTrace();
+            for (int i = 0; i < trace.length; i++) {
+              if ("applyDeletes".equals(trace[i].getMethodName())) {
+                sawMaybe = true;
+                break;
+              }
+            }
+          }
+        }
+
+      };
+
+    // add a couple of files
+
+    String[] keywords = { "1", "2" };
+    String[] unindexed = { "Netherlands", "Italy" };
+    String[] unstored = { "Amsterdam has lots of bridges",
+        "Venice has lots of canals" };
+    String[] text = { "Amsterdam", "Venice" };
+
+    for(int pass=0;pass<2;pass++) {
+      boolean autoCommit = (0==pass);
+
+      Directory ramDir = new RAMDirectory();
+      MockRAMDirectory dir = new MockRAMDirectory(ramDir);
+      IndexWriter modifier = new IndexWriter(dir, autoCommit,
+                                             new WhitespaceAnalyzer(), true);
+      modifier.setUseCompoundFile(true);
+      modifier.setMaxBufferedDeleteTerms(2);
+
+      dir.failOn(failure.reset());
+
+      // First we add a bunch of documents
+      // below maxBufferedDocs, so not flushed
+
+      for (int i = 0; i < keywords.length; i++) {
+        Document doc = new Document();
+        doc.add(new Field("id", keywords[i], Field.Store.YES,
+                          Field.Index.UN_TOKENIZED));
+        doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                          Field.Index.NO));
+        doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                          Field.Index.TOKENIZED));
+        doc.add(new Field("city", text[i], Field.Store.YES,
+                          Field.Index.TOKENIZED));
+        modifier.addDocument(doc);
+      }
+
+      // this will cause a flush (and a commit, if ac)
+
+      modifier.optimize();
+      // cause the commit in the !ac case
+
+      if (!autoCommit) {
+        modifier.close();
+      }
+
+      // We should see the one doc added above
+
+      Term term = new Term("city", "Amsterdam");
+      int hitCount = getHitCount(dir, term);
+      assertEquals(1, hitCount);
+
+      if (!autoCommit) {
+        modifier = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer());
+        modifier.setUseCompoundFile(true);
+      }
+
+      // Now buffer a delete for one of these files
+
+      modifier.deleteDocuments(term);
+
+      // We'll now try to start a trans
+      // !ac: no problem; nothing gets written
+      // ac: tries to flush so triggers the failure,
+      // so we retry the start (which will succeed the second time because the failure is a one-shot
+      
+      try {
+        modifier.startTransaction();
+        assert !autoCommit;
+      } catch (IOException ioe) {
+        assert autoCommit;
+        modifier.startTransaction();
+      }
+
+      // Now we add the docs again
+      // again, in both cases (ac, !ac) they're buffered
+
+      for (int i = 0; i < keywords.length; i++) {
+        Document doc = new Document();
+        doc.add(new Field("id", keywords[i], Field.Store.YES,
+                          Field.Index.UN_TOKENIZED));
+        doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                          Field.Index.NO));
+        doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                          Field.Index.TOKENIZED));
+        doc.add(new Field("city", text[i], Field.Store.YES,
+                          Field.Index.TOKENIZED));
+        modifier.addDocument(doc);
+      }
+
+      // Now we try to flush the docs to disk
+      // in the ac case, this will work fine because it already hit the exception
+      // and we restarted the trans, so the buffered docs have already been incorporated anyway
+      // so we manually rollback (rather than part of the except handling)
+      // in the !ac case, this will trigger the failure and we'll rollback
+
+      try {
+
+        modifier.flush();
+
+        assert autoCommit;
+        
+        modifier.rollbackTransaction();
+
+      } catch (IOException ioe) {
+        modifier.rollbackTransaction();
+      }
+
+      if (!autoCommit) {
+        modifier.close();
+      }
+
+      hitCount = getHitCount(dir, term);
+
+      // the final result has to have a hit count of zero
+      // we've added to docs that hit
+      // the first is gone because of the delete (which better not have gotten lost)
+      // and the second because we rolledback the trans that contained the add
+
+      assertEquals(0, hitCount);
+
+      if (autoCommit) {
+        modifier.close();
+      }
+      dir.close();
+    }
+  }
+
   private String arrayToString(String[] l) {
     String s = "";
     for (int i = 0; i < l.length; i++) {
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 553298)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -239,6 +239,8 @@
 
   private SegmentInfos localRollbackSegmentInfos;      // segmentInfos we will fallback to if the commit fails
   private boolean localAutoCommit;                // saved autoCommit during local transaction
+  private int localNumBufferedDeleteTerms;
+  private HashMap localBufferedDeleteTerms;
   private boolean autoCommit = true;              // false if we should commit only on close
 
   SegmentInfos segmentInfos = new SegmentInfos();       // the segments
@@ -1332,9 +1334,11 @@
    * commitTransaction() or rollbackTransaction() to finish
    * the transaction.
    */
-  private void startTransaction() throws IOException {
+  void startTransaction() throws IOException {
     localRollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
     localAutoCommit = autoCommit;
+    localNumBufferedDeleteTerms = numBufferedDeleteTerms;
+    localBufferedDeleteTerms = bufferedDeleteTerms;
     if (localAutoCommit) {
       flush();
       // Turn off auto-commit during our local transaction:
@@ -1349,7 +1353,7 @@
    * Rolls back the transaction and restores state to where
    * we were at the start.
    */
-  private void rollbackTransaction() throws IOException {
+  void rollbackTransaction() throws IOException {
 
     // First restore autoCommit in case we hit an exception below:
     autoCommit = localAutoCommit;
@@ -1362,6 +1366,10 @@
     segmentInfos.addAll(localRollbackSegmentInfos);
     localRollbackSegmentInfos = null;
 
+    numBufferedDeleteTerms = localNumBufferedDeleteTerms;
+    bufferedDeleteTerms = localBufferedDeleteTerms;
+    localBufferedDeleteTerms = null;
+    
     // Ask deleter to locate unreferenced files we had
     // created & remove them:
     deleter.checkpoint(segmentInfos, false);
@@ -1378,7 +1386,7 @@
    * segments file and remove and pending deletions we have
    * accumulated during the transaction
    */
-  private void commitTransaction() throws IOException {
+  void commitTransaction() throws IOException {
 
     // First restore autoCommit in case we hit an exception below:
     autoCommit = localAutoCommit;
@@ -1433,6 +1441,8 @@
       deleter.checkpoint(segmentInfos, false);
       deleter.refresh();
 
+      // okay to clear; not rollbackable anyway
+      // (see other comments at rollback points)
       bufferedDeleteTerms.clear();
       numBufferedDeleteTerms = 0;
 
@@ -1899,6 +1909,9 @@
 
         SegmentInfos rollback = null;
 
+        HashMap saveBufferedDeleteTerms = null;
+        int saveNumBufferedDeleteTerms = 0;
+
         if (flushDeletes)
           rollback = (SegmentInfos) segmentInfos.clone();
 
@@ -1933,6 +1946,8 @@
             // buffer deletes longer and then flush them to
             // multiple flushed segments, when
             // autoCommit=false
+            saveBufferedDeleteTerms = bufferedDeleteTerms;
+            saveNumBufferedDeleteTerms = numBufferedDeleteTerms;
             applyDeletes(flushDocs);
             doAfterFlush();
           }
@@ -1947,6 +1962,12 @@
               // SegmentInfo instances:
               segmentInfos.clear();
               segmentInfos.addAll(rollback);
+
+              if (saveBufferedDeleteTerms != null) {
+                numBufferedDeleteTerms = saveNumBufferedDeleteTerms;
+                bufferedDeleteTerms = saveBufferedDeleteTerms;
+              }
+              
             } else {
               // Remove segment we added, if any:
               if (newSegment != null && 
@@ -2321,7 +2342,13 @@
       }
 
       // Clean up bufferedDeleteTerms.
-      bufferedDeleteTerms.clear();
+
+      // Rollbacks of buffered deletes are based on restoring the old
+      // map, so don't modify this one. Rare enough that the gc
+      // overhead is almost certainly lower than the alternate, which
+      // would be clone to support rollback.
+
+      bufferedDeleteTerms = new HashMap();
       numBufferedDeleteTerms = 0;
     }
   }
