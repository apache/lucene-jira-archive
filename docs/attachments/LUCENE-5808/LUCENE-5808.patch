Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java	(working copy)
@@ -26,7 +26,7 @@
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat; // javadocs
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
@@ -53,7 +53,7 @@
 //   - build depth-N prefix hash?
 //   - or: longer dense skip lists than just next byte?
 
-/** Wraps {@link Lucene41PostingsFormat} format for on-disk
+/** Wraps {@link Lucene410PostingsFormat} format for on-disk
  *  storage, but then at read time loads and stores all
  *  terms & postings directly in RAM as byte[], int[].
  *
@@ -101,12 +101,12 @@
   
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return PostingsFormat.forName("Lucene41").fieldsConsumer(state);
+    return PostingsFormat.forName("Lucene410").fieldsConsumer(state);
   }
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    FieldsProducer postings = PostingsFormat.forName("Lucene41").fieldsProducer(state);
+    FieldsProducer postings = PostingsFormat.forName("Lucene410").fieldsProducer(state);
     if (state.context.context != IOContext.Context.MERGE) {
       FieldsProducer loadedPostings;
       try {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing410PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing410PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing410PostingsFormat.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsBaseFormat;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
@@ -32,20 +33,20 @@
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.util.IOUtils;
 
-/** FSTOrd + Pulsing41
+/** FSTOrd + Pulsing410
  *  @lucene.experimental */
 
-public class FSTOrdPulsing41PostingsFormat extends PostingsFormat {
+public class FSTOrdPulsing410PostingsFormat extends PostingsFormat {
   private final PostingsBaseFormat wrappedPostingsBaseFormat;
   private final int freqCutoff;
 
-  public FSTOrdPulsing41PostingsFormat() {
+  public FSTOrdPulsing410PostingsFormat() {
     this(1);
   }
   
-  public FSTOrdPulsing41PostingsFormat(int freqCutoff) {
-    super("FSTOrdPulsing41");
-    this.wrappedPostingsBaseFormat = new Lucene41PostingsBaseFormat();
+  public FSTOrdPulsing410PostingsFormat(int freqCutoff) {
+    super("FSTOrdPulsing410");
+    this.wrappedPostingsBaseFormat = new Lucene410PostingsBaseFormat();
     this.freqCutoff = freqCutoff;
   }
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing41PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing41PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdPulsing41PostingsFormat.java	(working copy)
@@ -1,88 +0,0 @@
-package org.apache.lucene.codecs.memory;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsBaseFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
-import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
-import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.IOUtils;
-
-/** FSTOrd + Pulsing41
- *  @lucene.experimental */
-
-public class FSTOrdPulsing41PostingsFormat extends PostingsFormat {
-  private final PostingsBaseFormat wrappedPostingsBaseFormat;
-  private final int freqCutoff;
-
-  public FSTOrdPulsing41PostingsFormat() {
-    this(1);
-  }
-  
-  public FSTOrdPulsing41PostingsFormat(int freqCutoff) {
-    super("FSTOrdPulsing41");
-    this.wrappedPostingsBaseFormat = new Lucene41PostingsBaseFormat();
-    this.freqCutoff = freqCutoff;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = null;
-    PostingsWriterBase pulsingWriter = null;
-
-    boolean success = false;
-    try {
-      docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
-      pulsingWriter = new PulsingPostingsWriter(state, freqCutoff, docsWriter);
-      FieldsConsumer ret = new FSTOrdTermsWriter(state, pulsingWriter);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsWriter, pulsingWriter);
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase docsReader = null;
-    PostingsReaderBase pulsingReader = null;
-    boolean success = false;
-    try {
-      docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
-      pulsingReader = new PulsingPostingsReader(state, docsReader);
-      FieldsProducer ret = new FSTOrdTermsReader(state, pulsingReader);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsReader, pulsingReader);
-      }
-    }
-  }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing410PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing410PostingsFormat.java	(revision 1608464)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing410PostingsFormat.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsBaseFormat;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
@@ -32,21 +33,21 @@
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.util.IOUtils;
 
-/** FST + Pulsing41, test only, since
+/** FST + Pulsing410, test only, since
  *  FST does no delta encoding here!
  *  @lucene.experimental */
 
-public class FSTPulsing41PostingsFormat extends PostingsFormat {
+public class FSTPulsing410PostingsFormat extends PostingsFormat {
   private final PostingsBaseFormat wrappedPostingsBaseFormat;
   private final int freqCutoff;
 
-  public FSTPulsing41PostingsFormat() {
+  public FSTPulsing410PostingsFormat() {
     this(1);
   }
   
-  public FSTPulsing41PostingsFormat(int freqCutoff) {
-    super("FSTPulsing41");
-    this.wrappedPostingsBaseFormat = new Lucene41PostingsBaseFormat();
+  public FSTPulsing410PostingsFormat(int freqCutoff) {
+    super("FSTPulsing410");
+    this.wrappedPostingsBaseFormat = new Lucene410PostingsBaseFormat();
     this.freqCutoff = freqCutoff;
   }
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing41PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing41PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTPulsing41PostingsFormat.java	(working copy)
@@ -1,89 +0,0 @@
-package org.apache.lucene.codecs.memory;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsBaseFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
-import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
-import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.IOUtils;
-
-/** FST + Pulsing41, test only, since
- *  FST does no delta encoding here!
- *  @lucene.experimental */
-
-public class FSTPulsing41PostingsFormat extends PostingsFormat {
-  private final PostingsBaseFormat wrappedPostingsBaseFormat;
-  private final int freqCutoff;
-
-  public FSTPulsing41PostingsFormat() {
-    this(1);
-  }
-  
-  public FSTPulsing41PostingsFormat(int freqCutoff) {
-    super("FSTPulsing41");
-    this.wrappedPostingsBaseFormat = new Lucene41PostingsBaseFormat();
-    this.freqCutoff = freqCutoff;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = null;
-    PostingsWriterBase pulsingWriter = null;
-
-    boolean success = false;
-    try {
-      docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
-      pulsingWriter = new PulsingPostingsWriter(state, freqCutoff, docsWriter);
-      FieldsConsumer ret = new FSTTermsWriter(state, pulsingWriter);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsWriter, pulsingWriter);
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase docsReader = null;
-    PostingsReaderBase pulsingReader = null;
-    boolean success = false;
-    try {
-      docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
-      pulsingReader = new PulsingPostingsReader(state, docsReader);
-      FieldsProducer ret = new FSTTermsReader(state, pulsingReader);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsReader, pulsingReader);
-      }
-    }
-  }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing410PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing410PostingsFormat.java	(revision 1608464)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing410PostingsFormat.java	(working copy)
@@ -18,28 +18,28 @@
  */
 
 import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsBaseFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 
 /**
- * Concrete pulsing implementation over {@link Lucene41PostingsFormat}.
+ * Concrete pulsing implementation over {@link Lucene410PostingsFormat}.
  * 
  * @lucene.experimental
  */
-public class Pulsing41PostingsFormat extends PulsingPostingsFormat {
+public class Pulsing410PostingsFormat extends PulsingPostingsFormat {
 
-  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat() {
+  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene410" format. */
+  public Pulsing410PostingsFormat() {
     this(1);
   }
 
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat(int freqCutoff) {
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene410" format. */
+  public Pulsing410PostingsFormat(int freqCutoff) {
     this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
   }
 
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super("Pulsing41", new Lucene41PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene410" format. */
+  public Pulsing410PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
+    super("Pulsing410", new Lucene410PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
   }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing41PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing41PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing41PostingsFormat.java	(working copy)
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-
-/**
- * Concrete pulsing implementation over {@link Lucene41PostingsFormat}.
- * 
- * @lucene.experimental
- */
-public class Pulsing41PostingsFormat extends PulsingPostingsFormat {
-
-  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat() {
-    this(1);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat(int freqCutoff) {
-    this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene41" format. */
-  public Pulsing41PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super("Pulsing41", new Lucene41PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
-  }
-}
Index: lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
===================================================================
--- lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(revision 1608975)
+++ lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(working copy)
@@ -13,12 +13,12 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat
+org.apache.lucene.codecs.pulsing.Pulsing410PostingsFormat
 org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat
 org.apache.lucene.codecs.memory.MemoryPostingsFormat
 org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat
 org.apache.lucene.codecs.memory.DirectPostingsFormat
-org.apache.lucene.codecs.memory.FSTPulsing41PostingsFormat
-org.apache.lucene.codecs.memory.FSTOrdPulsing41PostingsFormat
+org.apache.lucene.codecs.memory.FSTPulsing410PostingsFormat
+org.apache.lucene.codecs.memory.FSTOrdPulsing410PostingsFormat
 org.apache.lucene.codecs.memory.FSTPostingsFormat
 org.apache.lucene.codecs.memory.FSTOrdPostingsFormat
Index: lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java	(working copy)
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds;
+import org.apache.lucene.codecs.lucene410ords.Lucene410WithOrds;
 import org.apache.lucene.index.BasePostingsFormatTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -26,7 +26,7 @@
  * Basic tests of a PF using FixedGap terms dictionary
  */
 public class TestFixedGapPostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene41WithOrds(TestUtil.nextInt(random(), 1, 1000)));
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene410WithOrds(TestUtil.nextInt(random(), 1, 1000)));
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java	(working copy)
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval;
+import org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapFixedInterval;
 import org.apache.lucene.index.BasePostingsFormatTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -26,7 +26,7 @@
  * Basic tests of a PF using VariableGap terms dictionary (fixed interval)
  */
 public class TestVarGapDocFreqIntervalPostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene41VarGapFixedInterval(TestUtil.nextInt(random(), 1, 1000)));
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene410VarGapFixedInterval(TestUtil.nextInt(random(), 1, 1000)));
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java	(working copy)
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval;
+import org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapDocFreqInterval;
 import org.apache.lucene.index.BasePostingsFormatTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -26,7 +26,7 @@
  * Basic tests of a PF using VariableGap terms dictionary (fixed interval, docFreq threshold)
  */
 public class TestVarGapFixedIntervalPostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene41VarGapDocFreqInterval(TestUtil.nextInt(random(), 1, 100), TestUtil.nextInt(random(), 1, 1000)));
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene410VarGapDocFreqInterval(TestUtil.nextInt(random(), 1, 100), TestUtil.nextInt(random(), 1, 1000)));
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/bloom/TestBloomPostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/bloom/TestBloomPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/bloom/TestBloomPostingsFormat.java	(working copy)
@@ -25,7 +25,7 @@
  * Basic tests for BloomPostingsFormat
  */
 public class TestBloomPostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new TestBloomFilteredLucene41Postings());
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new TestBloomFilteredLucene410Postings());
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTOrdPulsing41PostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTOrdPulsing41PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTOrdPulsing41PostingsFormat.java	(working copy)
@@ -25,7 +25,7 @@
  * Tests FSTOrdPulsing41PostingsFormat 
  */
 public class TestFSTOrdPulsing41PostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new FSTOrdPulsing41PostingsFormat());
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new FSTOrdPulsing410PostingsFormat());
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTPulsing41PostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTPulsing41PostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/memory/TestFSTPulsing41PostingsFormat.java	(working copy)
@@ -25,7 +25,7 @@
  * Tests FSTPulsing41PostingsFormat 
  */
 public class TestFSTPulsing41PostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new FSTPulsing41PostingsFormat());
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new FSTPulsing410PostingsFormat());
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java	(working copy)
@@ -50,7 +50,7 @@
 public class Test10KPulsings extends LuceneTestCase {
   public void test10kPulsed() throws Exception {
     // we always run this test with pulsing codec.
-    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing41PostingsFormat(1));
+    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing410PostingsFormat(1));
     
     File f = createTempDir("10kpulsed");
     BaseDirectoryWrapper dir = newFSDirectory(f);
@@ -101,7 +101,7 @@
   public void test10kNotPulsed() throws Exception {
     // we always run this test with pulsing codec.
     int freqCutoff = TestUtil.nextInt(random(), 1, 10);
-    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing41PostingsFormat(freqCutoff));
+    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing410PostingsFormat(freqCutoff));
     
     File f = createTempDir("10knotpulsed");
     BaseDirectoryWrapper dir = newFSDirectory(f);
Index: lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingPostingsFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingPostingsFormat.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingPostingsFormat.java	(working copy)
@@ -27,7 +27,7 @@
  */
 public class TestPulsingPostingsFormat extends BasePostingsFormatTestCase {
   // TODO: randomize cutoff
-  private final Codec codec = TestUtil.alwaysPostingsFormat(new Pulsing41PostingsFormat());
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new Pulsing410PostingsFormat());
 
   @Override
   protected Codec getCodec() {
Index: lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java	(revision 1608975)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java	(working copy)
@@ -44,7 +44,7 @@
   // TODO: this is a basic test. this thing is complicated, add more
   public void testSophisticatedReuse() throws Exception {
     // we always run this test with pulsing codec.
-    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing41PostingsFormat(1));
+    Codec cp = TestUtil.alwaysPostingsFormat(new Pulsing410PostingsFormat(1));
     Directory dir = newDirectory();
     RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
Index: lucene/core/src/java/org/apache/lucene/codecs/Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(revision 1608975)
+++ lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(working copy)
@@ -119,7 +119,7 @@
     loader.reload(classloader);
   }
   
-  private static Codec defaultCodec = Codec.forName("Lucene49");
+  private static Codec defaultCodec = Codec.forName("Lucene410");
   
   /** expert: returns the default codec used for newly created
    *  {@link IndexWriterConfig}s.
Index: lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java	(revision 1608975)
+++ lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java	(working copy)
@@ -21,13 +21,13 @@
  * A codec that forwards all its method calls to another codec.
  * <p>
  * Extend this class when you need to reuse the functionality of an existing
- * codec. For example, if you want to build a codec that redefines Lucene49's
+ * codec. For example, if you want to build a codec that redefines Lucene410's
  * {@link LiveDocsFormat}:
  * <pre class="prettyprint">
  *   public final class CustomCodec extends FilterCodec {
  *
  *     public CustomCodec() {
- *       super("CustomCodec", new Lucene49Codec());
+ *       super("CustomCodec", new Lucene410Codec());
  *     }
  *
  *     public LiveDocsFormat liveDocsFormat() {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java	(revision 1608975)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java	(working copy)
@@ -50,7 +50,7 @@
  * Therefore, we'll trim df before passing it to the interface. see trim(int)
  *
  */
-final class Lucene41SkipReader extends MultiLevelSkipListReader {
+public final class Lucene41SkipReader extends MultiLevelSkipListReader {
   // private boolean DEBUG = Lucene41PostingsReader.DEBUG;
   private final int blockSize;
 
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java	(revision 1608975)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java	(working copy)
@@ -43,7 +43,7 @@
  * 4. start offset.
  *
  */
-final class Lucene41SkipWriter extends MultiLevelSkipListWriter {
+public final class Lucene41SkipWriter extends MultiLevelSkipListWriter {
   // private boolean DEBUG = Lucene41PostingsReader.DEBUG;
   
   private int[] lastSkipDoc;
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsAndPositionsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsAndPositionsEnum.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsAndPositionsEnum.java	(working copy)
@@ -0,0 +1,142 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter.IntBlockTermState;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+class BlockDocsAndPositionsEnum extends BlockDocsEnum {
+  
+  final int[] posDeltaBuffer = new int[BLOCK_SIZE];
+  int posBufferUpto;
+  
+  BlockDocsAndPositionsEnum(FieldInfo fieldInfo, IndexInput docIn, IndexInput posIn) throws IOException {
+    super(fieldInfo, docIn, posIn);
+  }
+  
+  DocsAndPositionsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
+    return super.reset(liveDocs, termState, true);
+  }
+  
+  @Override
+  void skipperMoved() throws IOException {
+    super.skipperMoved();
+    //posBufferUpto = BLOCK_SIZE;
+  }
+  
+  @Override
+  public int nextPosition() throws IOException {
+    if (posPendingFP != -1) {
+      posIn.seek(posPendingFP);
+      posPendingFP = -1;
+
+      // Force buffer refill:
+      posBufferUpto = BLOCK_SIZE;
+    }
+
+    if (posPendingCount > freq) {
+      skipPositions();
+      posPendingCount = freq;
+    }
+
+    if (posBufferUpto == BLOCK_SIZE) {
+      refillPositions();
+      posBufferUpto = 0;
+    }
+    position += posDeltaBuffer[posBufferUpto++];
+    posPendingCount--;
+    return position;
+  }
+  
+  void refillPositions() throws IOException {
+    if (posIn.getFilePointer() == lastPosBlockFP) {
+      final int count = (int) (totalTermFreq % BLOCK_SIZE);
+      int payloadLength = 0;
+      for (int i = 0; i < count; i++) {
+        int code = posIn.readVInt();
+        if (indexHasPayloads) {
+          if ((code & 1) != 0) {
+            payloadLength = posIn.readVInt();
+          }
+          posDeltaBuffer[i] = code >>> 1;
+          if (payloadLength != 0) {
+            posIn.seek(posIn.getFilePointer() + payloadLength);
+          }
+        } else {
+          posDeltaBuffer[i] = code;
+        }
+        if (indexHasOffsets) {
+          if ((posIn.readVInt() & 1) != 0) {
+            // offset length changed
+            posIn.readVInt();
+          }
+        }
+      }
+    } else {
+      ForUtil.readBlock(posIn, encoded, posDeltaBuffer);
+    }
+  }
+
+  // TODO: in theory we could avoid loading frq block
+  // when not needed, ie, use skip data to load how far to
+  // seek the pos pointer ... instead of having to load frq
+  // blocks only to sum up how many positions to skip
+  void skipPositions() throws IOException {
+    // Skip positions now:
+    int toSkip = posPendingCount - freq;
+
+    final int leftInBlock = BLOCK_SIZE - posBufferUpto;
+    if (toSkip < leftInBlock) {
+      posBufferUpto += toSkip;
+    } else {
+      toSkip -= leftInBlock;
+      while (toSkip >= BLOCK_SIZE) {
+        assert posIn.getFilePointer() != lastPosBlockFP;
+        ForUtil.skipBlock(posIn);
+        toSkip -= BLOCK_SIZE;
+      }
+      refillPositions();
+      posBufferUpto = toSkip;
+    }
+
+    position = 0;
+  }
+
+  @Override
+  public int startOffset() {
+    return -1;
+  }
+
+  @Override
+  public int endOffset() {
+    return -1;
+  }
+
+  @Override
+  public BytesRef getPayload() {
+    return null;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsAndPositionsEnum.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsEnum.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsEnum.java	(working copy)
@@ -0,0 +1,349 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene410.ForUtil.MAX_ENCODED_SIZE;
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.lucene41.Lucene41SkipReader;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter.IntBlockTermState;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+class BlockDocsEnum extends DocsAndPositionsEnum {
+  
+  // 'static' per-field settings
+  
+  /** true if the index has frequencies stored for the field */
+  final boolean indexHasFreq; 
+  /** true if the index has positions stored for the field */
+  final boolean indexHasPos;
+  /** true if the index has offsets stored for the field */
+  final boolean indexHasOffsets;
+  /** true if the index has payloads stored for the field */
+  final boolean indexHasPayloads;
+  
+  /** reference to the original .doc input for the parent postingsreader */
+  final IndexInput startDocIn;
+  
+  final int[] docDeltaBuffer = new int[BLOCK_SIZE];
+  final int[] freqBuffer =     new int[BLOCK_SIZE];
+  int docBufferUpto;
+  
+  // per-use settings
+  
+  /** current clone of the .doc input */
+  IndexInput docIn;
+  /** current clone of the .pos input */
+  final IndexInput posIn;
+  /** true if the caller actually needs frequencies */
+  boolean needsFreq;
+  /** number of docs in this posting list */
+  int docFreq;
+  /** sum of freqs in this posting list (or docFreq when omitted) */
+  long totalTermFreq;
+  /** how many docs we've read */
+  int docUpto;
+  /** doc we last read */
+  int doc;
+  /** accumulator for doc deltas */
+  int accum;
+  /** freq we last read */
+  int freq;
+  /** docid when there is a single pulsed posting, otherwise -1 */
+  int singletonDocID;
+  
+  /** buffer for reading raw compressed data */
+  byte encoded[];
+  
+  Lucene41SkipReader skipper;
+  boolean skipped;
+  
+  /** Where this term's postings start in the .doc file */
+  long docTermStartFP;
+
+  /** Where this term's skip data starts (after
+   * docTermStartFP) in the .doc file (or -1 if there is
+   * no skip data for this term) */
+  long skipOffset;
+
+  /** docID for next skip point, we won't use skipper if 
+   * target docID is not larger than this */
+  int nextSkipDoc;
+
+  Bits liveDocs;
+  
+  int position;                             // current position
+  // how many positions "behind" we are; nextPosition must
+  // skip these to "catch up":
+  int posPendingCount;
+
+  // Lazy pos seek: if != -1 then we must seek to this FP
+  // before reading positions:
+  long posPendingFP;
+  
+  // Where this term's postings start in the .pos file:
+  long posTermStartFP;
+
+  // Where this term's payloads/offsets start in the .pay
+  // file:
+  long payTermStartFP;
+
+  // File pointer where the last (vInt encoded) pos delta
+  // block is.  We need this to know whether to bulk
+  // decode vs vInt decode the block:
+  long lastPosBlockFP;
+  
+  BlockDocsEnum(FieldInfo fieldInfo, IndexInput docIn, IndexInput posIn) throws IOException {
+    this.startDocIn = docIn;
+    this.docIn = null;
+    if (posIn == null) {
+      this.posIn = null;
+    } else {
+      this.posIn = posIn.clone();
+    }
+    indexHasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
+    indexHasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    indexHasPayloads = fieldInfo.hasPayloads();
+    if (!indexHasFreq) {
+      Arrays.fill(freqBuffer, 1);
+    }
+  }
+  
+  final boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
+    return docIn == startDocIn &&
+      indexHasFreq == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0) &&
+      indexHasPos == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) &&
+      indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
+      indexHasPayloads == fieldInfo.hasPayloads();
+  }
+  
+  BlockDocsEnum reset(Bits liveDocs, IntBlockTermState termState, boolean needsFreq) throws IOException {
+    this.liveDocs = liveDocs;
+    docFreq = termState.docFreq;
+    docTermStartFP = termState.docStartFP;
+    posTermStartFP = termState.posStartFP;
+    payTermStartFP = termState.payStartFP;
+    skipOffset = termState.skipOffset;
+    totalTermFreq = indexHasFreq ? termState.totalTermFreq : docFreq;
+    singletonDocID = termState.singletonDocID;
+    if (docFreq > 1) {
+      // lazy init
+      if (docIn == null) {
+        docIn = startDocIn.clone();
+      }
+      docIn.seek(docTermStartFP);
+    }
+    
+    if (posIn != null) {
+      if (totalTermFreq >= BLOCK_SIZE && encoded == null) {
+        encoded = new byte[MAX_ENCODED_SIZE];
+      }
+      if (totalTermFreq < BLOCK_SIZE) {
+        lastPosBlockFP = posTermStartFP;
+      } else if (totalTermFreq == BLOCK_SIZE) {
+        lastPosBlockFP = -1;
+      } else {
+        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
+      }
+    } else if (docFreq >= BLOCK_SIZE && encoded == null) {
+      encoded = new byte[MAX_ENCODED_SIZE];
+    }
+
+    posPendingFP = posTermStartFP;
+    posPendingCount = 0;
+    doc = -1;
+    this.needsFreq = needsFreq;
+    accum = 0;
+    docUpto = 0;
+    if (docFreq > BLOCK_SIZE) {
+      nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
+    } else {
+      nextSkipDoc = NO_MORE_DOCS; // not enough docs for skipping
+    }
+    docBufferUpto = BLOCK_SIZE;
+    skipped = false;
+    return this;
+  }
+
+  @Override
+  public final int docID() {
+    return doc;
+  }
+  
+  @Override
+  public final int freq() {
+    return freq;
+  }
+  
+  @Override
+  public final long cost() {
+    return docFreq;
+  }
+  
+  @Override
+  public final int nextDoc() throws IOException {
+    while (true) {
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+      if (docBufferUpto == BLOCK_SIZE) {
+        refillDocs();
+      }
+      accum += docDeltaBuffer[docBufferUpto];
+      freq = freqBuffer[docBufferUpto];
+      posPendingCount += freq;
+      docBufferUpto++;
+      docUpto++;
+
+      if (liveDocs == null || liveDocs.get(accum)) {
+        position = 0;
+        return doc = accum;
+      }
+    }
+  }
+
+  @Override
+  public final int advance(int target) throws IOException {
+    if (target > nextSkipDoc) {
+      initSkipper();
+
+      final int newDocUpto = skipper.skipTo(target) + 1; 
+
+      if (newDocUpto > docUpto) {
+        docUpto = newDocUpto;
+        skipperMoved();
+      }
+      nextSkipDoc = skipper.getNextSkipDoc();
+    }
+    
+    if (docUpto == docFreq) {
+      return doc = NO_MORE_DOCS;
+    }
+    if (docBufferUpto == BLOCK_SIZE) {
+      refillDocs();
+    }
+    
+    return scanTo(target);
+  }
+  
+  final void refillDocs() throws IOException {
+    final int left = docFreq - docUpto;
+
+    if (left >= BLOCK_SIZE) {
+      ForUtil.readBlock(docIn, encoded, docDeltaBuffer);
+
+      if (indexHasFreq) {
+        if (needsFreq) {
+          ForUtil.readBlock(docIn, encoded, freqBuffer);
+        } else {
+          ForUtil.skipBlock(docIn); // skip over freqs
+        }
+      }
+    } else if (docFreq == 1) {
+      docDeltaBuffer[0] = singletonDocID;
+      freqBuffer[0] = (int) totalTermFreq;
+    } else {
+      // Read vInts:
+      ForUtil.readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, indexHasFreq);
+    }
+    docBufferUpto = 0;
+  }  
+  
+  void initSkipper() {
+    if (skipper == null) {
+      // Lazy init: first time this enum has ever been used for skipping
+      skipper = new Lucene41SkipReader(docIn.clone(),
+                                       Lucene410PostingsWriter.maxSkipLevels,
+                                       BLOCK_SIZE,
+                                       indexHasPos,
+                                       indexHasOffsets,
+                                       indexHasPayloads);
+    }
+
+    if (!skipped) {
+      assert skipOffset != -1;
+      // This is the first time this enum has skipped
+      // since reset() was called; load the skip data:
+      skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
+      skipped = true;
+    }
+  }
+  
+  void skipperMoved() throws IOException {
+    // Force to read next block
+    docBufferUpto = BLOCK_SIZE;
+    accum = skipper.getDoc();
+    docIn.seek(skipper.getDocPointer());
+    posPendingFP = skipper.getPosPointer();
+    posPendingCount = skipper.getPosBufferUpto();
+  }
+  
+  final int scanTo(int target) throws IOException {
+    // this is an inlined/pared down version of nextDoc()
+    while (true) {
+      accum += docDeltaBuffer[docBufferUpto];
+      freq = freqBuffer[docBufferUpto];
+      posPendingCount += freq;
+      docBufferUpto++;
+      docUpto++;
+
+      if (accum >= target) {
+        break;
+      }
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+    }
+
+    if (liveDocs == null || liveDocs.get(accum)) {
+      position = 0;
+      return doc = accum;
+    } else {
+      return nextDoc();
+    }
+  }
+  
+  @Override
+  public int nextPosition() throws IOException {
+    return -1;
+  }
+
+  @Override
+  public int startOffset() throws IOException {
+    return -1;
+  }
+
+  @Override
+  public int endOffset() throws IOException {
+    return -1;
+  }
+
+  @Override
+  public BytesRef getPayload() throws IOException {
+    return null;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/BlockDocsEnum.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/EverythingEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/EverythingEnum.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/EverythingEnum.java	(working copy)
@@ -0,0 +1,510 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene410.ForUtil.MAX_ENCODED_SIZE;
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene41.Lucene41SkipReader;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter.IntBlockTermState;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+// Also handles payloads + offsets
+final class EverythingEnum extends DocsAndPositionsEnum {
+  
+  private byte[] encoded;
+
+  private final int[] docDeltaBuffer = new int[BLOCK_SIZE];
+  private final int[] freqBuffer     = new int[BLOCK_SIZE];
+  private final int[] posDeltaBuffer = new int[BLOCK_SIZE];
+
+  private final int[] payloadLengthBuffer;
+  private final int[] offsetStartDeltaBuffer;
+  private final int[] offsetLengthBuffer;
+
+  private byte[] payloadBytes;
+  private int payloadByteUpto;
+  private int payloadLength;
+
+  private int lastStartOffset;
+  private int startOffset;
+  private int endOffset;
+
+  private int docBufferUpto;
+  private int posBufferUpto;
+
+  private Lucene41SkipReader skipper;
+  private boolean skipped;
+
+  final IndexInput startDocIn;
+
+  IndexInput docIn;
+  final IndexInput posIn;
+  final IndexInput payIn;
+  final BytesRef payload;
+
+  final boolean indexHasOffsets;
+  final boolean indexHasPayloads;
+
+  private int docFreq;                              // number of docs in this posting list
+  private long totalTermFreq;                       // number of positions in this posting list
+  private int docUpto;                              // how many docs we've read
+  private int doc;                                  // doc we last read
+  private int accum;                                // accumulator for doc deltas
+  private int freq;                                 // freq we last read
+  private int position;                             // current position
+
+  // how many positions "behind" we are; nextPosition must
+  // skip these to "catch up":
+  private int posPendingCount;
+
+  // Lazy pos seek: if != -1 then we must seek to this FP
+  // before reading positions:
+  private long posPendingFP;
+
+  // Lazy pay seek: if != -1 then we must seek to this FP
+  // before reading payloads/offsets:
+  private long payPendingFP;
+
+  // Where this term's postings start in the .doc file:
+  private long docTermStartFP;
+
+  // Where this term's postings start in the .pos file:
+  private long posTermStartFP;
+
+  // Where this term's payloads/offsets start in the .pay
+  // file:
+  private long payTermStartFP;
+
+  // File pointer where the last (vInt encoded) pos delta
+  // block is.  We need this to know whether to bulk
+  // decode vs vInt decode the block:
+  private long lastPosBlockFP;
+
+  // Where this term's skip data starts (after
+  // docTermStartFP) in the .doc file (or -1 if there is
+  // no skip data for this term):
+  private long skipOffset;
+
+  private int nextSkipDoc;
+
+  private Bits liveDocs;
+  
+  private boolean needsOffsets; // true if we actually need offsets
+  private boolean needsPayloads; // true if we actually need payloads
+  private int singletonDocID; // docid when there is a single pulsed posting, otherwise -1
+  
+  public EverythingEnum(FieldInfo fieldInfo, IndexInput docIn, IndexInput posIn, IndexInput payIn) throws IOException {
+    this.startDocIn = docIn;
+    this.docIn = null;
+    this.posIn = posIn.clone();
+    this.payIn = payIn.clone();
+    indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    if (indexHasOffsets) {
+      offsetStartDeltaBuffer = new int[BLOCK_SIZE];
+      offsetLengthBuffer = new int[BLOCK_SIZE];
+    } else {
+      offsetStartDeltaBuffer = null;
+      offsetLengthBuffer = null;
+      startOffset = -1;
+      endOffset = -1;
+    }
+
+    indexHasPayloads = fieldInfo.hasPayloads();
+    if (indexHasPayloads) {
+      payloadLengthBuffer = new int[BLOCK_SIZE];
+      payloadBytes = new byte[128];
+      payload = new BytesRef();
+    } else {
+      payloadLengthBuffer = null;
+      payloadBytes = null;
+      payload = null;
+    }
+  }
+
+  public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
+    return docIn == startDocIn &&
+      indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
+      indexHasPayloads == fieldInfo.hasPayloads();
+  }
+  
+  public EverythingEnum reset(Bits liveDocs, IntBlockTermState termState, int flags) throws IOException {
+    this.liveDocs = liveDocs;
+    docFreq = termState.docFreq;
+    docTermStartFP = termState.docStartFP;
+    posTermStartFP = termState.posStartFP;
+    payTermStartFP = termState.payStartFP;
+    skipOffset = termState.skipOffset;
+    totalTermFreq = termState.totalTermFreq;
+    singletonDocID = termState.singletonDocID;
+    if (docFreq > 1) {
+      if (docIn == null) {
+        // lazy init
+        docIn = startDocIn.clone();
+      }
+      docIn.seek(docTermStartFP);
+    }
+    if (totalTermFreq >= BLOCK_SIZE && encoded == null) {
+      encoded = new byte[MAX_ENCODED_SIZE];
+    }
+    posPendingFP = posTermStartFP;
+    payPendingFP = payTermStartFP;
+    posPendingCount = 0;
+    if (totalTermFreq < BLOCK_SIZE) {
+      lastPosBlockFP = posTermStartFP;
+    } else if (totalTermFreq == BLOCK_SIZE) {
+      lastPosBlockFP = -1;
+    } else {
+      lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
+    }
+
+    this.needsOffsets = (flags & DocsAndPositionsEnum.FLAG_OFFSETS) != 0;
+    this.needsPayloads = (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) != 0;
+
+    doc = -1;
+    accum = 0;
+    docUpto = 0;
+    nextSkipDoc = BLOCK_SIZE - 1;
+    docBufferUpto = BLOCK_SIZE;
+    skipped = false;
+    return this;
+  }
+  
+  @Override
+  public int freq() throws IOException {
+    return freq;
+  }
+
+  @Override
+  public int docID() {
+    return doc;
+  }
+
+  private void refillDocs() throws IOException {
+    final int left = docFreq - docUpto;
+    assert left > 0;
+
+    if (left >= BLOCK_SIZE) {
+      ForUtil.readBlock(docIn, encoded, docDeltaBuffer);
+      ForUtil.readBlock(docIn, encoded, freqBuffer);
+    } else if (docFreq == 1) {
+      docDeltaBuffer[0] = singletonDocID;
+      freqBuffer[0] = (int) totalTermFreq;
+    } else {
+      ForUtil.readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
+    }
+    docBufferUpto = 0;
+  }
+  
+  private void refillPositions() throws IOException {
+    if (posIn.getFilePointer() == lastPosBlockFP) {
+      final int count = (int) (totalTermFreq % BLOCK_SIZE);
+      int payloadLength = 0;
+      int offsetLength = 0;
+      payloadByteUpto = 0;
+      for(int i=0;i<count;i++) {
+        int code = posIn.readVInt();
+        if (indexHasPayloads) {
+          if ((code & 1) != 0) {
+            payloadLength = posIn.readVInt();
+          }
+          payloadLengthBuffer[i] = payloadLength;
+          posDeltaBuffer[i] = code >>> 1;
+          if (payloadLength != 0) {
+            if (payloadByteUpto + payloadLength > payloadBytes.length) {
+              payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payloadLength);
+            }
+            posIn.readBytes(payloadBytes, payloadByteUpto, payloadLength);
+            payloadByteUpto += payloadLength;
+          }
+        } else {
+          posDeltaBuffer[i] = code;
+        }
+
+        if (indexHasOffsets) {
+          int deltaCode = posIn.readVInt();
+          if ((deltaCode & 1) != 0) {
+            offsetLength = posIn.readVInt();
+          }
+          offsetStartDeltaBuffer[i] = deltaCode >>> 1;
+          offsetLengthBuffer[i] = offsetLength;
+        }
+      }
+      payloadByteUpto = 0;
+    } else {
+      ForUtil.readBlock(posIn, encoded, posDeltaBuffer);
+
+      if (indexHasPayloads) {
+        if (needsPayloads) {
+          ForUtil.readBlock(payIn, encoded, payloadLengthBuffer);
+          int numBytes = payIn.readVInt();
+          if (numBytes > payloadBytes.length) {
+            payloadBytes = ArrayUtil.grow(payloadBytes, numBytes);
+          }
+          payIn.readBytes(payloadBytes, 0, numBytes);
+        } else {
+          // this works, because when writing a vint block we always force the first length to be written
+          ForUtil.skipBlock(payIn); // skip over lengths
+          int numBytes = payIn.readVInt(); // read length of payloadBytes
+          payIn.seek(payIn.getFilePointer() + numBytes); // skip over payloadBytes
+        }
+        payloadByteUpto = 0;
+      }
+
+      if (indexHasOffsets) {
+        if (needsOffsets) {
+          ForUtil.readBlock(payIn, encoded, offsetStartDeltaBuffer);
+          ForUtil.readBlock(payIn, encoded, offsetLengthBuffer);
+        } else {
+          // this works, because when writing a vint block we always force the first length to be written
+          ForUtil.skipBlock(payIn); // skip over starts
+          ForUtil.skipBlock(payIn); // skip over lengths
+        }
+      }
+    }
+  }
+
+  @Override
+  public int nextDoc() throws IOException {
+    while (true) {
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+      if (docBufferUpto == BLOCK_SIZE) {
+        refillDocs();
+      }
+      accum += docDeltaBuffer[docBufferUpto];
+      freq = freqBuffer[docBufferUpto];
+      posPendingCount += freq;
+      docBufferUpto++;
+      docUpto++;
+
+      if (liveDocs == null || liveDocs.get(accum)) {
+        doc = accum;
+        position = 0;
+        lastStartOffset = 0;
+        return doc;
+      }
+    }
+  }
+  
+  @Override
+  public int advance(int target) throws IOException {
+    // TODO: make frq block load lazy/skippable
+
+    if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
+
+      if (skipper == null) {
+        // Lazy init: first time this enum has ever been used for skipping
+        skipper = new Lucene41SkipReader(docIn.clone(),
+                                      Lucene410PostingsWriter.maxSkipLevels,
+                                      BLOCK_SIZE,
+                                      true,
+                                      indexHasOffsets,
+                                      indexHasPayloads);
+      }
+
+      if (!skipped) {
+        assert skipOffset != -1;
+        // This is the first time this enum has skipped
+        // since reset() was called; load the skip data:
+        skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
+        skipped = true;
+      }
+
+      final int newDocUpto = skipper.skipTo(target) + 1; 
+
+      if (newDocUpto > docUpto) {
+        // Skipper moved
+        assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
+        docUpto = newDocUpto;
+
+        // Force to read next block
+        docBufferUpto = BLOCK_SIZE;
+        accum = skipper.getDoc();
+        docIn.seek(skipper.getDocPointer());
+        posPendingFP = skipper.getPosPointer();
+        payPendingFP = skipper.getPayPointer();
+        posPendingCount = skipper.getPosBufferUpto();
+        lastStartOffset = 0; // new document
+        payloadByteUpto = skipper.getPayloadByteUpto();
+      }
+      nextSkipDoc = skipper.getNextSkipDoc();
+    }
+    if (docUpto == docFreq) {
+      return doc = NO_MORE_DOCS;
+    }
+    if (docBufferUpto == BLOCK_SIZE) {
+      refillDocs();
+    }
+
+    // Now scan:
+    while (true) {
+      accum += docDeltaBuffer[docBufferUpto];
+      freq = freqBuffer[docBufferUpto];
+      posPendingCount += freq;
+      docBufferUpto++;
+      docUpto++;
+
+      if (accum >= target) {
+        break;
+      }
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+    }
+
+    if (liveDocs == null || liveDocs.get(accum)) {
+      position = 0;
+      lastStartOffset = 0;
+      return doc = accum;
+    } else {
+      return nextDoc();
+    }
+  }
+
+  // TODO: in theory we could avoid loading frq block
+  // when not needed, ie, use skip data to load how far to
+  // seek the pos pointer ... instead of having to load frq
+  // blocks only to sum up how many positions to skip
+  private void skipPositions() throws IOException {
+    // Skip positions now:
+    int toSkip = posPendingCount - freq;
+
+    final int leftInBlock = BLOCK_SIZE - posBufferUpto;
+    if (toSkip < leftInBlock) {
+      int end = posBufferUpto + toSkip;
+      while(posBufferUpto < end) {
+        if (indexHasPayloads) {
+          payloadByteUpto += payloadLengthBuffer[posBufferUpto];
+        }
+        posBufferUpto++;
+      }
+    } else {
+      toSkip -= leftInBlock;
+      while(toSkip >= BLOCK_SIZE) {
+        assert posIn.getFilePointer() != lastPosBlockFP;
+        ForUtil.skipBlock(posIn);
+
+        if (indexHasPayloads) {
+          // Skip payloadLength block:
+          ForUtil.skipBlock(payIn);
+
+          // Skip payloadBytes block:
+          int numBytes = payIn.readVInt();
+          payIn.seek(payIn.getFilePointer() + numBytes);
+        }
+
+        if (indexHasOffsets) {
+          ForUtil.skipBlock(payIn);
+          ForUtil.skipBlock(payIn);
+        }
+        toSkip -= BLOCK_SIZE;
+      }
+      refillPositions();
+      payloadByteUpto = 0;
+      posBufferUpto = 0;
+      while(posBufferUpto < toSkip) {
+        if (indexHasPayloads) {
+          payloadByteUpto += payloadLengthBuffer[posBufferUpto];
+        }
+        posBufferUpto++;
+      }
+    }
+
+    position = 0;
+    lastStartOffset = 0;
+  }
+
+  @Override
+  public int nextPosition() throws IOException {
+    if (posPendingFP != -1) {
+      posIn.seek(posPendingFP);
+      posPendingFP = -1;
+
+      if (payPendingFP != -1) {
+        payIn.seek(payPendingFP);
+        payPendingFP = -1;
+      }
+
+      // Force buffer refill:
+      posBufferUpto = BLOCK_SIZE;
+    }
+
+    if (posPendingCount > freq) {
+      skipPositions();
+      posPendingCount = freq;
+    }
+
+    if (posBufferUpto == BLOCK_SIZE) {
+      refillPositions();
+      posBufferUpto = 0;
+    }
+    position += posDeltaBuffer[posBufferUpto];
+
+    if (indexHasPayloads) {
+      payloadLength = payloadLengthBuffer[posBufferUpto];
+      payload.bytes = payloadBytes;
+      payload.offset = payloadByteUpto;
+      payload.length = payloadLength;
+      payloadByteUpto += payloadLength;
+    }
+
+    if (indexHasOffsets) {
+      startOffset = lastStartOffset + offsetStartDeltaBuffer[posBufferUpto];
+      endOffset = startOffset + offsetLengthBuffer[posBufferUpto];
+      lastStartOffset = startOffset;
+    }
+
+    posBufferUpto++;
+    posPendingCount--;
+    return position;
+  }
+
+  @Override
+  public int startOffset() {
+    return startOffset;
+  }
+
+  @Override
+  public int endOffset() {
+    return endOffset;
+  }
+
+  @Override
+  public BytesRef getPayload() {
+    if (payloadLength == 0) {
+      return null;
+    } else {
+      return payload;
+    }
+  }
+  
+  @Override
+  public long cost() {
+    return docFreq;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/EverythingEnum.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/ForUtil.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/ForUtil.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/ForUtil.java	(working copy)
@@ -0,0 +1,408 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+
+/**
+ * Encode all values in normal area with fixed bit width, 
+ * which is determined by the max value in this block.
+ */
+final class ForUtil {
+
+  /**
+   * Special number of bits per value used whenever all values to encode are equal.
+   */
+  private static final int ALL_VALUES_EQUAL = 0;
+
+  /**
+   * Upper limit of the number of bytes that might be required to stored
+   * <code>BLOCK_SIZE</code> encoded values.
+   */
+  static final int MAX_ENCODED_SIZE = 32 << 4;
+
+  /**
+   * Write a block of data (<code>For</code> format).
+   *
+   * @param data     the data to write
+   * @param encoded  a buffer to use to encode data
+   * @param out      the destination output
+   * @throws IOException If there is a low-level I/O error
+   */
+  static void writeBlock(int[] data, byte[] encoded, IndexOutput out) throws IOException {
+    if (isAllEqual(data)) {
+      out.writeByte((byte) ALL_VALUES_EQUAL);
+      out.writeVInt(data[0]);
+    } else {
+      final int numBits = bitsRequired(data);
+      out.writeByte((byte) numBits);
+      int encodedSize = numBits << 4;
+      encode(numBits, data, encoded);
+      out.writeBytes(encoded, encodedSize);
+    }
+  }
+
+  /**
+   * Read the next block of data (<code>For</code> format).
+   *
+   * @param in        the input to use to read data
+   * @param encoded   a buffer that can be used to store encoded data
+   * @param decoded   where to write decoded data
+   * @throws IOException If there is a low-level I/O error
+   */
+  static void readBlock(IndexInput in, byte[] encoded, int[] decoded) throws IOException {
+    final int numBits = in.readByte();
+
+    if (numBits == ALL_VALUES_EQUAL) {
+      final int value = in.readVInt();
+      Arrays.fill(decoded, value);
+    } else {
+      int encodedSize = numBits << 4;
+      in.readBytes(encoded, 0, encodedSize);
+      decode(numBits, encoded, decoded);
+    }
+  }
+  
+  /**
+   * Read values that have been written using variable-length encoding instead of bit-packing.
+   */
+  static void readVIntBlock(IndexInput docIn, int[] docBuffer, int[] freqBuffer, int num, boolean indexHasFreq) throws IOException {
+    if (indexHasFreq) {
+      readVIntDocsFreqs(docIn, docBuffer, freqBuffer, num);
+    } else {
+      readVIntDocs(docIn, docBuffer, num);
+    }
+  }
+  
+  static void readVIntDocsFreqs(IndexInput docIn, int[] docBuffer, int[] freqBuffer, int num) throws IOException {
+    for (int i = 0; i < num; i++) {
+      final int code = docIn.readVInt();
+      docBuffer[i] = code >>> 1;
+      if ((code & 1) != 0) {
+        freqBuffer[i] = 1;
+      } else {
+        freqBuffer[i] = docIn.readVInt();
+      }
+    }
+  }
+  
+  static void readVIntDocs(IndexInput docIn, int[] docBuffer, int num) throws IOException {
+    for (int i = 0; i < num; i++) {
+      docBuffer[i] = docIn.readVInt();
+    }
+  }
+
+  /**
+   * Skip the next block of data.
+   *
+   * @param in      the input where to read data
+   * @throws IOException If there is a low-level I/O error
+   */
+  static void skipBlock(IndexInput in) throws IOException {
+    final int numBits = in.readByte();
+
+    if (numBits == ALL_VALUES_EQUAL) {
+      in.readVInt();
+    } else {
+      final int encodedSize = numBits << 4;
+      in.seek(in.getFilePointer() + encodedSize);
+    }
+  }
+
+  private static boolean isAllEqual(final int[] data) {
+    final int v = data[0];
+    for (int i = 1; i < BLOCK_SIZE; ++i) {
+      if (data[i] != v) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  /**
+   * Compute the number of bits required to serialize any of the longs in
+   * <code>data</code>.
+   */
+  private static int bitsRequired(final int[] data) {
+    long or = 0;
+    for (int i = 0; i < BLOCK_SIZE; ++i) {
+      assert data[i] >= 0;
+      or |= data[i];
+    }
+    return bitsRequired(or);
+  }
+  
+  // NOTE: always decodes 128 values
+  private static void decode1(byte[] blocks, int[] values) {
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 16; iter++) {
+      byte block = blocks[iter];
+      values[valuesOffset++] = (block >>> 7) & 1;
+      values[valuesOffset++] = (block >>> 6) & 1;
+      values[valuesOffset++] = (block >>> 5) & 1;
+      values[valuesOffset++] = (block >>> 4) & 1;
+      values[valuesOffset++] = (block >>> 3) & 1;
+      values[valuesOffset++] = (block >>> 2) & 1;
+      values[valuesOffset++] = (block >>> 1) & 1;
+      values[valuesOffset++] = block & 1;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode2(byte[] blocks, int[] values) {
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 32; iter++) {
+      byte block = blocks[iter];
+      values[valuesOffset++] = (block >>> 6) & 3;
+      values[valuesOffset++] = (block >>> 4) & 3;
+      values[valuesOffset++] = (block >>> 2) & 3;
+      values[valuesOffset++] = block & 3;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode3(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 16; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = byte0 >>> 5;
+      values[valuesOffset++] = (byte0 >>> 2) & 7;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte0 & 3) << 1) | (byte1 >>> 7);
+      values[valuesOffset++] = (byte1 >>> 4) & 7;
+      values[valuesOffset++] = (byte1 >>> 1) & 7;
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte1 & 1) << 2) | (byte2 >>> 6);
+      values[valuesOffset++] = (byte2 >>> 3) & 7;
+      values[valuesOffset++] = byte2 & 7;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode4(byte[] blocks, int[] values) {
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 64; iter++) {
+      byte block = blocks[iter];
+      values[valuesOffset++] = (block >>> 4) & 15;
+      values[valuesOffset++] = block & 15;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode6(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 32; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = byte0 >>> 2;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte0 & 3) << 4) | (byte1 >>> 4);
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte1 & 15) << 2) | (byte2 >>> 6);
+      values[valuesOffset++] = byte2 & 63;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode8(byte[] blocks, int[] values) {
+    for (int iter = 0; iter < 128; iter++) {
+      values[iter] = blocks[iter] & 0xFF;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode10(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 32; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = (byte0 << 2) | (byte1 >>> 6);
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte1 & 63) << 4) | (byte2 >>> 4);
+      int byte3 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte2 & 15) << 6) | (byte3 >>> 2);
+      int byte4 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte3 & 3) << 8) | byte4;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode12(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 64; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = (byte0 << 4) | (byte1 >>> 4);
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte1 & 15) << 8) | byte2;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode14(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int iter = 0; iter < 32; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = (byte0 << 6) | (byte1 >>> 2);
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      int byte3 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte1 & 3) << 12) | (byte2 << 4) | (byte3 >>> 4);
+      int byte4 = blocks[blocksOffset++] & 0xFF;
+      int byte5 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte3 & 15) << 10) | (byte4 << 2) | (byte5 >>> 6);
+      int byte6 = blocks[blocksOffset++] & 0xFF;
+      values[valuesOffset++] = ((byte5 & 63) << 8) | byte6;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode16(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    for (int iter = 0; iter < 128; iter++) {
+      values[iter] = ((blocks[blocksOffset++] & 0xFF) << 8) | (blocks[blocksOffset++] & 0xFF);
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  public static void decode20(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int i = 0; i < 64; i++) {
+      values[valuesOffset++] = ((blocks[blocksOffset++] & 0xFF) << 12) | 
+                               ((blocks[blocksOffset++] & 0xFF) << 4)  | 
+                               ((blocks[blocksOffset  ] & 0xFF) >>> 4);
+      values[valuesOffset++] = ((blocks[blocksOffset++] & 15) << 16)   | 
+                               ((blocks[blocksOffset++] & 0xFF) << 8)  | 
+                                (blocks[blocksOffset++] & 0xFF);
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  private static void decode24(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    for (int iter = 0; iter < 128; iter++) {
+      int byte0 = blocks[blocksOffset++] & 0xFF;
+      int byte1 = blocks[blocksOffset++] & 0xFF;
+      int byte2 = blocks[blocksOffset++] & 0xFF;
+      values[iter] = (byte0 << 16) | (byte1 << 8) | byte2;
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  public static void decode28(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    int valuesOffset = 0;
+    for (int i = 0; i < 64; i++) {
+      values[valuesOffset++] = ((blocks[blocksOffset++] & 0xFF) << 20) | 
+                               ((blocks[blocksOffset++] & 0xFF) << 12) | 
+                               ((blocks[blocksOffset++] & 0xFF) << 4)  | 
+                               ((blocks[blocksOffset  ] & 0xFF) >>> 4);
+      values[valuesOffset++] = ((blocks[blocksOffset++] & 15) << 24)   | 
+                               ((blocks[blocksOffset++] & 0xFF) << 16) | 
+                               ((blocks[blocksOffset++] & 0xFF) << 8)  | 
+                                (blocks[blocksOffset++] & 0xFF);
+    }
+  }
+
+  // NOTE: always decodes 128 values
+  public static void decode32(byte[] blocks, int[] values) {
+    int blocksOffset = 0;
+    for (int i = 0; i < 128; i++) {
+      values[i] = ((blocks[blocksOffset++] & 0xFF) << 24) | 
+                  ((blocks[blocksOffset++] & 0xFF) << 16) | 
+                  ((blocks[blocksOffset++] & 0xFF) << 8)  | 
+                   (blocks[blocksOffset++] & 0xFF);
+    }
+  }
+
+  public static void decode(int bpv, byte[] source, int[] dest) {
+    switch(bpv) {
+      case 1:  decode1(source, dest); break;
+      case 2:  decode2(source, dest); break;
+      case 3:  decode3(source, dest); break;
+      case 4:  decode4(source, dest); break;
+      case 6:  decode6(source, dest); break;
+      case 8:  decode8(source, dest); break;
+      case 10: decode10(source, dest); break;
+      case 12: decode12(source, dest); break;
+      case 14: decode14(source, dest); break;
+      case 16: decode16(source, dest); break;
+      case 20: decode20(source, dest); break;
+      case 24: decode24(source, dest); break;
+      case 28: decode28(source, dest); break;
+      case 32: decode32(source, dest); break;
+    }
+  }
+  
+  final static int SUPPORTED_BITS_PER_VALUE[] = new int[] {
+    1, 2, 3, 4, 6, 8, 10, 12, 14, 16, 20, 24, 28, 32
+  };
+  
+  public static int bitsRequired(long maxValue) {
+    if (maxValue < 0) {
+      throw new IllegalArgumentException("maxValue must be non-negative (got: " + maxValue + ")");
+    }
+    int bitsRequired = Math.max(1, 64 - Long.numberOfLeadingZeros(maxValue));
+    int index = Arrays.binarySearch(SUPPORTED_BITS_PER_VALUE, bitsRequired);
+    if (index < 0) {
+      return SUPPORTED_BITS_PER_VALUE[-index-1];
+    } else {
+      return bitsRequired;
+    }
+  }
+
+  // NOTE: always encodes 128 values
+  public static void encode(int bitsPerValue, int[] values, byte[] blocks) {
+    int valuesOffset = 0;
+    int blocksOffset = 0;
+    int nextBlock = 0;
+    int bitsLeft = 8;
+    while (valuesOffset < 128) {
+      final int v = values[valuesOffset++];
+      assert bitsRequired(v & 0xFFFFFFFFL) <= bitsPerValue;
+      if (bitsPerValue < bitsLeft) {
+        // just buffer
+        nextBlock |= v << (bitsLeft - bitsPerValue);
+        bitsLeft -= bitsPerValue;
+      } else {
+        // flush as many blocks as possible
+        int bits = bitsPerValue - bitsLeft;
+        blocks[blocksOffset++] = (byte) (nextBlock | (v >>> bits));
+        while (bits >= 8) {
+          bits -= 8;
+          blocks[blocksOffset++] = (byte) (v >>> bits);
+        }
+        // then buffer
+        bitsLeft = 8 - bits;
+        nextBlock = (v & ((1 << bits) - 1)) << bitsLeft;
+      }
+    }
+    assert bitsLeft == 8;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/ForUtil.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java	(working copy)
@@ -0,0 +1,140 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
+import org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat;
+import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene49.Lucene49NormsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+
+/**
+ * Implements the Lucene 4.10 index format, with configurable per-field postings
+ * and docvalues formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene410 package documentation for file format details.
+ * @lucene.experimental
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene411Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+public class Lucene410Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
+  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene410Codec.this.getPostingsFormatForField(field);
+    }
+  };
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene410Codec.this.getDocValuesFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene410Codec() {
+    super("Lucene410");
+  }
+  
+  @Override
+  public final StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public final TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public final FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public final SegmentInfoFormat segmentInfoFormat() {
+    return segmentInfosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene410"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene49"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
+  @Override
+  public final DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene410");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene49");
+
+  private final NormsFormat normsFormat = new Lucene49NormsFormat();
+
+  @Override
+  public final NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsBaseFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsBaseFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsBaseFormat.java	(working copy)
@@ -0,0 +1,51 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PostingsBaseFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** 
+ * Provides a {@link PostingsReaderBase} and {@link
+ * PostingsWriterBase}.
+ *
+ * @lucene.experimental */
+
+// TODO: should these also be named / looked up via SPI?
+public final class Lucene410PostingsBaseFormat extends PostingsBaseFormat {
+
+  /** Sole constructor. */
+  public Lucene410PostingsBaseFormat() {
+    super("Lucene410");
+  }
+
+  @Override
+  public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
+    return new Lucene410PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+  }
+
+  @Override
+  public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
+    return new Lucene410PostingsWriter(state);
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsBaseFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsFormat.java	(working copy)
@@ -0,0 +1,451 @@
+package org.apache.lucene.codecs.lucene410;
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.10 postings format, which encodes postings in packed integer blocks 
+ * for fast decode.
+ *
+ * <p>
+ * Basic idea:
+ * <ul>
+ *   <li>
+ *   <b>Packed Blocks and VInt Blocks</b>: 
+ *   <p>In packed blocks, integers are encoded with the same bit width ({@link PackedInts packed format}):
+ *      the block size (i.e. number of integers inside block) is fixed (currently 128). Additionally blocks
+ *      that are all the same value are encoded in an optimized way.</p>
+ *   <p>In VInt blocks, integers are encoded as {@link DataOutput#writeVInt VInt}:
+ *      the block size is variable.</p>
+ *   </li>
+ *
+ *   <li> 
+ *   <b>Block structure</b>: 
+ *   <p>When the postings are long enough, Lucene410PostingsFormat will try to encode most integer data 
+ *      as a packed block.</p> 
+ *   <p>Take a term with 259 documents as an example, the first 256 document ids are encoded as two packed 
+ *      blocks, while the remaining 3 are encoded as one VInt block. </p>
+ *   <p>Different kinds of data are always encoded separately into different packed blocks, but may 
+ *      possibly be interleaved into the same VInt block. </p>
+ *   <p>This strategy is applied to pairs: 
+ *      &lt;document number, frequency&gt;,
+ *      &lt;position, payload length&gt;, 
+ *      &lt;position, offset start, offset length&gt;, and
+ *      &lt;position, payload length, offsetstart, offset length&gt;.</p>
+ *   </li>
+ *
+ *   <li>
+ *   <b>Skipdata settings</b>: 
+ *   <p>The structure of skip table is quite similar to previous version of Lucene. Skip interval is the 
+ *      same as block size, and each skip entry points to the beginning of each block. However, for 
+ *      the first block, skip data is omitted.</p>
+ *   </li>
+ *
+ *   <li>
+ *   <b>Positions, Payloads, and Offsets</b>: 
+ *   <p>A position is an integer indicating where the term occurs within one document. 
+ *      A payload is a blob of metadata associated with current position. 
+ *      An offset is a pair of integers indicating the tokenized start/end offsets for given term 
+ *      in current position: it is essentially a specialized payload. </p>
+ *   <p>When payloads and offsets are not omitted, numPositions==numPayloads==numOffsets (assuming a 
+ *      null payload contributes one count). As mentioned in block structure, it is possible to encode 
+ *      these three either combined or separately. 
+ *   <p>In all cases, payloads and offsets are stored together. When encoded as a packed block, 
+ *      position data is separated out as .pos, while payloads and offsets are encoded in .pay (payload 
+ *      metadata will also be stored directly in .pay). When encoded as VInt blocks, all these three are 
+ *      stored interleaved into the .pos (so is payload metadata).</p>
+ *   <p>With this strategy, the majority of payload and offset data will be outside .pos file. 
+ *      So for queries that require only position data, running on a full index with payloads and offsets, 
+ *      this reduces disk pre-fetches.</p>
+ *   </li>
+ * </ul>
+ * </p>
+ *
+ * <p>
+ * Files and detailed format:
+ * <ul>
+ *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
+ *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
+ *   <li><tt>.doc</tt>: <a href="#Frequencies">Frequencies and Skip Data</a></li>
+ *   <li><tt>.pos</tt>: <a href="#Positions">Positions</a></li>
+ *   <li><tt>.pay</tt>: <a href="#Payloads">Payloads and Offsets</a></li>
+ * </ul>
+ * </p>
+ *
+ * <a name="Termdictionary" id="Termdictionary"></a>
+ * <dl>
+ * <dd>
+ * <b>Term Dictionary</b>
+ *
+ * <p>The .tim file contains the list of terms in each
+ * field along with per-term statistics (such as docfreq)
+ * and pointers to the frequencies, positions, payload and
+ * skip data in the .doc, .pos, and .pay files.
+ * See {@link BlockTreeTermsWriter} for more details on the format.
+ * </p>
+ *
+ * <p>NOTE: The term dictionary can plug into different postings implementations:
+ * the postings writer/reader are actually responsible for encoding 
+ * and decoding the PostingsHeader and TermMetadata sections described here:</p>
+ *
+ * <ul>
+ *   <li>PostingsHeader --&gt; Header, PackedBlockSize</li>
+ *   <li>TermMetadata --&gt; (DocFPDelta|SingletonDocID), PosFPDelta?, PosVIntBlockFPDelta?, PayFPDelta?, 
+ *                            SkipFPDelta?</li>
+ *   <li>Header, --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>PackedBlockSize, SingletonDocID --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>DocFPDelta, PosFPDelta, PayFPDelta, PosVIntBlockFPDelta, SkipFPDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
+ *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *    <li>Header is a {@link CodecUtil#writeHeader CodecHeader} storing the version information
+ *        for the postings.</li>
+ *    <li>PackedBlockSize is the fixed block size for packed blocks. In packed block, bit width is 
+ *        determined by the largest integer. Smaller block size result in smaller variance among width 
+ *        of integers hence smaller indexes. Larger block size result in more efficient bulk i/o hence
+ *        better acceleration. This value should always be a multiple of 64, currently fixed as 128 as 
+ *        a tradeoff. It is also the skip interval used to accelerate {@link DocsEnum#advance(int)}.
+ *    <li>DocFPDelta determines the position of this term's TermFreqs within the .doc file. 
+ *        In particular, it is the difference of file offset between this term's
+ *        data and previous term's data (or zero, for the first term in the block).On disk it is 
+ *        stored as the difference from previous value in sequence. </li>
+ *    <li>PosFPDelta determines the position of this term's TermPositions within the .pos file.
+ *        While PayFPDelta determines the position of this term's &lt;TermPayloads, TermOffsets?&gt; within 
+ *        the .pay file. Similar to DocFPDelta, it is the difference between two file positions (or 
+ *        neglected, for fields that omit payloads and offsets).</li>
+ *    <li>PosVIntBlockFPDelta determines the position of this term's last TermPosition in last pos packed
+ *        block within the .pos file. It is synonym for PayVIntBlockFPDelta or OffsetVIntBlockFPDelta. 
+ *        This is actually used to indicate whether it is necessary to load following
+ *        payloads and offsets from .pos instead of .pay. Every time a new block of positions are to be 
+ *        loaded, the PostingsReader will use this value to check whether current block is packed format
+ *        or VInt. When packed format, payloads and offsets are fetched from .pay, otherwise from .pos. 
+ *        (this value is neglected when total number of positions i.e. totalTermFreq is less or equal 
+ *        to PackedBlockSize).
+ *    <li>SkipFPDelta determines the position of this term's SkipData within the .doc
+ *        file. In particular, it is the length of the TermFreq data.
+ *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum
+ *        (i.e. 128 in Lucene410PostingsFormat).</li>
+ *    <li>SingletonDocID is an optimization when a term only appears in one document. In this case, instead
+ *        of writing a file pointer to the .doc file (DocFPDelta), and then a VIntBlock at that location, the 
+ *        single document ID is written to the term dictionary.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Termindex" id="Termindex"></a>
+ * <dl>
+ * <dd>
+ * <b>Term Index</b>
+ * <p>The .tip file contains an index into the term dictionary, so that it can be 
+ * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.</p>
+ * </dd>
+ * </dl>
+ *
+ *
+ * <a name="Frequencies" id="Frequencies"></a>
+ * <dl>
+ * <dd>
+ * <b>Frequencies and Skip Data</b>
+ *
+ * <p>The .doc file contains the lists of documents which contain each term, along
+ * with the frequency of the term in that document (except when frequencies are
+ * omitted: {@link IndexOptions#DOCS_ONLY}). It also saves skip data to the beginning of 
+ * each packed or VInt block, when the length of document list is larger than packed block size.</p>
+ *
+ * <ul>
+ *   <li>docFile(.doc) --&gt; Header, &lt;TermFreqs, SkipData?&gt;<sup>TermCount</sup>, Footer</li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermFreqs --&gt; &lt;PackedBlock&gt; <sup>PackedDocBlockNum</sup>,  
+ *                        VIntBlock? </li>
+ *   <li>PackedBlock --&gt; PackedDocDeltaBlock, PackedFreqBlock?
+ *   <li>VIntBlock --&gt; &lt;DocDelta[, Freq?]&gt;<sup>DocFreq-PackedBlockSize*PackedDocBlockNum</sup>
+ *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
+ *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt;, SkipDatum?</li>
+ *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>TrimmedDocFreq/(PackedBlockSize^(Level + 1))</sup></li>
+ *   <li>SkipDatum --&gt; DocSkip, DocFPSkip, &lt;PosFPSkip, PosBlockOffset, PayLength?, 
+ *                        PayFPSkip?&gt;?, SkipChildLevelPointer?</li>
+ *   <li>PackedDocDeltaBlock, PackedFreqBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>DocDelta, Freq, DocSkip, DocFPSkip, PosFPSkip, PosBlockOffset, PayByteUpto, PayFPSkip 
+ *       --&gt; 
+ *   {@link DataOutput#writeVInt VInt}</li>
+ *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
+ *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>PackedDocDeltaBlock is theoretically generated from two steps: 
+ *     <ol>
+ *       <li>Calculate the difference between each document number and previous one, 
+ *           and get a d-gaps list (for the first document, use absolute value); </li>
+ *       <li>For those d-gaps from first one to PackedDocBlockNum*PackedBlockSize<sup>th</sup>, 
+ *           separately encode as packed blocks.</li>
+ *     </ol>
+ *     If frequencies are not omitted, PackedFreqBlock will be generated without d-gap step.
+ *   </li>
+ *   <li>VIntBlock stores remaining d-gaps (along with frequencies when possible) with a format 
+ *       that encodes DocDelta and Freq:
+ *       <p>DocDelta: if frequencies are indexed, this determines both the document
+ *       number and the frequency. In particular, DocDelta/2 is the difference between
+ *       this document number and the previous document number (or zero when this is the
+ *       first document in a TermFreqs). When DocDelta is odd, the frequency is one.
+ *       When DocDelta is even, the frequency is read as another VInt. If frequencies
+ *       are omitted, DocDelta contains the gap (not multiplied by 2) between document
+ *       numbers and no frequency information is stored.</p>
+ *       <p>For example, the TermFreqs for a term which occurs once in document seven
+ *          and three times in document eleven, with frequencies indexed, would be the
+ *          following sequence of VInts:</p>
+ *       <p>15, 8, 3</p>
+ *       <p>If frequencies were omitted ({@link IndexOptions#DOCS_ONLY}) it would be this
+ *          sequence of VInts instead:</p>
+ *       <p>7,4</p>
+ *   </li>
+ *   <li>PackedDocBlockNum is the number of packed blocks for current term's docids or frequencies. 
+ *       In particular, PackedDocBlockNum = floor(DocFreq/PackedBlockSize) </li>
+ *   <li>TrimmedDocFreq = DocFreq % PackedBlockSize == 0 ? DocFreq - 1 : DocFreq. 
+ *       We use this trick since the definition of skip entry is a little different from base interface.
+ *       In {@link MultiLevelSkipListWriter}, skip data is assumed to be saved for
+ *       skipInterval<sup>th</sup>, 2*skipInterval<sup>th</sup> ... posting in the list. However, 
+ *       in Lucene410PostingsFormat, the skip data is saved for skipInterval+1<sup>th</sup>, 
+ *       2*skipInterval+1<sup>th</sup> ... posting (skipInterval==PackedBlockSize in this case). 
+ *       When DocFreq is multiple of PackedBlockSize, MultiLevelSkipListWriter will expect one 
+ *       more skip data than Lucene410SkipWriter. </li>
+ *   <li>SkipDatum is the metadata of one skip entry.
+ *      For the first block (no matter packed or VInt), it is omitted.</li>
+ *   <li>DocSkip records the document number of every PackedBlockSize<sup>th</sup> document number in
+ *       the postings (i.e. last document number in each packed block). On disk it is stored as the 
+ *       difference from previous value in the sequence. </li>
+ *   <li>DocFPSkip records the file offsets of each block (excluding )posting at 
+ *       PackedBlockSize+1<sup>th</sup>, 2*PackedBlockSize+1<sup>th</sup> ... , in DocFile. 
+ *       The file offsets are relative to the start of current term's TermFreqs. 
+ *       On disk it is also stored as the difference from previous SkipDatum in the sequence.</li>
+ *   <li>Since positions and payloads are also block encoded, the skip should skip to related block first,
+ *       then fetch the values according to in-block offset. PosFPSkip and PayFPSkip record the file 
+ *       offsets of related block in .pos and .pay, respectively. While PosBlockOffset indicates
+ *       which value to fetch inside the related block (PayBlockOffset is unnecessary since it is always
+ *       equal to PosBlockOffset). Same as DocFPSkip, the file offsets are relative to the start of 
+ *       current term's TermFreqs, and stored as a difference sequence.</li>
+ *   <li>PayByteUpto indicates the start offset of the current payload. It is equivalent to
+ *       the sum of the payload lengths in the current block up to PosBlockOffset</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Positions" id="Positions"></a>
+ * <dl>
+ * <dd>
+ * <b>Positions</b>
+ * <p>The .pos file contains the lists of positions that each term occurs at within documents. It also
+ *    sometimes stores part of payloads and offsets for speedup.</p>
+ * <ul>
+ *   <li>PosFile(.pos) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup>, Footer</li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermPositions --&gt; &lt;PackedPosDeltaBlock&gt; <sup>PackedPosBlockNum</sup>,  
+ *                            VIntBlock? </li>
+ *   <li>VIntBlock --&gt; &lt;PositionDelta[, PayloadLength?], PayloadData?, 
+ *                        OffsetDelta?, OffsetLength?&gt;<sup>PosVIntCount</sup>
+ *   <li>PackedPosDeltaBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>PositionDelta, OffsetDelta, OffsetLength --&gt; 
+ *       {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayloadData --&gt; {@link DataOutput#writeByte byte}<sup>PayLength</sup></li>
+ *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>TermPositions are order by term (terms are implicit, from the term dictionary), and position 
+ *       values for each term document pair are incremental, and ordered by document number.</li>
+ *   <li>PackedPosBlockNum is the number of packed blocks for current term's positions, payloads or offsets. 
+ *       In particular, PackedPosBlockNum = floor(totalTermFreq/PackedBlockSize) </li>
+ *   <li>PosVIntCount is the number of positions encoded as VInt format. In particular, 
+ *       PosVIntCount = totalTermFreq - PackedPosBlockNum*PackedBlockSize</li>
+ *   <li>The procedure how PackedPosDeltaBlock is generated is the same as PackedDocDeltaBlock 
+ *       in chapter <a href="#Frequencies">Frequencies and Skip Data</a>.</li>
+ *   <li>PositionDelta is, if payloads are disabled for the term's field, the
+ *       difference between the position of the current occurrence in the document and
+ *       the previous occurrence (or zero, if this is the first occurrence in this
+ *       document). If payloads are enabled for the term's field, then PositionDelta/2
+ *       is the difference between the current and the previous position. If payloads
+ *       are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
+ *       the length of the payload at the current term position.</li>
+ *   <li>For example, the TermPositions for a term which occurs as the fourth term in
+ *       one document, and as the fifth and ninth term in a subsequent document, would
+ *       be the following sequence of VInts (payloads disabled):
+ *       <p>4, 5, 4</p></li>
+ *   <li>PayloadData is metadata associated with the current term position. If
+ *       PayloadLength is stored at the current position, then it indicates the length
+ *       of this payload. If PayloadLength is not stored, then this payload has the same
+ *       length as the payload at the previous position.</li>
+ *   <li>OffsetDelta/2 is the difference between this position's startOffset from the
+ *       previous occurrence (or zero, if this is the first occurrence in this document).
+ *       If OffsetDelta is odd, then the length (endOffset-startOffset) differs from the
+ *       previous occurrence and an OffsetLength follows. Offset data is only written for
+ *       {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Payloads" id="Payloads"></a>
+ * <dl>
+ * <dd>
+ * <b>Payloads and Offsets</b>
+ * <p>The .pay file will store payloads and offsets associated with certain term-document positions. 
+ *    Some payloads and offsets will be separated out into .pos file, for performance reasons.</p>
+ * <ul>
+ *   <li>PayFile(.pay): --&gt; Header, &lt;TermPayloads, TermOffsets?&gt; <sup>TermCount</sup>, Footer</li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermPayloads --&gt; &lt;PackedPayLengthBlock, SumPayLength, PayData&gt; <sup>PackedPayBlockNum</sup>
+ *   <li>TermOffsets --&gt; &lt;PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock&gt; <sup>PackedPayBlockNum</sup>
+ *   <li>PackedPayLengthBlock, PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>SumPayLength --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>SumPayLength</sup></li>
+ *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>The order of TermPayloads/TermOffsets will be the same as TermPositions, note that part of 
+ *       payload/offsets are stored in .pos.</li>
+ *   <li>The procedure how PackedPayLengthBlock and PackedOffsetLengthBlock are generated is the 
+ *       same as PackedFreqBlock in chapter <a href="#Frequencies">Frequencies and Skip Data</a>. 
+ *       While PackedStartDeltaBlock follows a same procedure as PackedDocDeltaBlock.</li>
+ *   <li>PackedPayBlockNum is always equal to PackedPosBlockNum, for the same term. It is also synonym 
+ *       for PackedOffsetBlockNum.</li>
+ *   <li>SumPayLength is the total length of payloads written within one block, should be the sum
+ *       of PayLengths in one packed block.</li>
+ *   <li>PayLength in PackedPayLengthBlock is the length of each payload associated with the current 
+ *       position.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ * </p>
+ *
+ * @lucene.experimental
+ */
+
+public final class Lucene410PostingsFormat extends PostingsFormat {
+  /**
+   * Filename extension for document number, frequencies, and skip data.
+   * See chapter: <a href="#Frequencies">Frequencies and Skip Data</a>
+   */
+  public static final String DOC_EXTENSION = "doc";
+
+  /**
+   * Filename extension for positions. 
+   * See chapter: <a href="#Positions">Positions</a>
+   */
+  public static final String POS_EXTENSION = "pos";
+
+  /**
+   * Filename extension for payloads and offsets.
+   * See chapter: <a href="#Payloads">Payloads and Offsets</a>
+   */
+  public static final String PAY_EXTENSION = "pay";
+
+  private final int minTermBlockSize;
+  private final int maxTermBlockSize;
+
+  /**
+   * Fixed packed block size, number of integers encoded in 
+   * a single packed block.
+   */
+  public final static int BLOCK_SIZE = 128;
+
+  /** Creates {@code Lucene410PostingsFormat} with default
+   *  settings. */
+  public Lucene410PostingsFormat() {
+    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Creates {@code Lucene410PostingsFormat} with custom
+   *  values for {@code minBlockSize} and {@code
+   *  maxBlockSize} passed to block terms dictionary.
+   *  @see BlockTreeTermsWriter#BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int) */
+  public Lucene410PostingsFormat(int minTermBlockSize, int maxTermBlockSize) {
+    super("Lucene410");
+    this.minTermBlockSize = minTermBlockSize;
+    assert minTermBlockSize > 1;
+    this.maxTermBlockSize = maxTermBlockSize;
+    assert minTermBlockSize <= maxTermBlockSize;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(blocksize=" + BLOCK_SIZE + ")";
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new Lucene410PostingsWriter(state);
+
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, 
+                                                    postingsWriter,
+                                                    minTermBlockSize, 
+                                                    maxTermBlockSize);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsWriter);
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new Lucene410PostingsReader(state.directory,
+                                                                    state.fieldInfos,
+                                                                    state.segmentInfo,
+                                                                    state.context,
+                                                                    state.segmentSuffix);
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(state.directory,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo,
+                                                    postingsReader,
+                                                    state.context,
+                                                    state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsReader);
+      }
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsReader.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsReader.java	(working copy)
@@ -0,0 +1,227 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter.IntBlockTermState;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Concrete class that reads docId(maybe frq,pos,offset,payloads) list
+ * with postings format.
+ *
+ * @lucene.experimental
+ */
+public final class Lucene410PostingsReader extends PostingsReaderBase {
+
+  private static final long BASE_RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene410PostingsReader.class);
+
+  private final IndexInput docIn;
+  private final IndexInput posIn;
+  private final IndexInput payIn;
+
+  private int version;
+
+  /** Sole constructor. */
+  public Lucene410PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
+    boolean success = false;
+    IndexInput docIn = null;
+    IndexInput posIn = null;
+    IndexInput payIn = null;
+    try {
+      docIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, 
+                                                           segmentSuffix, 
+                                                           Lucene410PostingsFormat.DOC_EXTENSION),
+                            ioContext);
+      version = CodecUtil.checkHeader(docIn,
+                                      Lucene410PostingsWriter.DOC_CODEC,
+                                      Lucene410PostingsWriter.VERSION_START,
+                                      Lucene410PostingsWriter.VERSION_CURRENT);
+
+      if (fieldInfos.hasProx()) {
+        posIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, 
+                                                             segmentSuffix, 
+                                                             Lucene410PostingsFormat.POS_EXTENSION),
+                              ioContext);
+        CodecUtil.checkHeader(posIn, Lucene410PostingsWriter.POS_CODEC, version, version);
+
+        if (fieldInfos.hasPayloads() || fieldInfos.hasOffsets()) {
+          payIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, 
+                                                               segmentSuffix,
+                                                               Lucene410PostingsFormat.PAY_EXTENSION),
+                                ioContext);
+          CodecUtil.checkHeader(payIn, Lucene410PostingsWriter.PAY_CODEC, version, version);
+        }
+      }
+
+      this.docIn = docIn;
+      this.posIn = posIn;
+      this.payIn = payIn;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docIn, posIn, payIn);
+      }
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    // Make sure we are talking to the matching postings writer
+    CodecUtil.checkHeader(termsIn,
+                          Lucene410PostingsWriter.TERMS_CODEC,
+                          Lucene410PostingsWriter.VERSION_START,
+                          Lucene410PostingsWriter.VERSION_CURRENT);
+    final int indexBlockSize = termsIn.readVInt();
+    if (indexBlockSize != BLOCK_SIZE) {
+      throw new IllegalStateException("index-time BLOCK_SIZE (" + indexBlockSize + ") != read-time BLOCK_SIZE (" + BLOCK_SIZE + ")");
+    }
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new IntBlockTermState();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(docIn, posIn, payIn);
+  }
+
+  @Override
+  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute) throws IOException {
+    final IntBlockTermState termState = (IntBlockTermState) _termState;
+    final int fieldHasPositions = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+
+    if (absolute) {
+      termState.docStartFP = 0;
+      termState.posStartFP = 0;
+      termState.payStartFP = 0;
+    }
+
+    termState.docStartFP += longs[0];
+    if (fieldHasPositions >= 0) {
+      termState.posStartFP += longs[1];
+      if (fieldHasPositions > 0 || fieldInfo.hasPayloads()) {
+        termState.payStartFP += longs[2];
+      }
+    }
+    if (termState.docFreq == 1) {
+      termState.singletonDocID = in.readVInt();
+    } else {
+      termState.singletonDocID = -1;
+    }
+    if (fieldHasPositions >= 0) {
+      if (termState.totalTermFreq > BLOCK_SIZE) {
+        termState.lastPosBlockOffset = in.readVLong();
+      } else {
+        termState.lastPosBlockOffset = -1;
+      }
+    }
+    if (termState.docFreq > BLOCK_SIZE) {
+      termState.skipOffset = in.readVLong();
+    } else {
+      termState.skipOffset = -1;
+    }
+  }
+    
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    BlockDocsEnum docsEnum;
+    if (reuse instanceof BlockDocsEnum) {
+      docsEnum = (BlockDocsEnum) reuse;
+      if (!docsEnum.canReuse(docIn, fieldInfo)) {
+        docsEnum = new BlockDocsEnum(fieldInfo, docIn, null);
+      }
+    } else {
+      docsEnum = new BlockDocsEnum(fieldInfo, docIn, null);
+    }
+    return docsEnum.reset(liveDocs, (IntBlockTermState) termState, (flags & DocsEnum.FLAG_FREQS) != 0);
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
+                                               DocsAndPositionsEnum reuse, int flags)
+    throws IOException {
+
+    boolean indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    boolean indexHasPayloads = fieldInfo.hasPayloads();
+
+    if ((!indexHasOffsets || (flags & DocsAndPositionsEnum.FLAG_OFFSETS) == 0) &&
+        (!indexHasPayloads || (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) == 0)) {
+      BlockDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse instanceof BlockDocsAndPositionsEnum) {
+        docsAndPositionsEnum = (BlockDocsAndPositionsEnum) reuse;
+        if (!docsAndPositionsEnum.canReuse(docIn, fieldInfo)) {
+          docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo, docIn, posIn);
+        }
+      } else {
+        docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo, docIn, posIn);
+      }
+      return docsAndPositionsEnum.reset(liveDocs, (IntBlockTermState) termState);
+    } else {
+      EverythingEnum everythingEnum;
+      if (reuse instanceof EverythingEnum) {
+        everythingEnum = (EverythingEnum) reuse;
+        if (!everythingEnum.canReuse(docIn, fieldInfo)) {
+          everythingEnum = new EverythingEnum(fieldInfo, docIn, posIn, payIn);
+        }
+      } else {
+        everythingEnum = new EverythingEnum(fieldInfo, docIn, posIn, payIn);
+      }
+      return everythingEnum.reset(liveDocs, (IntBlockTermState) termState, flags);
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return BASE_RAM_BYTES_USED;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    if (docIn != null) {
+      CodecUtil.checksumEntireFile(docIn);
+    }
+    if (posIn != null) {
+      CodecUtil.checksumEntireFile(posIn);
+    }
+    if (payIn != null) {
+      CodecUtil.checksumEntireFile(payIn);
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsWriter.java	(working copy)
@@ -0,0 +1,522 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PushPostingsWriterBase;
+import org.apache.lucene.codecs.lucene41.Lucene41SkipWriter;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+import static org.apache.lucene.codecs.lucene410.ForUtil.MAX_ENCODED_SIZE;
+import static org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat.BLOCK_SIZE;
+
+/**
+ * Concrete class that writes docId(maybe frq,pos,offset,payloads) list
+ * with postings format.
+ *
+ * Postings list for each term will be stored separately. 
+ *
+ * @see Lucene41SkipWriter for details about skipping setting and postings layout.
+ * @lucene.experimental
+ */
+public final class Lucene410PostingsWriter extends PushPostingsWriterBase {
+
+  /** 
+   * Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  static final int maxSkipLevels = 10;
+
+  final static String TERMS_CODEC = "Lucene410PostingsWriterTerms";
+  final static String DOC_CODEC = "Lucene410PostingsWriterDoc";
+  final static String POS_CODEC = "Lucene410PostingsWriterPos";
+  final static String PAY_CODEC = "Lucene410PostingsWriterPay";
+
+  // Increment version to change it
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  IndexOutput docOut;
+  IndexOutput posOut;
+  IndexOutput payOut;
+
+  final static IntBlockTermState emptyState = new IntBlockTermState();
+  IntBlockTermState lastState;
+
+  // Holds starting file pointers for current term:
+  private long docStartFP;
+  private long posStartFP;
+  private long payStartFP;
+
+  final int[] docDeltaBuffer;
+  final int[] freqBuffer;
+  private int docBufferUpto;
+
+  final int[] posDeltaBuffer;
+  final int[] payloadLengthBuffer;
+  final int[] offsetStartDeltaBuffer;
+  final int[] offsetLengthBuffer;
+  private int posBufferUpto;
+
+  private byte[] payloadBytes;
+  private int payloadByteUpto;
+
+  private int lastBlockDocID;
+  private long lastBlockPosFP;
+  private long lastBlockPayFP;
+  private int lastBlockPosBufferUpto;
+  private int lastBlockPayloadByteUpto;
+
+  private int lastDocID;
+  private int lastPosition;
+  private int lastStartOffset;
+  private int docCount;
+
+  final byte[] encoded;
+
+  private final Lucene41SkipWriter skipWriter;
+  
+  /** Creates a postings writer */
+  // TODO: does this ctor even make sense?
+  public Lucene410PostingsWriter(SegmentWriteState state) throws IOException {
+    super();
+
+    docOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                                         state.segmentSuffix, 
+                                                                         Lucene410PostingsFormat.DOC_EXTENSION),
+                                                                         state.context);
+    IndexOutput posOut = null;
+    IndexOutput payOut = null;
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(docOut, DOC_CODEC, VERSION_CURRENT);
+      if (state.fieldInfos.hasProx()) {
+        posDeltaBuffer = new int[BLOCK_SIZE];
+        posOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                                             state.segmentSuffix, 
+                                                                             Lucene410PostingsFormat.POS_EXTENSION),
+                                                                             state.context);
+        CodecUtil.writeHeader(posOut, POS_CODEC, VERSION_CURRENT);
+
+        if (state.fieldInfos.hasPayloads()) {
+          payloadBytes = new byte[128];
+          payloadLengthBuffer = new int[BLOCK_SIZE];
+        } else {
+          payloadBytes = null;
+          payloadLengthBuffer = null;
+        }
+
+        if (state.fieldInfos.hasOffsets()) {
+          offsetStartDeltaBuffer = new int[BLOCK_SIZE];
+          offsetLengthBuffer = new int[BLOCK_SIZE];
+        } else {
+          offsetStartDeltaBuffer = null;
+          offsetLengthBuffer = null;
+        }
+
+        if (state.fieldInfos.hasPayloads() || state.fieldInfos.hasOffsets()) {
+          payOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                                               state.segmentSuffix, 
+                                                                               Lucene410PostingsFormat.PAY_EXTENSION),
+                                                                               state.context);
+          CodecUtil.writeHeader(payOut, PAY_CODEC, VERSION_CURRENT);
+        }
+      } else {
+        posDeltaBuffer = null;
+        payloadLengthBuffer = null;
+        offsetStartDeltaBuffer = null;
+        offsetLengthBuffer = null;
+        payloadBytes = null;
+      }
+      this.payOut = payOut;
+      this.posOut = posOut;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
+      }
+    }
+
+    docDeltaBuffer = new int[BLOCK_SIZE];
+    freqBuffer = new int[BLOCK_SIZE];
+
+    // TODO: should we try skipping every 2/4 blocks...?
+    skipWriter = new Lucene41SkipWriter(maxSkipLevels,
+                                        BLOCK_SIZE, 
+                                        state.segmentInfo.getDocCount(),
+                                        docOut,
+                                        posOut,
+                                        payOut);
+
+    encoded = new byte[MAX_ENCODED_SIZE];
+  }
+
+  final static class IntBlockTermState extends BlockTermState {
+    long docStartFP = 0;
+    long posStartFP = 0;
+    long payStartFP = 0;
+    long skipOffset = -1;
+    long lastPosBlockOffset = -1;
+    // docid when there is a single pulsed posting, otherwise -1
+    // freq is always implicitly totalTermFreq in this case.
+    int singletonDocID = -1;
+
+    @Override
+    public IntBlockTermState clone() {
+      IntBlockTermState other = new IntBlockTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      IntBlockTermState other = (IntBlockTermState) _other;
+      docStartFP = other.docStartFP;
+      posStartFP = other.posStartFP;
+      payStartFP = other.payStartFP;
+      lastPosBlockOffset = other.lastPosBlockOffset;
+      skipOffset = other.skipOffset;
+      singletonDocID = other.singletonDocID;
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " docStartFP=" + docStartFP + " posStartFP=" + posStartFP + " payStartFP=" + payStartFP + " lastPosBlockOffset=" + lastPosBlockOffset + " singletonDocID=" + singletonDocID;
+    }
+  }
+
+  @Override
+  public IntBlockTermState newTermState() {
+    return new IntBlockTermState();
+  }
+
+  @Override
+  public void init(IndexOutput termsOut) throws IOException {
+    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
+    termsOut.writeVInt(BLOCK_SIZE);
+  }
+
+  @Override
+  public int setField(FieldInfo fieldInfo) {
+    super.setField(fieldInfo);
+    skipWriter.setField(writePositions, writeOffsets, writePayloads);
+    lastState = emptyState;
+    if (writePositions) {
+      if (writePayloads || writeOffsets) {
+        return 3;  // doc + pos + pay FP
+      } else {
+        return 2;  // doc + pos FP
+      }
+    } else {
+      return 1;    // doc FP
+    }
+  }
+
+  @Override
+  public void startTerm() {
+    docStartFP = docOut.getFilePointer();
+    if (writePositions) {
+      posStartFP = posOut.getFilePointer();
+      if (writePayloads || writeOffsets) {
+        payStartFP = payOut.getFilePointer();
+      }
+    }
+    lastDocID = 0;
+    lastBlockDocID = -1;
+    skipWriter.resetSkip();
+  }
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    // Have collected a block of docs, and get a new doc. 
+    // Should write skip data as well as postings list for
+    // current block.
+    if (lastBlockDocID != -1 && docBufferUpto == 0) {
+      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockPayloadByteUpto);
+    }
+
+    final int docDelta = docID - lastDocID;
+
+    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
+    }
+
+    docDeltaBuffer[docBufferUpto] = docDelta;
+    if (writeFreqs) {
+      freqBuffer[docBufferUpto] = termDocFreq;
+    }
+    docBufferUpto++;
+    docCount++;
+
+    if (docBufferUpto == BLOCK_SIZE) {
+      ForUtil.writeBlock(docDeltaBuffer, encoded, docOut);
+      if (writeFreqs) {
+        ForUtil.writeBlock(freqBuffer, encoded, docOut);
+      }
+      // NOTE: don't set docBufferUpto back to 0 here;
+      // finishDoc will do so (because it needs to see that
+      // the block was filled so it can save skip data)
+    }
+
+    lastDocID = docID;
+    lastPosition = 0;
+    lastStartOffset = 0;
+  }
+
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+    posDeltaBuffer[posBufferUpto] = position - lastPosition;
+    if (writePayloads) {
+      if (payload == null || payload.length == 0) {
+        // no payload
+        payloadLengthBuffer[posBufferUpto] = 0;
+      } else {
+        payloadLengthBuffer[posBufferUpto] = payload.length;
+        if (payloadByteUpto + payload.length > payloadBytes.length) {
+          payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payload.length);
+        }
+        System.arraycopy(payload.bytes, payload.offset, payloadBytes, payloadByteUpto, payload.length);
+        payloadByteUpto += payload.length;
+      }
+    }
+
+    if (writeOffsets) {
+      assert startOffset >= lastStartOffset;
+      assert endOffset >= startOffset;
+      offsetStartDeltaBuffer[posBufferUpto] = startOffset - lastStartOffset;
+      offsetLengthBuffer[posBufferUpto] = endOffset - startOffset;
+      lastStartOffset = startOffset;
+    }
+    
+    posBufferUpto++;
+    lastPosition = position;
+    if (posBufferUpto == BLOCK_SIZE) {
+      ForUtil.writeBlock(posDeltaBuffer, encoded, posOut);
+
+      if (writePayloads) {
+        ForUtil.writeBlock(payloadLengthBuffer, encoded, payOut);
+        payOut.writeVInt(payloadByteUpto);
+        payOut.writeBytes(payloadBytes, 0, payloadByteUpto);
+        payloadByteUpto = 0;
+      }
+      if (writeOffsets) {
+        ForUtil.writeBlock(offsetStartDeltaBuffer, encoded, payOut);
+        ForUtil.writeBlock(offsetLengthBuffer, encoded, payOut);
+      }
+      posBufferUpto = 0;
+    }
+  }
+
+  @Override
+  public void finishDoc() throws IOException {
+    // Since we don't know df for current term, we had to buffer
+    // those skip data for each block, and when a new doc comes, 
+    // write them to skip file.
+    if (docBufferUpto == BLOCK_SIZE) {
+      lastBlockDocID = lastDocID;
+      if (posOut != null) {
+        if (payOut != null) {
+          lastBlockPayFP = payOut.getFilePointer();
+        }
+        lastBlockPosFP = posOut.getFilePointer();
+        lastBlockPosBufferUpto = posBufferUpto;
+        lastBlockPayloadByteUpto = payloadByteUpto;
+      }
+      docBufferUpto = 0;
+    }
+  }
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(BlockTermState _state) throws IOException {
+    IntBlockTermState state = (IntBlockTermState) _state;
+    assert state.docFreq > 0;
+
+    // TODO: wasteful we are counting this (counting # docs
+    // for this term) in two places?
+    assert state.docFreq == docCount: state.docFreq + " vs " + docCount;
+    
+    // docFreq == 1, don't write the single docid/freq to a separate file along with a pointer to it.
+    final int singletonDocID;
+    if (state.docFreq == 1) {
+      // pulse the singleton docid into the term dictionary, freq is implicitly totalTermFreq
+      singletonDocID = docDeltaBuffer[0];
+    } else {
+      singletonDocID = -1;
+      // vInt encode the remaining doc deltas and freqs:
+      for(int i=0;i<docBufferUpto;i++) {
+        final int docDelta = docDeltaBuffer[i];
+        final int freq = freqBuffer[i];
+        if (!writeFreqs) {
+          docOut.writeVInt(docDelta);
+        } else if (freqBuffer[i] == 1) {
+          docOut.writeVInt((docDelta<<1)|1);
+        } else {
+          docOut.writeVInt(docDelta<<1);
+          docOut.writeVInt(freq);
+        }
+      }
+    }
+
+    final long lastPosBlockOffset;
+
+    if (writePositions) {
+      // totalTermFreq is just total number of positions(or payloads, or offsets)
+      // associated with current term.
+      assert state.totalTermFreq != -1;
+      if (state.totalTermFreq > BLOCK_SIZE) {
+        // record file offset for last pos in last block
+        lastPosBlockOffset = posOut.getFilePointer() - posStartFP;
+      } else {
+        lastPosBlockOffset = -1;
+      }
+      if (posBufferUpto > 0) {       
+        // TODO: should we send offsets/payloads to
+        // .pay...?  seems wasteful (have to store extra
+        // vLong for low (< BLOCK_SIZE) DF terms = vast vast
+        // majority)
+
+        // vInt encode the remaining positions/payloads/offsets:
+        int lastPayloadLength = -1;  // force first payload length to be written
+        int lastOffsetLength = -1;   // force first offset length to be written
+        int payloadBytesReadUpto = 0;
+        for(int i=0;i<posBufferUpto;i++) {
+          final int posDelta = posDeltaBuffer[i];
+          if (writePayloads) {
+            final int payloadLength = payloadLengthBuffer[i];
+            if (payloadLength != lastPayloadLength) {
+              lastPayloadLength = payloadLength;
+              posOut.writeVInt((posDelta<<1)|1);
+              posOut.writeVInt(payloadLength);
+            } else {
+              posOut.writeVInt(posDelta<<1);
+            }
+            
+            if (payloadLength != 0) {
+              posOut.writeBytes(payloadBytes, payloadBytesReadUpto, payloadLength);
+              payloadBytesReadUpto += payloadLength;
+            }
+          } else {
+            posOut.writeVInt(posDelta);
+          }
+
+          if (writeOffsets) {
+            int delta = offsetStartDeltaBuffer[i];
+            int length = offsetLengthBuffer[i];
+            if (length == lastOffsetLength) {
+              posOut.writeVInt(delta << 1);
+            } else {
+              posOut.writeVInt(delta << 1 | 1);
+              posOut.writeVInt(length);
+              lastOffsetLength = length;
+            }
+          }
+        }
+
+        if (writePayloads) {
+          assert payloadBytesReadUpto == payloadByteUpto;
+          payloadByteUpto = 0;
+        }
+      }
+    } else {
+      lastPosBlockOffset = -1;
+    }
+
+    long skipOffset;
+    if (docCount > BLOCK_SIZE) {
+      skipOffset = skipWriter.writeSkip(docOut) - docStartFP;
+    } else {
+      skipOffset = -1;
+    }
+    state.docStartFP = docStartFP;
+    state.posStartFP = posStartFP;
+    state.payStartFP = payStartFP;
+    state.singletonDocID = singletonDocID;
+    state.skipOffset = skipOffset;
+    state.lastPosBlockOffset = lastPosBlockOffset;
+    docBufferUpto = 0;
+    posBufferUpto = 0;
+    lastDocID = 0;
+    docCount = 0;
+  }
+  
+  @Override
+  public void encodeTerm(long[] longs, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
+    IntBlockTermState state = (IntBlockTermState)_state;
+    if (absolute) {
+      lastState = emptyState;
+    }
+    longs[0] = state.docStartFP - lastState.docStartFP;
+    if (writePositions) {
+      longs[1] = state.posStartFP - lastState.posStartFP;
+      if (writePayloads || writeOffsets) {
+        longs[2] = state.payStartFP - lastState.payStartFP;
+      }
+    }
+    if (state.singletonDocID != -1) {
+      out.writeVInt(state.singletonDocID);
+    }
+    if (writePositions) {
+      if (state.lastPosBlockOffset != -1) {
+        out.writeVLong(state.lastPosBlockOffset);
+      }
+    }
+    if (state.skipOffset != -1) {
+      out.writeVLong(state.skipOffset);
+    }
+    lastState = state;
+  }
+
+  @Override
+  public void close() throws IOException {
+    // TODO: add a finish() at least to PushBase? DV too...?
+    boolean success = false;
+    try {
+      if (docOut != null) {
+        CodecUtil.writeFooter(docOut);
+      }
+      if (posOut != null) {
+        CodecUtil.writeFooter(posOut);
+      }
+      if (payOut != null) {
+        CodecUtil.writeFooter(payOut);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(docOut, posOut, payOut);
+      } else {
+        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
+      }
+      docOut = posOut = payOut = null;
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410PostingsWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/package.html	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/package.html	(working copy)
@@ -0,0 +1,404 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Lucene 4.9 file format.
+
+<h1>Apache Lucene - Index File Formats</h1>
+<div>
+<ul>
+<li><a href="#Introduction">Introduction</a></li>
+<li><a href="#Definitions">Definitions</a>
+  <ul>
+  <li><a href="#Inverted_Indexing">Inverted Indexing</a></li>
+  <li><a href="#Types_of_Fields">Types of Fields</a></li>
+  <li><a href="#Segments">Segments</a></li>
+  <li><a href="#Document_Numbers">Document Numbers</a></li>
+  </ul>
+</li>
+<li><a href="#Overview">Index Structure Overview</a></li>
+<li><a href="#File_Naming">File Naming</a></li>
+<li><a href="#file-names">Summary of File Extensions</a></li>
+  <ul>
+  <li><a href="#Lock_File">Lock File</a></li>
+  <li><a href="#History">History</a></li>
+  <li><a href="#Limitations">Limitations</a></li>
+  </ul>
+</ul>
+</div>
+<a name="Introduction"></a>
+<h2>Introduction</h2>
+<div>
+<p>This document defines the index file formats used in this version of Lucene.
+If you are using a different version of Lucene, please consult the copy of
+<code>docs/</code> that was distributed with
+the version you are using.</p>
+<p>Apache Lucene is written in Java, but several efforts are underway to write
+<a href="http://wiki.apache.org/lucene-java/LuceneImplementations">versions of
+Lucene in other programming languages</a>. If these versions are to remain
+compatible with Apache Lucene, then a language-independent definition of the
+Lucene index format is required. This document thus attempts to provide a
+complete and independent definition of the Apache Lucene file formats.</p>
+<p>As Lucene evolves, this document should evolve. Versions of Lucene in
+different programming languages should endeavor to agree on file formats, and
+generate new versions of this document.</p>
+</div>
+<a name="Definitions" id="Definitions"></a>
+<h2>Definitions</h2>
+<div>
+<p>The fundamental concepts in Lucene are index, document, field and term.</p>
+<p>An index contains a sequence of documents.</p>
+<ul>
+<li>A document is a sequence of fields.</li>
+<li>A field is a named sequence of terms.</li>
+<li>A term is a sequence of bytes.</li>
+</ul>
+<p>The same sequence of bytes in two different fields is considered a different 
+term. Thus terms are represented as a pair: the string naming the field, and the
+bytes within the field.</p>
+<a name="Inverted_Indexing"></a>
+<h3>Inverted Indexing</h3>
+<p>The index stores statistics about terms in order to make term-based search
+more efficient. Lucene's index falls into the family of indexes known as an
+<i>inverted index.</i> This is because it can list, for a term, the documents
+that contain it. This is the inverse of the natural relationship, in which
+documents list terms.</p>
+<a name="Types_of_Fields"></a>
+<h3>Types of Fields</h3>
+<p>In Lucene, fields may be <i>stored</i>, in which case their text is stored
+in the index literally, in a non-inverted manner. Fields that are inverted are
+called <i>indexed</i>. A field may be both stored and indexed.</p>
+<p>The text of a field may be <i>tokenized</i> into terms to be indexed, or the
+text of a field may be used literally as a term to be indexed. Most fields are
+tokenized, but sometimes it is useful for certain identifier fields to be
+indexed literally.</p>
+<p>See the {@link org.apache.lucene.document.Field Field}
+java docs for more information on Fields.</p>
+<a name="Segments" id="Segments"></a>
+<h3>Segments</h3>
+<p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
+Each segment is a fully independent index, which could be searched separately.
+Indexes evolve by:</p>
+<ol>
+<li>Creating new segments for newly added documents.</li>
+<li>Merging existing segments.</li>
+</ol>
+<p>Searches may involve multiple segments and/or multiple indexes, each index
+potentially composed of a set of segments.</p>
+<a name="Document_Numbers"></a>
+<h3>Document Numbers</h3>
+<p>Internally, Lucene refers to documents by an integer <i>document number</i>.
+The first document added to an index is numbered zero, and each subsequent
+document added gets a number one greater than the previous.</p>
+<p>Note that a document's number may change, so caution should be taken when
+storing these numbers outside of Lucene. In particular, numbers may change in
+the following situations:</p>
+<ul>
+<li>
+<p>The numbers stored in each segment are unique only within the segment, and
+must be converted before they can be used in a larger context. The standard
+technique is to allocate each segment a range of values, based on the range of
+numbers used in that segment. To convert a document number from a segment to an
+external value, the segment's <i>base</i> document number is added. To convert
+an external value back to a segment-specific value, the segment is identified
+by the range that the external value is in, and the segment's base value is
+subtracted. For example two five document segments might be combined, so that
+the first segment has a base value of zero, and the second of five. Document
+three from the second segment would have an external value of eight.</p>
+</li>
+<li>
+<p>When documents are deleted, gaps are created in the numbering. These are
+eventually removed as the index evolves through merging. Deleted documents are
+dropped when segments are merged. A freshly-merged segment thus has no gaps in
+its numbering.</p>
+</li>
+</ul>
+</div>
+<a name="Overview" id="Overview"></a>
+<h2>Index Structure Overview</h2>
+<div>
+<p>Each segment index maintains the following:</p>
+<ul>
+<li>
+{@link org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat Segment info}.
+   This contains metadata about a segment, such as the number of documents,
+   what files it uses, 
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat Field names}. 
+   This contains the set of field names used in the index.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Stored Field values}. 
+This contains, for each document, a list of attribute-value pairs, where the attributes 
+are field names. These are used to store auxiliary information about the document, such as 
+its title, url, or an identifier to access a database. The set of stored fields are what is 
+returned for each hit when searching. This is keyed by document number.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Term dictionary}. 
+A dictionary containing all of the terms used in all of the
+indexed fields of all of the documents. The dictionary also contains the number
+of documents which contain the term, and pointers to the term's frequency and
+proximity data.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Term Frequency data}. 
+For each term in the dictionary, the numbers of all the
+documents that contain that term, and the frequency of the term in that
+document, unless frequencies are omitted (IndexOptions.DOCS_ONLY)
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Term Proximity data}. 
+For each term in the dictionary, the positions that the
+term occurs in each document. Note that this will not exist if all fields in
+all documents omit position data.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene49.Lucene49NormsFormat Normalization factors}. 
+For each field in each document, a value is stored
+that is multiplied into the score for hits on that field.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vectors}. 
+For each field in each document, the term vector (sometimes
+called document vector) may be stored. A term vector consists of term text and
+term frequency. To add Term Vectors to your index see the 
+{@link org.apache.lucene.document.Field Field} constructors
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat Per-document values}. 
+Like stored values, these are also keyed by document
+number, but are generally intended to be loaded into main memory for fast
+access. Whereas stored values are generally intended for summary results from
+searches, per-document values are useful for things like scoring factors.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted documents}. 
+An optional file indicating which documents are deleted.
+</li>
+</ul>
+<p>Details on each of these are provided in their linked pages.</p>
+</div>
+<a name="File_Naming"></a>
+<h2>File Naming</h2>
+<div>
+<p>All files belonging to a segment have the same name with varying extensions.
+The extensions correspond to the different file formats described below. When
+using the Compound File format (default in 1.4 and greater) these files (except
+for the Segment info file, the Lock file, and Deleted documents file) are collapsed 
+into a single .cfs file (see below for details)</p>
+<p>Typically, all segments in an index are stored in a single directory,
+although this is not required.</p>
+<p>As of version 2.1 (lock-less commits), file names are never re-used (there
+is one exception, "segments.gen", see below). That is, when any file is saved
+to the Directory it is given a never before used filename. This is achieved
+using a simple generations approach. For example, the first segments file is
+segments_1, then segments_2, etc. The generation is a sequential long integer
+represented in alpha-numeric (base 36) form.</p>
+</div>
+<a name="file-names" id="file-names"></a>
+<h2>Summary of File Extensions</h2>
+<div>
+<p>The following table summarizes the names and extensions of the files in
+Lucene:</p>
+<table cellspacing="1" cellpadding="4">
+<tr>
+<th>Name</th>
+<th>Extension</th>
+<th>Brief Description</th>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.index.SegmentInfos Segments File}</td>
+<td>segments.gen, segments_N</td>
+<td>Stores information about a commit point</td>
+</tr>
+<tr>
+<td><a href="#Lock_File">Lock File</a></td>
+<td>write.lock</td>
+<td>The Write lock prevents multiple IndexWriters from writing to the same
+file.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat Segment Info}</td>
+<td>.si</td>
+<td>Stores metadata about a segment</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.store.CompoundFileDirectory Compound File}</td>
+<td>.cfs, .cfe</td>
+<td>An optional "virtual" file consisting of all the other index files for
+systems that frequently run out of file handles.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat Fields}</td>
+<td>.fnm</td>
+<td>Stores information about the fields</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Index}</td>
+<td>.fdx</td>
+<td>Contains pointers to field data</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Data}</td>
+<td>.fdt</td>
+<td>The stored fields for documents</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Term Dictionary}</td>
+<td>.tim</td>
+<td>The term dictionary, stores term info</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Term Index}</td>
+<td>.tip</td>
+<td>The index into the Term Dictionary</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Frequencies}</td>
+<td>.doc</td>
+<td>Contains the list of docs which contain each term along with frequency</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Positions}</td>
+<td>.pos</td>
+<td>Stores position information about where a term occurs in the index</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat Payloads}</td>
+<td>.pay</td>
+<td>Stores additional per-position metadata information such as character offsets and user payloads</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene49.Lucene49NormsFormat Norms}</td>
+<td>.nvd, .nvm</td>
+<td>Encodes length and boost factors for docs and fields</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat Per-Document Values}</td>
+<td>.dvd, .dvm</td>
+<td>Encodes additional scoring factors or other per-document information.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Index}</td>
+<td>.tvx</td>
+<td>Stores offset into the document data file</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Documents}</td>
+<td>.tvd</td>
+<td>Contains information about each document that has term vectors</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Fields}</td>
+<td>.tvf</td>
+<td>The field level info about term vectors</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted Documents}</td>
+<td>.del</td>
+<td>Info about what files are deleted</td>
+</tr>
+</table>
+</div>
+<a name="Lock_File" id="Lock_File"></a>
+<h2>Lock File</h2>
+The write lock, which is stored in the index directory by default, is named
+"write.lock". If the lock directory is different from the index directory then
+the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
+derived from the full path to the index directory. When this file is present, a
+writer is currently modifying the index (adding or removing documents). This
+lock file ensures that only one writer is modifying the index at a time.</p>
+<a name="History"></a>
+<h2>History</h2>
+<p>Compatibility notes are provided in this document, describing how file
+formats have changed from prior versions:</p>
+<ul>
+<li>In version 2.1, the file format was changed to allow lock-less commits (ie,
+no more commit lock). The change is fully backwards compatible: you can open a
+pre-2.1 index for searching or adding/deleting of docs. When the new segments
+file is saved (committed), it will be written in the new file format (meaning
+no specific "upgrade" process is needed). But note that once a commit has
+occurred, pre-2.1 Lucene will not be able to read the index.</li>
+<li>In version 2.3, the file format was changed to allow segments to share a
+single set of doc store (vectors &amp; stored fields) files. This allows for
+faster indexing in certain cases. The change is fully backwards compatible (in
+the same way as the lock-less commits change in 2.1).</li>
+<li>In version 2.4, Strings are now written as true UTF-8 byte sequence, not
+Java's modified UTF-8. See <a href="http://issues.apache.org/jira/browse/LUCENE-510">
+LUCENE-510</a> for details.</li>
+<li>In version 2.9, an optional opaque Map&lt;String,String&gt; CommitUserData
+may be passed to IndexWriter's commit methods (and later retrieved), which is
+recorded in the segments_N file. See <a href="http://issues.apache.org/jira/browse/LUCENE-1382">
+LUCENE-1382</a> for details. Also,
+diagnostics were added to each segment written recording details about why it
+was written (due to flush, merge; which OS/JRE was used; etc.). See issue
+<a href="http://issues.apache.org/jira/browse/LUCENE-1654">LUCENE-1654</a> for details.</li>
+<li>In version 3.0, compressed fields are no longer written to the index (they
+can still be read, but on merge the new segment will write them, uncompressed).
+See issue <a href="http://issues.apache.org/jira/browse/LUCENE-1960">LUCENE-1960</a> 
+for details.</li>
+<li>In version 3.1, segments records the code version that created them. See
+<a href="http://issues.apache.org/jira/browse/LUCENE-2720">LUCENE-2720</a> for details. 
+Additionally segments track explicitly whether or not they have term vectors. 
+See <a href="http://issues.apache.org/jira/browse/LUCENE-2811">LUCENE-2811</a> 
+for details.</li>
+<li>In version 3.2, numeric fields are written as natively to stored fields
+file, previously they were stored in text format only.</li>
+<li>In version 3.4, fields can omit position data while still indexing term
+frequencies.</li>
+<li>In version 4.0, the format of the inverted index became extensible via
+the {@link org.apache.lucene.codecs.Codec Codec} api. Fast per-document storage
+({@code DocValues}) was introduced. Normalization factors need no longer be a 
+single byte, they can be any {@link org.apache.lucene.index.NumericDocValues NumericDocValues}. 
+Terms need not be unicode strings, they can be any byte sequence. Term offsets 
+can optionally be indexed into the postings lists. Payloads can be stored in the 
+term vectors.</li>
+<li>In version 4.1, the format of the postings list changed to use either
+of FOR compression or variable-byte encoding, depending upon the frequency
+of the term. Terms appearing only once were changed to inline directly into
+the term dictionary. Stored fields are compressed by default. </li>
+<li>In version 4.2, term vectors are compressed by default. DocValues has 
+a new multi-valued type (SortedSet), that can be used for faceting/grouping/joining
+on multi-valued fields.</li>
+<li>In version 4.5, DocValues were extended to explicitly represent missing values.</li>
+<li>In version 4.6, FieldInfos were extended to support per-field DocValues generation, to 
+allow updating NumericDocValues fields.</li>
+<li>In version 4.8, checksum footers were added to the end of each index file 
+for improved data integrity. Specifically, the last 8 bytes of every index file
+contain the zlib-crc32 checksum of the file.</li>
+<li>In version 4.9, DocValues has a new multi-valued numeric type (SortedNumeric)
+that is suitable for faceting/sorting/analytics.
+</li>
+</ul>
+<a name="Limitations" id="Limitations"></a>
+<h2>Limitations</h2>
+<div>
+<p>Lucene uses a Java <code>int</code> to refer to
+document numbers, and the index file format uses an <code>Int32</code>
+on-disk to store document numbers. This is a limitation
+of both the index file format and the current implementation. Eventually these
+should be replaced with either <code>UInt64</code> values, or
+better yet, {@link org.apache.lucene.store.DataOutput#writeVInt VInt} values which have no limit.</p>
+</div>
+</body>
+</html>

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene410/package.html
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/package.html	(revision 1608975)
+++ lucene/core/src/java/org/apache/lucene/codecs/package.html	(working copy)
@@ -61,13 +61,13 @@
   If you just want to customise the {@link org.apache.lucene.codecs.PostingsFormat}, or use different postings
   formats for different fields, then you can register your custom postings format in the same way (in
   META-INF/services/org.apache.lucene.codecs.PostingsFormat), and then extend the default
-  {@link org.apache.lucene.codecs.lucene49.Lucene49Codec} and override
-  {@link org.apache.lucene.codecs.lucene49.Lucene49Codec#getPostingsFormatForField(String)} to return your custom
+  {@link org.apache.lucene.codecs.lucene410.Lucene410Codec} and override
+  {@link org.apache.lucene.codecs.lucene410.Lucene410Codec#getPostingsFormatForField(String)} to return your custom
   postings format.
 </p>
 <p>
   Similarly, if you just want to customise the {@link org.apache.lucene.codecs.DocValuesFormat} per-field, have 
-  a look at {@link org.apache.lucene.codecs.lucene49.Lucene49Codec#getDocValuesFormatForField(String)}.
+  a look at {@link org.apache.lucene.codecs.lucene410.Lucene410Codec#getDocValuesFormatForField(String)}.
 </p>
 </body>
 </html>
Index: lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
===================================================================
--- lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec	(revision 1608975)
+++ lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec	(working copy)
@@ -19,3 +19,4 @@
 org.apache.lucene.codecs.lucene45.Lucene45Codec
 org.apache.lucene.codecs.lucene46.Lucene46Codec
 org.apache.lucene.codecs.lucene49.Lucene49Codec
+org.apache.lucene.codecs.lucene410.Lucene410Codec
Index: lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
===================================================================
--- lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(revision 1608975)
+++ lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(working copy)
@@ -15,3 +15,4 @@
 
 org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
 org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat
+org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat
Index: lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DirectoryReader;
@@ -37,11 +37,11 @@
 
 public class TestExternalCodecs extends LuceneTestCase {
 
-  private static final class CustomPerFieldCodec extends Lucene49Codec {
+  private static final class CustomPerFieldCodec extends Lucene410Codec {
     
     private final PostingsFormat ramFormat = PostingsFormat.forName("RAMOnly");
-    private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-    private final PostingsFormat pulsingFormat = PostingsFormat.forName("Pulsing41");
+    private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene410");
+    private final PostingsFormat pulsingFormat = PostingsFormat.forName("Pulsing410");
 
     @Override
     public PostingsFormat getPostingsFormatForField(String field) {
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat.java	(working copy)
@@ -0,0 +1,71 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Tests Lucene410PostingsFormat
+ */
+public class TestLucene410PostingsFormat extends BasePostingsFormatTestCase {
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat());
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+  public void testAllBitsPerValue() {
+    for (int bpv = 1; bpv < 32; bpv++) {
+      doTestBpv(bpv);
+    }
+  }
+  
+  private void doTestBpv(int maxBPV) {
+    for (int i = 0; i < 100; i++) {
+      int original[] = randomInts(maxBPV);
+      int max = 0;
+      for (int value = 0; value < original.length; value++) {
+        max = Math.max(max, original[value]);
+      }
+      int actualBPV = ForUtil.bitsRequired(max);
+      byte[] encoded = new byte[512];
+      ForUtil.encode(actualBPV, original, encoded);
+      int[] decoded = new int[128];
+      ForUtil.decode(actualBPV, encoded, decoded);
+      assertArrayEquals("bpv=" + actualBPV, original, decoded);
+    }
+  }
+  
+  private int[] randomInts(int maxBpv) {
+    int ints[] = new int[128];
+    int max;    
+    if (maxBpv == 31) {
+        max = Integer.MAX_VALUE;
+    } else {
+        max = 1 << maxBpv;
+    }
+    
+    for (int i = 0; i < ints.length; i++) {
+        ints[i] = random().nextInt(max);
+    }
+    return ints;
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat2.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat2.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat2.java	(working copy)
@@ -0,0 +1,131 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+/** 
+ * Tests special cases of Lucene410PostingsFormat 
+ */
+public class TestLucene410PostingsFormat2 extends LuceneTestCase {
+  Directory dir;
+  RandomIndexWriter iw;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    dir = newFSDirectory(createTempDir("testDFBlockSize"));
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat()));
+    iw = new RandomIndexWriter(random(), dir, iwc);
+    iw.setDoRandomForceMerge(false); // we will ourselves
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    iw.shutdown();
+    TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat()));
+    iwc.setOpenMode(OpenMode.APPEND);
+    IndexWriter iw = new IndexWriter(dir, iwc);
+    iw.forceMerge(1);
+    iw.shutdown();
+    dir.close(); // just force a checkindex for now
+    super.tearDown();
+  }
+  
+  private Document newDocument() {
+    Document doc = new Document();
+    for (IndexOptions option : FieldInfo.IndexOptions.values()) {
+      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+      // turn on tvs for a cross-check, since we rely upon checkindex in this test (for now)
+      ft.setStoreTermVectors(true);
+      ft.setStoreTermVectorOffsets(true);
+      ft.setStoreTermVectorPositions(true);
+      ft.setStoreTermVectorPayloads(true);
+      ft.setIndexOptions(option);
+      doc.add(new Field(option.toString(), "", ft));
+    }
+    return doc;
+  }
+
+  /** tests terms with df = blocksize */
+  public void testDFBlockSize() throws Exception {
+    Document doc = newDocument();
+    for (int i = 0; i < Lucene410PostingsFormat.BLOCK_SIZE; i++) {
+      for (Field f : doc.getFields()) {
+        f.setStringValue(f.name() + " " + f.name() + "_2");
+      }
+      iw.addDocument(doc);
+    }
+  }
+
+  /** tests terms with df % blocksize = 0 */
+  public void testDFBlockSizeMultiple() throws Exception {
+    Document doc = newDocument();
+    for (int i = 0; i < Lucene410PostingsFormat.BLOCK_SIZE * 16; i++) {
+      for (Field f : doc.getFields()) {
+        f.setStringValue(f.name() + " " + f.name() + "_2");
+      }
+      iw.addDocument(doc);
+    }
+  }
+  
+  /** tests terms with ttf = blocksize */
+  public void testTTFBlockSize() throws Exception {
+    Document doc = newDocument();
+    for (int i = 0; i < Lucene410PostingsFormat.BLOCK_SIZE/2; i++) {
+      for (Field f : doc.getFields()) {
+        f.setStringValue(f.name() + " " + f.name() + " " + f.name() + "_2 " + f.name() + "_2");
+      }
+      iw.addDocument(doc);
+    }
+  }
+  
+  /** tests terms with ttf % blocksize = 0 */
+  public void testTTFBlockSizeMultiple() throws Exception {
+    Document doc = newDocument();
+    for (int i = 0; i < Lucene410PostingsFormat.BLOCK_SIZE/2; i++) {
+      for (Field f : doc.getFields()) {
+        String proto = (f.name() + " " + f.name() + " " + f.name() + " " + f.name() + " " 
+                       + f.name() + "_2 " + f.name() + "_2 " + f.name() + "_2 " + f.name() + "_2");
+        StringBuilder val = new StringBuilder();
+        for (int j = 0; j < 16; j++) {
+          val.append(proto);
+          val.append(" ");
+        }
+        f.setStringValue(val.toString());
+      }
+      iw.addDocument(doc);
+    }
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat2.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat3.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat3.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat3.java	(working copy)
@@ -0,0 +1,521 @@
+package org.apache.lucene.codecs.lucene410;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Random;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockFixedLengthPayloadFilter;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.MockVariableLengthPayloadFilter;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.English;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.automaton.AutomatonTestUtil;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.automaton.RegExp;
+
+/** 
+ * Tests partial enumeration (only pulling a subset of the indexed data) 
+ */
+public class TestLucene410PostingsFormat3 extends LuceneTestCase {
+  static final int MAXDOC = Lucene410PostingsFormat.BLOCK_SIZE * 20;
+  
+  // creates 8 fields with different options and does "duels" of fields against each other
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer();
+        if (fieldName.contains("payloadsFixed")) {
+          TokenFilter filter = new MockFixedLengthPayloadFilter(new Random(0), tokenizer, 1);
+          return new TokenStreamComponents(tokenizer, filter);
+        } else if (fieldName.contains("payloadsVariable")) {
+          TokenFilter filter = new MockVariableLengthPayloadFilter(new Random(0), tokenizer);
+          return new TokenStreamComponents(tokenizer, filter);
+        } else {
+          return new TokenStreamComponents(tokenizer);
+        }
+      }
+    };
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat()));
+    // TODO we could actually add more fields implemented with different PFs
+    // or, just put this test into the usual rotation?
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    FieldType docsOnlyType = new FieldType(TextField.TYPE_NOT_STORED);
+    // turn this on for a cross-check
+    docsOnlyType.setStoreTermVectors(true);
+    docsOnlyType.setIndexOptions(IndexOptions.DOCS_ONLY);
+    
+    FieldType docsAndFreqsType = new FieldType(TextField.TYPE_NOT_STORED);
+    // turn this on for a cross-check
+    docsAndFreqsType.setStoreTermVectors(true);
+    docsAndFreqsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
+    
+    FieldType positionsType = new FieldType(TextField.TYPE_NOT_STORED);
+    // turn these on for a cross-check
+    positionsType.setStoreTermVectors(true);
+    positionsType.setStoreTermVectorPositions(true);
+    positionsType.setStoreTermVectorOffsets(true);
+    positionsType.setStoreTermVectorPayloads(true);
+    FieldType offsetsType = new FieldType(positionsType);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field field1 = new Field("field1docs", "", docsOnlyType);
+    Field field2 = new Field("field2freqs", "", docsAndFreqsType);
+    Field field3 = new Field("field3positions", "", positionsType);
+    Field field4 = new Field("field4offsets", "", offsetsType);
+    Field field5 = new Field("field5payloadsFixed", "", positionsType);
+    Field field6 = new Field("field6payloadsVariable", "", positionsType);
+    Field field7 = new Field("field7payloadsFixedOffsets", "", offsetsType);
+    Field field8 = new Field("field8payloadsVariableOffsets", "", offsetsType);
+    doc.add(field1);
+    doc.add(field2);
+    doc.add(field3);
+    doc.add(field4);
+    doc.add(field5);
+    doc.add(field6);
+    doc.add(field7);
+    doc.add(field8);
+    for (int i = 0; i < MAXDOC; i++) {
+      String stringValue = Integer.toString(i) + " verycommon " + English.intToEnglish(i).replace('-', ' ') + " " + TestUtil.randomSimpleString(random());
+      field1.setStringValue(stringValue);
+      field2.setStringValue(stringValue);
+      field3.setStringValue(stringValue);
+      field4.setStringValue(stringValue);
+      field5.setStringValue(stringValue);
+      field6.setStringValue(stringValue);
+      field7.setStringValue(stringValue);
+      field8.setStringValue(stringValue);
+      iw.addDocument(doc);
+    }
+    iw.shutdown();
+    verify(dir);
+    TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
+    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat()));
+    iwc.setOpenMode(OpenMode.APPEND);
+    IndexWriter iw2 = new IndexWriter(dir, iwc);
+    iw2.forceMerge(1);
+    iw2.shutdown();
+    verify(dir);
+    dir.close();
+  }
+  
+  private void verify(Directory dir) throws Exception {
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext leaf : ir.leaves()) {
+      AtomicReader leafReader = leaf.reader();
+      assertTerms(leafReader.terms("field1docs"), leafReader.terms("field2freqs"), true);
+      assertTerms(leafReader.terms("field3positions"), leafReader.terms("field4offsets"), true);
+      assertTerms(leafReader.terms("field4offsets"), leafReader.terms("field5payloadsFixed"), true);
+      assertTerms(leafReader.terms("field5payloadsFixed"), leafReader.terms("field6payloadsVariable"), true);
+      assertTerms(leafReader.terms("field6payloadsVariable"), leafReader.terms("field7payloadsFixedOffsets"), true);
+      assertTerms(leafReader.terms("field7payloadsFixedOffsets"), leafReader.terms("field8payloadsVariableOffsets"), true);
+    }
+    ir.close();
+  }
+  
+  // following code is almost an exact dup of code from TestDuelingCodecs: sorry!
+  
+  public void assertTerms(Terms leftTerms, Terms rightTerms, boolean deep) throws Exception {
+    if (leftTerms == null || rightTerms == null) {
+      assertNull(leftTerms);
+      assertNull(rightTerms);
+      return;
+    }
+    assertTermsStatistics(leftTerms, rightTerms);
+    
+    // NOTE: we don't assert hasOffsets/hasPositions/hasPayloads because they are allowed to be different
+
+    TermsEnum leftTermsEnum = leftTerms.iterator(null);
+    TermsEnum rightTermsEnum = rightTerms.iterator(null);
+    assertTermsEnum(leftTermsEnum, rightTermsEnum, true);
+    
+    assertTermsSeeking(leftTerms, rightTerms);
+    
+    if (deep) {
+      int numIntersections = atLeast(3);
+      for (int i = 0; i < numIntersections; i++) {
+        String re = AutomatonTestUtil.randomRegexp(random());
+        CompiledAutomaton automaton = new CompiledAutomaton(new RegExp(re, RegExp.NONE).toAutomaton());
+        if (automaton.type == CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
+          // TODO: test start term too
+          TermsEnum leftIntersection = leftTerms.intersect(automaton, null);
+          TermsEnum rightIntersection = rightTerms.intersect(automaton, null);
+          assertTermsEnum(leftIntersection, rightIntersection, rarely());
+        }
+      }
+    }
+  }
+  
+  private void assertTermsSeeking(Terms leftTerms, Terms rightTerms) throws Exception {
+    TermsEnum leftEnum = null;
+    TermsEnum rightEnum = null;
+    
+    // just an upper bound
+    int numTests = atLeast(20);
+    Random random = random();
+    
+    // collect this number of terms from the left side
+    HashSet<BytesRef> tests = new HashSet<>();
+    int numPasses = 0;
+    while (numPasses < 10 && tests.size() < numTests) {
+      leftEnum = leftTerms.iterator(leftEnum);
+      BytesRef term = null;
+      while ((term = leftEnum.next()) != null) {
+        int code = random.nextInt(10);
+        if (code == 0) {
+          // the term
+          tests.add(BytesRef.deepCopyOf(term));
+        } else if (code == 1) {
+          // truncated subsequence of term
+          term = BytesRef.deepCopyOf(term);
+          if (term.length > 0) {
+            // truncate it
+            term.length = random.nextInt(term.length);
+          }
+        } else if (code == 2) {
+          // term, but ensure a non-zero offset
+          byte newbytes[] = new byte[term.length+5];
+          System.arraycopy(term.bytes, term.offset, newbytes, 5, term.length);
+          tests.add(new BytesRef(newbytes, 5, term.length));
+        }
+      }
+      numPasses++;
+    }
+    
+    ArrayList<BytesRef> shuffledTests = new ArrayList<>(tests);
+    Collections.shuffle(shuffledTests, random);
+    
+    for (BytesRef b : shuffledTests) {
+      leftEnum = leftTerms.iterator(leftEnum);
+      rightEnum = rightTerms.iterator(rightEnum);
+      
+      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
+      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
+      
+      SeekStatus leftStatus;
+      SeekStatus rightStatus;
+      
+      leftStatus = leftEnum.seekCeil(b);
+      rightStatus = rightEnum.seekCeil(b);
+      assertEquals(leftStatus, rightStatus);
+      if (leftStatus != SeekStatus.END) {
+        assertEquals(leftEnum.term(), rightEnum.term());
+      }
+      
+      leftStatus = leftEnum.seekCeil(b);
+      rightStatus = rightEnum.seekCeil(b);
+      assertEquals(leftStatus, rightStatus);
+      if (leftStatus != SeekStatus.END) {
+        assertEquals(leftEnum.term(), rightEnum.term());
+      }
+    }
+  }
+  
+  /** 
+   * checks collection-level statistics on Terms 
+   */
+  public void assertTermsStatistics(Terms leftTerms, Terms rightTerms) throws Exception {
+    if (leftTerms.getDocCount() != -1 && rightTerms.getDocCount() != -1) {
+      assertEquals(leftTerms.getDocCount(), rightTerms.getDocCount());
+    }
+    if (leftTerms.getSumDocFreq() != -1 && rightTerms.getSumDocFreq() != -1) {
+      assertEquals(leftTerms.getSumDocFreq(), rightTerms.getSumDocFreq());
+    }
+    if (leftTerms.getSumTotalTermFreq() != -1 && rightTerms.getSumTotalTermFreq() != -1) {
+      assertEquals(leftTerms.getSumTotalTermFreq(), rightTerms.getSumTotalTermFreq());
+    }
+    if (leftTerms.size() != -1 && rightTerms.size() != -1) {
+      assertEquals(leftTerms.size(), rightTerms.size());
+    }
+  }
+
+  /** 
+   * checks the terms enum sequentially
+   * if deep is false, it does a 'shallow' test that doesnt go down to the docsenums
+   */
+  public void assertTermsEnum(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum, boolean deep) throws Exception {
+    BytesRef term;
+    Bits randomBits = new RandomBits(MAXDOC, random().nextDouble(), random());
+    DocsAndPositionsEnum leftPositions = null;
+    DocsAndPositionsEnum rightPositions = null;
+    DocsEnum leftDocs = null;
+    DocsEnum rightDocs = null;
+    
+    while ((term = leftTermsEnum.next()) != null) {
+      assertEquals(term, rightTermsEnum.next());
+      assertTermStats(leftTermsEnum, rightTermsEnum);
+      if (deep) {
+        // with payloads + off
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
+                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
+                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
+
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
+                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
+                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
+        // with payloads only
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
+                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
+                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
+
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
+                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
+                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
+
+        // with offsets only
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
+                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
+                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
+
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
+                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
+                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
+        
+        // with positions only
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsEnum.FLAG_NONE),
+                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsEnum.FLAG_NONE));
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsEnum.FLAG_NONE),
+                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsEnum.FLAG_NONE));
+
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsEnum.FLAG_NONE),
+                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsEnum.FLAG_NONE));
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsEnum.FLAG_NONE),
+                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsEnum.FLAG_NONE));
+        
+        // with freqs:
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(null, leftDocs),
+            rightDocs = rightTermsEnum.docs(null, rightDocs));
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
+
+        // w/o freqs:
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(null, leftDocs, DocsEnum.FLAG_NONE),
+            rightDocs = rightTermsEnum.docs(null, rightDocs, DocsEnum.FLAG_NONE));
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(randomBits, leftDocs, DocsEnum.FLAG_NONE),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs, DocsEnum.FLAG_NONE));
+        
+        // with freqs:
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(null, leftDocs),
+            rightDocs = rightTermsEnum.docs(null, rightDocs));
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
+
+        // w/o freqs:
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(null, leftDocs, DocsEnum.FLAG_NONE),
+            rightDocs = rightTermsEnum.docs(null, rightDocs, DocsEnum.FLAG_NONE));
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(randomBits, leftDocs, DocsEnum.FLAG_NONE),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs, DocsEnum.FLAG_NONE));
+      }
+    }
+    assertNull(rightTermsEnum.next());
+  }
+  
+  /**
+   * checks term-level statistics
+   */
+  public void assertTermStats(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum) throws Exception {
+    assertEquals(leftTermsEnum.docFreq(), rightTermsEnum.docFreq());
+    if (leftTermsEnum.totalTermFreq() != -1 && rightTermsEnum.totalTermFreq() != -1) {
+      assertEquals(leftTermsEnum.totalTermFreq(), rightTermsEnum.totalTermFreq());
+    }
+  }
+  
+  /**
+   * checks docs + freqs + positions + payloads, sequentially
+   */
+  public void assertDocsAndPositionsEnum(DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
+    if (leftDocs == null || rightDocs == null) {
+      assertNull(leftDocs);
+      assertNull(rightDocs);
+      return;
+    }
+    assertEquals(-1, leftDocs.docID());
+    assertEquals(-1, rightDocs.docID());
+    int docid;
+    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      assertEquals(docid, rightDocs.nextDoc());
+      int freq = leftDocs.freq();
+      assertEquals(freq, rightDocs.freq());
+      for (int i = 0; i < freq; i++) {
+        assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
+        // we don't assert offsets/payloads, they are allowed to be different
+      }
+    }
+    assertEquals(DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
+  }
+  
+  /**
+   * checks docs + freqs, sequentially
+   */
+  public void assertDocsEnum(DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
+    if (leftDocs == null) {
+      assertNull(rightDocs);
+      return;
+    }
+    assertEquals(-1, leftDocs.docID());
+    assertEquals(-1, rightDocs.docID());
+    int docid;
+    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      assertEquals(docid, rightDocs.nextDoc());
+      // we don't assert freqs, they are allowed to be different
+    }
+    assertEquals(DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
+  }
+  
+  /**
+   * checks advancing docs
+   */
+  public void assertDocsSkipping(int docFreq, DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
+    if (leftDocs == null) {
+      assertNull(rightDocs);
+      return;
+    }
+    int docid = -1;
+    int averageGap = MAXDOC / (1+docFreq);
+    int skipInterval = 16;
+
+    while (true) {
+      if (random().nextBoolean()) {
+        // nextDoc()
+        docid = leftDocs.nextDoc();
+        assertEquals(docid, rightDocs.nextDoc());
+      } else {
+        // advance()
+        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random().nextGaussian() * averageGap));
+        docid = leftDocs.advance(skip);
+        assertEquals(docid, rightDocs.advance(skip));
+      }
+      
+      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
+        return;
+      }
+      // we don't assert freqs, they are allowed to be different
+    }
+  }
+  
+  /**
+   * checks advancing docs + positions
+   */
+  public void assertPositionsSkipping(int docFreq, DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
+    if (leftDocs == null || rightDocs == null) {
+      assertNull(leftDocs);
+      assertNull(rightDocs);
+      return;
+    }
+    
+    int docid = -1;
+    int averageGap = MAXDOC / (1+docFreq);
+    int skipInterval = 16;
+
+    while (true) {
+      if (random().nextBoolean()) {
+        // nextDoc()
+        docid = leftDocs.nextDoc();
+        assertEquals(docid, rightDocs.nextDoc());
+      } else {
+        // advance()
+        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random().nextGaussian() * averageGap));
+        docid = leftDocs.advance(skip);
+        assertEquals(docid, rightDocs.advance(skip));
+      }
+      
+      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
+        return;
+      }
+      int freq = leftDocs.freq();
+      assertEquals(freq, rightDocs.freq());
+      for (int i = 0; i < freq; i++) {
+        assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
+        // we don't compare the payloads, its allowed that one is empty etc
+      }
+    }
+  }
+  
+  private static class RandomBits implements Bits {
+    FixedBitSet bits;
+    
+    RandomBits(int maxDoc, double pctLive, Random random) {
+      bits = new FixedBitSet(maxDoc);
+      for (int i = 0; i < maxDoc; i++) {
+        if (random.nextDouble() <= pctLive) {        
+          bits.set(i);
+        }
+      }
+    }
+    
+    @Override
+    public boolean get(int index) {
+      return bits.get(index);
+    }
+
+    @Override
+    public int length() {
+      return bits.length();
+    }
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410PostingsFormat3.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene49.Lucene49Codec;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
@@ -82,7 +83,7 @@
     IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
     final DocValuesFormat fast = DocValuesFormat.forName("Lucene49");
     final DocValuesFormat slow = DocValuesFormat.forName("SimpleText");
-    iwc.setCodec(new Lucene49Codec() {
+    iwc.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         if ("dv1".equals(field)) {
Index: lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java	(working copy)
@@ -21,10 +21,11 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.codecs.lucene49.Lucene49Codec;
 import org.apache.lucene.codecs.memory.MemoryPostingsFormat;
-import org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat;
+import org.apache.lucene.codecs.pulsing.Pulsing410PostingsFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -200,8 +201,8 @@
 
   }
 
-  public static class MockCodec extends Lucene49Codec {
-    final PostingsFormat lucene40 = new Lucene41PostingsFormat();
+  public static class MockCodec extends Lucene410Codec {
+    final PostingsFormat lucene40 = new Lucene410PostingsFormat();
     final PostingsFormat simpleText = new SimpleTextPostingsFormat();
     final PostingsFormat memory = new MemoryPostingsFormat();
     
@@ -217,8 +218,8 @@
     }
   }
 
-  public static class MockCodec2 extends Lucene49Codec {
-    final PostingsFormat lucene40 = new Lucene41PostingsFormat();
+  public static class MockCodec2 extends Lucene410Codec {
+    final PostingsFormat lucene40 = new Lucene410PostingsFormat();
     final PostingsFormat simpleText = new SimpleTextPostingsFormat();
     
     @Override
@@ -268,13 +269,13 @@
   }
   
   public void testSameCodecDifferentInstance() throws Exception {
-    Codec codec = new Lucene49Codec() {
+    Codec codec = new Lucene410Codec() {
       @Override
       public PostingsFormat getPostingsFormatForField(String field) {
         if ("id".equals(field)) {
-          return new Pulsing41PostingsFormat(1);
+          return new Pulsing410PostingsFormat(1);
         } else if ("date".equals(field)) {
-          return new Pulsing41PostingsFormat(1);
+          return new Pulsing410PostingsFormat(1);
         } else {
           return super.getPostingsFormatForField(field);
         }
@@ -284,13 +285,13 @@
   }
   
   public void testSameCodecDifferentParams() throws Exception {
-    Codec codec = new Lucene49Codec() {
+    Codec codec = new Lucene410Codec() {
       @Override
       public PostingsFormat getPostingsFormatForField(String field) {
         if ("id".equals(field)) {
-          return new Pulsing41PostingsFormat(1);
+          return new Pulsing410PostingsFormat(1);
         } else if ("date".equals(field)) {
-          return new Pulsing41PostingsFormat(2);
+          return new Pulsing410PostingsFormat(2);
         } else {
           return super.getPostingsFormatForField(field);
         }
Index: lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -28,8 +28,8 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
-import org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
+import org.apache.lucene.codecs.pulsing.Pulsing410PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -1073,9 +1073,9 @@
     aux2.close();
   }
 
-  private static final class CustomPerFieldCodec extends Lucene49Codec {
+  private static final class CustomPerFieldCodec extends Lucene410Codec {
     private final PostingsFormat simpleTextFormat = PostingsFormat.forName("SimpleText");
-    private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+    private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene410");
     private final PostingsFormat memoryFormat = PostingsFormat.forName("Memory");
 
     @Override
@@ -1124,7 +1124,7 @@
   
   private static final class UnRegisteredCodec extends FilterCodec {
     public UnRegisteredCodec() {
-      super("NotRegistered", new Lucene49Codec());
+      super("NotRegistered", new Lucene410Codec());
     }
   }
   
@@ -1153,7 +1153,7 @@
       Directory dir = newDirectory();
       IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT,
           new MockAnalyzer(random()));
-      conf.setCodec(TestUtil.alwaysPostingsFormat(new Pulsing41PostingsFormat(1 + random().nextInt(20))));
+      conf.setCodec(TestUtil.alwaysPostingsFormat(new Pulsing410PostingsFormat(1 + random().nextInt(20))));
       IndexWriter w = new IndexWriter(dir, conf);
       try {
         w.addIndexes(toAdd);
Index: lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java	(working copy)
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
@@ -39,7 +39,7 @@
   public void test() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setCodec(new Lucene49Codec());
+    conf.setCodec(new Lucene410Codec());
     RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
     Document doc = new Document();
     // these fields should sometimes get term vectors, etc
Index: lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java	(working copy)
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
@@ -39,7 +39,7 @@
   public void test() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setCodec(new Lucene49Codec());
+    conf.setCodec(new Lucene410Codec());
     RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
     Document doc = new Document();
     // these fields should sometimes get term vectors, etc
Index: lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java	(working copy)
@@ -17,7 +17,7 @@
 import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
 import org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat;
 import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
@@ -494,7 +494,7 @@
   public void testDifferentDVFormatPerField() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new Lucene49DocValuesFormat();
@@ -1062,7 +1062,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     conf.setMergePolicy(NoMergePolicy.INSTANCE); // disable merges to simplify test assertions.
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new Lucene49DocValuesFormat();
@@ -1079,7 +1079,7 @@
     // change format
     conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     conf.setMergePolicy(NoMergePolicy.INSTANCE); // disable merges to simplify test assertions.
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new AssertingDocValuesFormat();
Index: lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(working copy)
@@ -24,7 +24,7 @@
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
@@ -358,7 +358,7 @@
     iwc.setMergeScheduler(new TrackingCMS(atLeastOneMerge));
     if (TestUtil.getPostingsFormat("id").equals("SimpleText")) {
       // no
-      iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat()));
+      iwc.setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat()));
     }
     IndexWriter w = new IndexWriter(d, iwc);
     for(int i=0;i<1000;i++) {
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(working copy)
@@ -628,7 +628,7 @@
     MockDirectoryWrapper dir = newMockDirectory();
 
     IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(Codec.forName("Lucene49"));
+    iwc.setCodec(Codec.forName("Lucene410"));
     IndexWriter w = new IndexWriter(dir, iwc);
     Document doc = new Document();
     doc.add(newStringField("id", "id", Field.Store.NO));
Index: lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(working copy)
@@ -50,7 +50,7 @@
   public void setUp() throws Exception {
     super.setUp();
 
-    // for now its SimpleText vs Lucene49(random postings format)
+    // for now its SimpleText vs Lucene410(random postings format)
     // as this gives the best overall coverage. when we have more
     // codecs we should probably pick 2 from Codec.availableCodecs()
     
Index: lucene/core/src/test/org/apache/lucene/index/TestFlex.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestFlex.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestFlex.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.store.*;
 import org.apache.lucene.analysis.*;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.document.*;
 import org.apache.lucene.util.*;
 
@@ -65,7 +65,7 @@
   public void testTermOrd() throws Exception {
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT,
-                                                             new MockAnalyzer(random())).setCodec(TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())));
+                                                             new MockAnalyzer(random())).setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat())));
     Document doc = new Document();
     doc.add(newTextField("f", "a b c", Field.Store.NO));
     w.addDocument(doc);
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments.java	(working copy)
@@ -276,7 +276,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     iwc.setRAMBufferSizeMB(.2);
-    Codec codec = Codec.forName("Lucene49");
+    Codec codec = Codec.forName("Lucene410");
     iwc.setCodec(codec);
     iwc.setMergePolicy(NoMergePolicy.INSTANCE);
     final IndexWriter w = new IndexWriter(dir, iwc);
Index: lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.store.Directory;
@@ -68,7 +68,7 @@
 
   public void testSimpleSkip() throws IOException {
     Directory dir = new CountingRAMDirectory(new RAMDirectory());
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new PayloadAnalyzer()).setCodec(TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())).setMergePolicy(newLogMergePolicy()));
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new PayloadAnalyzer()).setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat())).setMergePolicy(newLogMergePolicy()));
     Term term = new Term("test", "a");
     for (int i = 0; i < 5000; i++) {
       Document d1 = new Document();
Index: lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java	(working copy)
@@ -17,7 +17,7 @@
 import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
 import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
 import org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
@@ -479,7 +479,7 @@
   public void testDifferentDVFormatPerField() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new Lucene49DocValuesFormat();
@@ -1057,7 +1057,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     conf.setMergePolicy(NoMergePolicy.INSTANCE); // disable merges to simplify test assertions.
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new Lucene49DocValuesFormat();
@@ -1074,7 +1074,7 @@
     // change format
     conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     conf.setMergePolicy(NoMergePolicy.INSTANCE); // disable merges to simplify test assertions.
-    conf.setCodec(new Lucene49Codec() {
+    conf.setCodec(new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return new AssertingDocValuesFormat();
Index: lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
@@ -75,7 +75,7 @@
 
   public void testPrevTermAtEnd() throws IOException
   {
-    IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())));
+    IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat())));
     addDoc(writer, "aaa bbb");
     writer.shutdown();
     SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
Index: lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java	(working copy)
@@ -220,7 +220,7 @@
     TieredMergePolicy tmp = (TieredMergePolicy) iwc.getMergePolicy();
     tmp.setFloorSegmentMB(0.00001);
     // We need stable sizes for each segment:
-    iwc.setCodec(Codec.forName("Lucene49"));
+    iwc.setCodec(Codec.forName("Lucene410"));
     iwc.setMergeScheduler(new SerialMergeScheduler());
     iwc.setMaxBufferedDocs(100);
     iwc.setRAMBufferSizeMB(-1);
Index: lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java	(revision 1608975)
+++ lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java	(working copy)
@@ -26,8 +26,8 @@
 public class TestNamedSPILoader extends LuceneTestCase {
   
   public void testLookup() {
-    Codec codec = Codec.forName("Lucene49");
-    assertEquals("Lucene49", codec.getName());
+    Codec codec = Codec.forName("Lucene410");
+    assertEquals("Lucene410", codec.getName());
   }
   
   // we want an exception if its not found.
@@ -40,6 +40,6 @@
   
   public void testAvailableServices() {
     Set<String> codecs = Codec.availableCodecs();
-    assertTrue(codecs.contains("Lucene49"));
+    assertTrue(codecs.contains("Lucene410"));
   }
 }
Index: lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 1608975)
+++ lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -36,7 +36,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -149,7 +149,7 @@
     Directory ramdir = new RAMDirectory();
     Analyzer analyzer = randomAnalyzer();
     IndexWriter writer = new IndexWriter(ramdir,
-                                         new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setCodec(TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())));
+                                         new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setCodec(TestUtil.alwaysPostingsFormat(new Lucene410PostingsFormat())));
     Document doc = new Document();
     Field field1 = newTextField("foo", fooField.toString(), Field.Store.NO);
     Field field2 = newTextField("term", termField.toString(), Field.Store.NO);
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java	(revision 1608975)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java	(working copy)
@@ -32,7 +32,7 @@
 import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -184,7 +184,7 @@
    *  codec to use. */
   protected IndexWriterConfig getIndexWriterConfig(Version matchVersion, Analyzer indexAnalyzer, IndexWriterConfig.OpenMode openMode) {
     IndexWriterConfig iwc = new IndexWriterConfig(matchVersion, indexAnalyzer);
-    iwc.setCodec(new Lucene49Codec());
+    iwc.setCodec(new Lucene410Codec());
     iwc.setOpenMode(openMode);
 
     // This way all merged segments will be sorted at
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java	(working copy)
@@ -23,10 +23,10 @@
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 
 /**
- * Acts like {@link Lucene49Codec} but with additional asserts.
+ * Acts like {@link Lucene410Codec} but with additional asserts.
  */
 public final class AssertingCodec extends FilterCodec {
 
@@ -37,7 +37,7 @@
   private final NormsFormat norms = new AssertingNormsFormat();
 
   public AssertingCodec() {
-    super("Asserting", new Lucene49Codec());
+    super("Asserting", new Lucene410Codec());
   }
 
   @Override
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.index.AssertingAtomicReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
@@ -36,10 +36,10 @@
 import org.apache.lucene.util.BytesRef;
 
 /**
- * Just like {@link Lucene41PostingsFormat} but with additional asserts.
+ * Just like {@link Lucene410PostingsFormat} but with additional asserts.
  */
 public final class AssertingPostingsFormat extends PostingsFormat {
-  private final PostingsFormat in = new Lucene41PostingsFormat();
+  private final PostingsFormat in = new Lucene410PostingsFormat();
   
   public AssertingPostingsFormat() {
     super("Asserting");
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene410Postings.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene410Postings.java	(revision 1608464)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene410Postings.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
@@ -29,12 +30,12 @@
 
 /**
  * A class used for testing {@link BloomFilteringPostingsFormat} with a concrete
- * delegate (Lucene41). Creates a Bloom filter on ALL fields and with tiny
+ * delegate (Lucene410). Creates a Bloom filter on ALL fields and with tiny
  * amounts of memory reserved for the filter. DO NOT USE IN A PRODUCTION
  * APPLICATION This is not a realistic application of Bloom Filters as they
  * ordinarily are larger and operate on only primary key type fields.
  */
-public final class TestBloomFilteredLucene41Postings extends PostingsFormat {
+public final class TestBloomFilteredLucene410Postings extends PostingsFormat {
   
   private BloomFilteringPostingsFormat delegate;
   
@@ -54,9 +55,9 @@
     }
   }
   
-  public TestBloomFilteredLucene41Postings() {
-    super("TestBloomFilteredLucene41Postings");
-    delegate = new BloomFilteringPostingsFormat(new Lucene41PostingsFormat(),
+  public TestBloomFilteredLucene410Postings() {
+    super("TestBloomFilteredLucene410Postings");
+    delegate = new BloomFilteringPostingsFormat(new Lucene410PostingsFormat(),
         new LowMemoryBloomFactory());
   }
   
@@ -74,6 +75,6 @@
 
   @Override
   public String toString() {
-    return "TestBloomFilteredLucene41Postings(" + delegate + ")";
+    return "TestBloomFilteredLucene410Postings(" + delegate + ")";
   }
 }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene41Postings.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene41Postings.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/TestBloomFilteredLucene41Postings.java	(working copy)
@@ -1,79 +0,0 @@
-package org.apache.lucene.codecs.bloom;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * A class used for testing {@link BloomFilteringPostingsFormat} with a concrete
- * delegate (Lucene41). Creates a Bloom filter on ALL fields and with tiny
- * amounts of memory reserved for the filter. DO NOT USE IN A PRODUCTION
- * APPLICATION This is not a realistic application of Bloom Filters as they
- * ordinarily are larger and operate on only primary key type fields.
- */
-public final class TestBloomFilteredLucene41Postings extends PostingsFormat {
-  
-  private BloomFilteringPostingsFormat delegate;
-  
-  // Special class used to avoid OOM exceptions where Junit tests create many
-  // fields.
-  static class LowMemoryBloomFactory extends BloomFilterFactory {
-    @Override
-    public FuzzySet getSetForField(SegmentWriteState state,FieldInfo info) {
-      return FuzzySet.createSetBasedOnMaxMemory(1024);
-    }
-    
-    @Override
-    public boolean isSaturated(FuzzySet bloomFilter, FieldInfo fieldInfo) {
-      // For test purposes always maintain the BloomFilter - even past the point
-      // of usefulness when all bits are set
-      return false;
-    }
-  }
-  
-  public TestBloomFilteredLucene41Postings() {
-    super("TestBloomFilteredLucene41Postings");
-    delegate = new BloomFilteringPostingsFormat(new Lucene41PostingsFormat(),
-        new LowMemoryBloomFactory());
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
-      throws IOException {
-    return delegate.fieldsConsumer(state);
-  }
-  
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state)
-      throws IOException {
-    return delegate.fieldsProducer(state);
-  }
-
-  @Override
-  public String toString() {
-    return "TestBloomFilteredLucene41Postings(" + delegate + ")";
-  }
-}
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java	(working copy)
@@ -25,8 +25,8 @@
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat;
 import org.apache.lucene.codecs.lucene49.Lucene49NormsFormat;
 
@@ -36,7 +36,7 @@
 public class CheapBastardCodec extends FilterCodec {
   
   // TODO: would be better to have no terms index at all and bsearch a terms dict
-  private final PostingsFormat postings = new Lucene41PostingsFormat(100, 200);
+  private final PostingsFormat postings = new Lucene410PostingsFormat(100, 200);
   // uncompressing versions, waste lots of disk but no ram
   private final StoredFieldsFormat storedFields = new Lucene40StoredFieldsFormat();
   private final TermVectorsFormat termVectors = new Lucene40TermVectorsFormat();
@@ -44,7 +44,7 @@
   private final NormsFormat norms = new Lucene49NormsFormat();
 
   public CheapBastardCodec() {
-    super("CheapBastard", new Lucene49Codec());
+    super("CheapBastard", new Lucene410Codec());
   }
   
   @Override
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/CompressingCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/CompressingCodec.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/CompressingCodec.java	(working copy)
@@ -23,13 +23,13 @@
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.compressing.dummy.DummyCompressingCodec;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
 
 /**
  * A codec that uses {@link CompressingStoredFieldsFormat} for its stored
- * fields and delegates to {@link Lucene49Codec} for everything else.
+ * fields and delegates to {@link Lucene410Codec} for everything else.
  */
 public abstract class CompressingCodec extends FilterCodec {
 
@@ -73,7 +73,7 @@
    * Creates a compressing codec with a given segment suffix
    */
   public CompressingCodec(String name, String segmentSuffix, CompressionMode compressionMode, int chunkSize) {
-    super(name, new Lucene49Codec());
+    super(name, new Lucene410Codec());
     this.storedFieldsFormat = new CompressingStoredFieldsFormat(name, segmentSuffix, compressionMode, chunkSize);
     this.termVectorsFormat = new CompressingTermVectorsFormat(name, segmentSuffix, compressionMode, chunkSize);
   }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410ords/Lucene410WithOrds.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410ords/Lucene410WithOrds.java	(revision 1608464)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410ords/Lucene410WithOrds.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.codecs.lucene41ords;
+package org.apache.lucene.codecs.lucene410ords;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -30,35 +30,35 @@
 import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
 import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsReader;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.util.BytesRef;
 
-// TODO: we could make separate base class that can wrapp
+// TODO: we could make separate base class that can wrap
 // any PostingsBaseFormat and make it ord-able...
 
 /**
- * Customized version of {@link Lucene41PostingsFormat} that uses
+ * Customized version of {@link Lucene410PostingsFormat} that uses
  * {@link FixedGapTermsIndexWriter}.
  */
-public final class Lucene41WithOrds extends PostingsFormat {
+public final class Lucene410WithOrds extends PostingsFormat {
   final int termIndexInterval;
   
-  public Lucene41WithOrds() {
+  public Lucene410WithOrds() {
     this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
   }
   
-  public Lucene41WithOrds(int termIndexInterval) {
-    super("Lucene41WithOrds");
+  public Lucene410WithOrds(int termIndexInterval) {
+    super("Lucene410WithOrds");
     this.termIndexInterval = termIndexInterval;
   }
 
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
+    PostingsWriterBase docs = new Lucene410PostingsWriter(state);
 
     // TODO: should we make the terms index more easily
     // pluggable?  Ie so that this codec would record which
@@ -95,7 +95,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene410PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     TermsIndexReaderBase indexReader;
 
     boolean success = false;
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapDocFreqInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapDocFreqInterval.java	(revision 1608464)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapDocFreqInterval.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.codecs.lucene41vargap;
+package org.apache.lucene.codecs.lucene410vargap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -31,9 +31,9 @@
 import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
 import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsReader;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -41,20 +41,20 @@
 // any PostingsBaseFormat and make it ord-able...
 
 /**
- * Customized version of {@link Lucene41PostingsFormat} that uses
+ * Customized version of {@link Lucene410PostingsFormat} that uses
  * {@link VariableGapTermsIndexWriter} with a fixed interval, but
  * forcing high docfreq terms to be indexed terms.
  */
-public final class Lucene41VarGapDocFreqInterval extends PostingsFormat {
+public final class Lucene410VarGapDocFreqInterval extends PostingsFormat {
   final int termIndexInterval;
   final int docFreqThreshold;
   
-  public Lucene41VarGapDocFreqInterval() {
+  public Lucene410VarGapDocFreqInterval() {
     this(1000000, FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
   }
   
-  public Lucene41VarGapDocFreqInterval(int docFreqThreshold, int termIndexInterval) {
-    super("Lucene41VarGapFixedInterval");
+  public Lucene410VarGapDocFreqInterval(int docFreqThreshold, int termIndexInterval) {
+    super("Lucene410VarGapFixedInterval");
     this.termIndexInterval = termIndexInterval;
     this.docFreqThreshold = docFreqThreshold;
   }
@@ -61,7 +61,7 @@
 
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
+    PostingsWriterBase docs = new Lucene410PostingsWriter(state);
 
     // TODO: should we make the terms index more easily
     // pluggable?  Ie so that this codec would record which
@@ -98,7 +98,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene410PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     TermsIndexReaderBase indexReader;
 
     boolean success = false;
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapFixedInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapFixedInterval.java	(revision 1608464)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene410vargap/Lucene410VarGapFixedInterval.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.codecs.lucene41vargap;
+package org.apache.lucene.codecs.lucene410vargap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -31,9 +31,9 @@
 import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
 import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsReader;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -41,24 +41,24 @@
 // any PostingsBaseFormat and make it ord-able...
 
 /**
- * Customized version of {@link Lucene41PostingsFormat} that uses
+ * Customized version of {@link Lucene410PostingsFormat} that uses
  * {@link VariableGapTermsIndexWriter} with a fixed interval.
  */
-public final class Lucene41VarGapFixedInterval extends PostingsFormat {
+public final class Lucene410VarGapFixedInterval extends PostingsFormat {
   final int termIndexInterval;
   
-  public Lucene41VarGapFixedInterval() {
+  public Lucene410VarGapFixedInterval() {
     this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
   }
   
-  public Lucene41VarGapFixedInterval(int termIndexInterval) {
-    super("Lucene41VarGapFixedInterval");
+  public Lucene410VarGapFixedInterval(int termIndexInterval) {
+    super("Lucene410VarGapFixedInterval");
     this.termIndexInterval = termIndexInterval;
   }
 
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
+    PostingsWriterBase docs = new Lucene410PostingsWriter(state);
 
     // TODO: should we make the terms index more easily
     // pluggable?  Ie so that this codec would record which
@@ -95,7 +95,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene410PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     TermsIndexReaderBase indexReader;
 
     boolean success = false;
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(working copy)
@@ -1,142 +0,0 @@
-package org.apache.lucene.codecs.lucene41ords;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blockterms.BlockTermsReader;
-import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
-import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
-import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
-import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
-
-// TODO: we could make separate base class that can wrapp
-// any PostingsBaseFormat and make it ord-able...
-
-/**
- * Customized version of {@link Lucene41PostingsFormat} that uses
- * {@link FixedGapTermsIndexWriter}.
- */
-public final class Lucene41WithOrds extends PostingsFormat {
-  final int termIndexInterval;
-  
-  public Lucene41WithOrds() {
-    this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
-  }
-  
-  public Lucene41WithOrds(int termIndexInterval) {
-    super("Lucene41WithOrds");
-    this.termIndexInterval = termIndexInterval;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
-
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    TermsIndexWriterBase indexWriter;
-    boolean success = false;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state, termIndexInterval);
-      success = true;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-
-    success = false;
-    try {
-      // Must use BlockTermsWriter (not BlockTree) because
-      // BlockTree doens't support ords (yet)...
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          docs.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-    TermsIndexReaderBase indexReader;
-
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                 state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postings.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-}
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/package.html
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/package.html	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/package.html	(working copy)
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec for testing that supports {@link org.apache.lucene.index.TermsEnum#ord()}
-</body>
-</html>
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java	(working copy)
@@ -1,144 +0,0 @@
-package org.apache.lucene.codecs.lucene41vargap;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blockterms.BlockTermsReader;
-import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
-import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
-import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
-import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
-import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
-import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-// TODO: we could make separate base class that can wrapp
-// any PostingsBaseFormat and make it ord-able...
-
-/**
- * Customized version of {@link Lucene41PostingsFormat} that uses
- * {@link VariableGapTermsIndexWriter} with a fixed interval, but
- * forcing high docfreq terms to be indexed terms.
- */
-public final class Lucene41VarGapDocFreqInterval extends PostingsFormat {
-  final int termIndexInterval;
-  final int docFreqThreshold;
-  
-  public Lucene41VarGapDocFreqInterval() {
-    this(1000000, FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
-  }
-  
-  public Lucene41VarGapDocFreqInterval(int docFreqThreshold, int termIndexInterval) {
-    super("Lucene41VarGapFixedInterval");
-    this.termIndexInterval = termIndexInterval;
-    this.docFreqThreshold = docFreqThreshold;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
-
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    TermsIndexWriterBase indexWriter;
-    boolean success = false;
-    try {
-      indexWriter = new VariableGapTermsIndexWriter(state, new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThreshold, termIndexInterval));
-      success = true;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-
-    success = false;
-    try {
-      // Must use BlockTermsWriter (not BlockTree) because
-      // BlockTree doens't support ords (yet)...
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          docs.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-    TermsIndexReaderBase indexReader;
-
-    boolean success = false;
-    try {
-      indexReader = new VariableGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postings.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-}
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java	(working copy)
@@ -1,141 +0,0 @@
-package org.apache.lucene.codecs.lucene41vargap;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blockterms.BlockTermsReader;
-import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
-import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
-import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
-import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
-import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
-import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-// TODO: we could make separate base class that can wrapp
-// any PostingsBaseFormat and make it ord-able...
-
-/**
- * Customized version of {@link Lucene41PostingsFormat} that uses
- * {@link VariableGapTermsIndexWriter} with a fixed interval.
- */
-public final class Lucene41VarGapFixedInterval extends PostingsFormat {
-  final int termIndexInterval;
-  
-  public Lucene41VarGapFixedInterval() {
-    this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
-  }
-  
-  public Lucene41VarGapFixedInterval(int termIndexInterval) {
-    super("Lucene41VarGapFixedInterval");
-    this.termIndexInterval = termIndexInterval;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
-
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    TermsIndexWriterBase indexWriter;
-    boolean success = false;
-    try {
-      indexWriter = new VariableGapTermsIndexWriter(state, new VariableGapTermsIndexWriter.EveryNTermSelector(termIndexInterval));
-      success = true;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-
-    success = false;
-    try {
-      // Must use BlockTermsWriter (not BlockTree) because
-      // BlockTree doens't support ords (yet)...
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          docs.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-    TermsIndexReaderBase indexReader;
-
-    boolean success = false;
-    try {
-      indexReader = new VariableGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postings.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-}
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html	(working copy)
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codecs for testing that support {@link org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader}
-</body>
-</html>
Index: lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java	(working copy)
@@ -40,12 +40,11 @@
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.store.Directory;
@@ -57,8 +56,6 @@
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util.RamUsageTester;
 import org.apache.lucene.util.TestUtil;
 import org.junit.AfterClass;
@@ -1398,7 +1395,7 @@
     // TODO: would be better to use / delegate to the current
     // Codec returned by getCodec()
 
-    iwc.setCodec(new Lucene49Codec() {
+    iwc.setCodec(new Lucene410Codec() {
         @Override
         public PostingsFormat getPostingsFormatForField(String field) {
 
@@ -1452,13 +1449,17 @@
                     TermsEnum termsEnum = terms.iterator(null);
                     DocsEnum docs = null;
                     while(termsEnum.next() != null) {
+                      final boolean doPos;
                       BytesRef term = termsEnum.term();
                       if (random().nextBoolean()) {
                         docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);
+                        doPos = false;
                       } else if (docs instanceof DocsAndPositionsEnum) {
                         docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);
+                        doPos = true;
                       } else {
                         docs = termsEnum.docsAndPositions(null, null, 0);
+                        doPos = true;
                       }
                       int docFreq = 0;
                       long totalTermFreq = 0;
@@ -1465,7 +1466,7 @@
                       while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
                         docFreq++;
                         totalTermFreq += docs.freq();
-                        if (docs instanceof DocsAndPositionsEnum) {
+                        if (doPos) {
                           DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;
                           int limit = TestUtil.nextInt(random(), 1, docs.freq());
                           for(int i=0;i<limit;i++) {
@@ -1503,12 +1504,16 @@
                     // Also test seeking the TermsEnum:
                     for(String term : termFreqs.keySet()) {
                       if (termsEnum.seekExact(new BytesRef(term))) {
+                        final boolean doPos;
                         if (random().nextBoolean()) {
                           docs = termsEnum.docs(null, docs, DocsEnum.FLAG_FREQS);
+                          doPos = false;
                         } else if (docs instanceof DocsAndPositionsEnum) {
                           docs = termsEnum.docsAndPositions(null, (DocsAndPositionsEnum) docs, 0);
+                          doPos = true;
                         } else {
                           docs = termsEnum.docsAndPositions(null, null, 0);
+                          doPos = true;
                         }
 
                         int docFreq = 0;
@@ -1516,7 +1521,7 @@
                         while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
                           docFreq++;
                           totalTermFreq += docs.freq();
-                          if (docs instanceof DocsAndPositionsEnum) {
+                          if (doPos) {
                             DocsAndPositionsEnum posEnum = (DocsAndPositionsEnum) docs;
                             int limit = TestUtil.nextInt(random(), 1, docs.freq());
                             for(int i=0;i<limit;i++) {
Index: lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java	(working copy)
@@ -32,7 +32,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleField;
@@ -491,7 +491,7 @@
     // get another codec, other than the default: so we are merging segments across different codecs
     final Codec otherCodec;
     if ("SimpleText".equals(Codec.getDefault().getName())) {
-      otherCodec = new Lucene49Codec();
+      otherCodec = new Lucene410Codec();
     } else {
       otherCodec = new SimpleTextCodec();
     }
Index: lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(working copy)
@@ -31,23 +31,23 @@
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingPostingsFormat;
-import org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
-import org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds;
-import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval;
-import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.bloom.TestBloomFilteredLucene410Postings;
+import org.apache.lucene.codecs.lucene410.Lucene410PostingsFormat;
+import org.apache.lucene.codecs.lucene410ords.Lucene410WithOrds;
+import org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapDocFreqInterval;
+import org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapFixedInterval;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat;
 import org.apache.lucene.codecs.memory.DirectPostingsFormat;
 import org.apache.lucene.codecs.memory.FSTOrdPostingsFormat;
-import org.apache.lucene.codecs.memory.FSTOrdPulsing41PostingsFormat;
+import org.apache.lucene.codecs.memory.FSTOrdPulsing410PostingsFormat;
 import org.apache.lucene.codecs.memory.FSTPostingsFormat;
-import org.apache.lucene.codecs.memory.FSTPulsing41PostingsFormat;
+import org.apache.lucene.codecs.memory.FSTPulsing410PostingsFormat;
 import org.apache.lucene.codecs.memory.MemoryDocValuesFormat;
 import org.apache.lucene.codecs.memory.MemoryPostingsFormat;
 import org.apache.lucene.codecs.mockrandom.MockRandomPostingsFormat;
 import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
-import org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat;
+import org.apache.lucene.codecs.pulsing.Pulsing410PostingsFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
 import org.apache.lucene.util.LuceneTestCase;
@@ -62,7 +62,7 @@
  * documents in different orders and the test will still be deterministic
  * and reproducable.
  */
-public class RandomCodec extends Lucene49Codec {
+public class RandomCodec extends Lucene410Codec {
   /** Shuffled list of postings formats to use for new mappings */
   private List<PostingsFormat> formats = new ArrayList<>();
   
@@ -127,25 +127,25 @@
     int lowFreqCutoff = TestUtil.nextInt(random, 2, 100);
 
     add(avoidCodecs,
-        new Lucene41PostingsFormat(minItemsPerBlock, maxItemsPerBlock),
+        new Lucene410PostingsFormat(minItemsPerBlock, maxItemsPerBlock),
         new FSTPostingsFormat(),
         new FSTOrdPostingsFormat(),
-        new FSTPulsing41PostingsFormat(1 + random.nextInt(20)),
-        new FSTOrdPulsing41PostingsFormat(1 + random.nextInt(20)),
+        new FSTPulsing410PostingsFormat(1 + random.nextInt(20)),
+        new FSTOrdPulsing410PostingsFormat(1 + random.nextInt(20)),
         new DirectPostingsFormat(LuceneTestCase.rarely(random) ? 1 : (LuceneTestCase.rarely(random) ? Integer.MAX_VALUE : maxItemsPerBlock),
                                  LuceneTestCase.rarely(random) ? 1 : (LuceneTestCase.rarely(random) ? Integer.MAX_VALUE : lowFreqCutoff)),
-        new Pulsing41PostingsFormat(1 + random.nextInt(20), minItemsPerBlock, maxItemsPerBlock),
+        new Pulsing410PostingsFormat(1 + random.nextInt(20), minItemsPerBlock, maxItemsPerBlock),
         // add pulsing again with (usually) different parameters
-        new Pulsing41PostingsFormat(1 + random.nextInt(20), minItemsPerBlock, maxItemsPerBlock),
+        new Pulsing410PostingsFormat(1 + random.nextInt(20), minItemsPerBlock, maxItemsPerBlock),
         //TODO as a PostingsFormat which wraps others, we should allow TestBloomFilteredLucene41Postings to be constructed 
         //with a choice of concrete PostingsFormats. Maybe useful to have a generic means of marking and dealing 
         //with such "wrapper" classes?
-        new TestBloomFilteredLucene41Postings(),                
+        new TestBloomFilteredLucene410Postings(),                
         new MockRandomPostingsFormat(random),
         new NestedPulsingPostingsFormat(),
-        new Lucene41WithOrds(TestUtil.nextInt(random, 1, 1000)),
-        new Lucene41VarGapFixedInterval(TestUtil.nextInt(random, 1, 1000)),
-        new Lucene41VarGapDocFreqInterval(TestUtil.nextInt(random, 1, 100), TestUtil.nextInt(random, 1, 1000)),
+        new Lucene410WithOrds(TestUtil.nextInt(random, 1, 1000)),
+        new Lucene410VarGapFixedInterval(TestUtil.nextInt(random, 1, 1000)),
+        new Lucene410VarGapDocFreqInterval(TestUtil.nextInt(random, 1, 100), TestUtil.nextInt(random, 1, 1000)),
         new SimpleTextPostingsFormat(),
         new AssertingPostingsFormat(),
         new MemoryPostingsFormat(true, random.nextFloat()),
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(working copy)
@@ -37,6 +37,7 @@
 import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
 import org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
 import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
 import org.apache.lucene.codecs.lucene46.Lucene46RWCodec;
@@ -200,6 +201,7 @@
       codec = Codec.forName("Lucene46");
       LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
       assert codec instanceof Lucene46RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
+      // nocommit: add Lucene4.9 impersonation + test
     } else if (("random".equals(TEST_POSTINGSFORMAT) == false) || ("random".equals(TEST_DOCVALUESFORMAT) == false)) {
       // the user wired postings or DV: this is messy
       // refactor into RandomCodec....
@@ -206,7 +208,7 @@
       
       final PostingsFormat format;
       if ("random".equals(TEST_POSTINGSFORMAT)) {
-        format = PostingsFormat.forName("Lucene41");
+        format = PostingsFormat.forName("Lucene410");
       } else if ("MockRandom".equals(TEST_POSTINGSFORMAT)) {
         format = new MockRandomPostingsFormat(new Random(random.nextLong()));
       } else {
@@ -221,7 +223,7 @@
         dvFormat = DocValuesFormat.forName(TEST_DOCVALUESFORMAT);
       }
       
-      codec = new Lucene49Codec() {       
+      codec = new Lucene410Codec() {       
         @Override
         public PostingsFormat getPostingsFormatForField(String field) {
           return format;
@@ -239,8 +241,8 @@
       };
     } else if ("SimpleText".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 9 && LuceneTestCase.rarely(random) && !shouldAvoidCodec("SimpleText"))) {
       codec = new SimpleTextCodec();
-    } else if ("CheapBastard".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 8 && !shouldAvoidCodec("CheapBastard") && !shouldAvoidCodec("Lucene41"))) {
-      // we also avoid this codec if Lucene41 is avoided, since thats the postings format it uses.
+    } else if ("CheapBastard".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 8 && !shouldAvoidCodec("CheapBastard") && !shouldAvoidCodec("Lucene410"))) {
+      // we also avoid this codec if Lucene410 is avoided, since thats the postings format it uses.
       codec = new CheapBastardCodec();
     } else if ("Asserting".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 7 && !shouldAvoidCodec("Asserting"))) {
       codec = new AssertingCodec();
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java	(revision 1608975)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java	(working copy)
@@ -46,7 +46,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
@@ -682,7 +682,7 @@
     if (LuceneTestCase.VERBOSE) {
       System.out.println("forcing postings format to:" + format);
     }
-    return new Lucene49Codec() {
+    return new Lucene410Codec() {
       @Override
       public PostingsFormat getPostingsFormatForField(String field) {
         return format;
@@ -700,7 +700,7 @@
     if (LuceneTestCase.VERBOSE) {
       System.out.println("forcing docvalues format to:" + format);
     }
-    return new Lucene49Codec() {
+    return new Lucene410Codec() {
       @Override
       public DocValuesFormat getDocValuesFormatForField(String field) {
         return format;
Index: lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
===================================================================
--- lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(revision 1608975)
+++ lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat	(working copy)
@@ -16,9 +16,9 @@
 org.apache.lucene.codecs.mockrandom.MockRandomPostingsFormat
 org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat
 org.apache.lucene.codecs.ramonly.RAMOnlyPostingsFormat
-org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds
-org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval
-org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval
-org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings
+org.apache.lucene.codecs.lucene410ords.Lucene410WithOrds
+org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapFixedInterval
+org.apache.lucene.codecs.lucene410vargap.Lucene410VarGapDocFreqInterval
+org.apache.lucene.codecs.bloom.TestBloomFilteredLucene410Postings
 org.apache.lucene.codecs.asserting.AssertingPostingsFormat
 org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat
Index: solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java	(revision 1608975)
+++ solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java	(working copy)
@@ -3,7 +3,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.util.plugin.SolrCoreAware;
@@ -51,7 +51,7 @@
   @Override
   public void init(NamedList args) {
     super.init(args);
-    codec = new Lucene49Codec() {
+    codec = new Lucene410Codec() {
       @Override
       public PostingsFormat getPostingsFormatForField(String field) {
         final SchemaField schemaField = core.getLatestSchema().getFieldOrNull(field);
Index: solr/core/src/test/org/apache/solr/core/TestCodecSupport.java
===================================================================
--- solr/core/src/test/org/apache/solr/core/TestCodecSupport.java	(revision 1608975)
+++ solr/core/src/test/org/apache/solr/core/TestCodecSupport.java	(working copy)
@@ -38,14 +38,14 @@
     Map<String, SchemaField> fields = h.getCore().getLatestSchema().getFields();
     SchemaField schemaField = fields.get("string_pulsing_f");
     PerFieldPostingsFormat format = (PerFieldPostingsFormat) codec.postingsFormat();
-    assertEquals("Pulsing41", format.getPostingsFormatForField(schemaField.getName()).getName());
+    assertEquals("Pulsing410", format.getPostingsFormatForField(schemaField.getName()).getName());
     schemaField = fields.get("string_simpletext_f");
     assertEquals("SimpleText",
         format.getPostingsFormatForField(schemaField.getName()).getName());
     schemaField = fields.get("string_standard_f");
-    assertEquals("Lucene41", format.getPostingsFormatForField(schemaField.getName()).getName());
+    assertEquals("Lucene410", format.getPostingsFormatForField(schemaField.getName()).getName());
     schemaField = fields.get("string_f");
-    assertEquals("Lucene41", format.getPostingsFormatForField(schemaField.getName()).getName());
+    assertEquals("Lucene410", format.getPostingsFormatForField(schemaField.getName()).getName());
   }
 
   public void testDocValuesFormats() {
@@ -68,10 +68,10 @@
 
     assertEquals("SimpleText", format.getPostingsFormatForField("foo_simple").getName());
     assertEquals("SimpleText", format.getPostingsFormatForField("bar_simple").getName());
-    assertEquals("Pulsing41", format.getPostingsFormatForField("foo_pulsing").getName());
-    assertEquals("Pulsing41", format.getPostingsFormatForField("bar_pulsing").getName());
-    assertEquals("Lucene41", format.getPostingsFormatForField("foo_standard").getName());
-    assertEquals("Lucene41", format.getPostingsFormatForField("bar_standard").getName());
+    assertEquals("Pulsing410", format.getPostingsFormatForField("foo_pulsing").getName());
+    assertEquals("Pulsing410", format.getPostingsFormatForField("bar_pulsing").getName());
+    assertEquals("Lucene410", format.getPostingsFormatForField("foo_standard").getName());
+    assertEquals("Lucene410", format.getPostingsFormatForField("bar_standard").getName());
   }
 
   public void testDynamicFieldsDocValuesFormats() {
Index: solr/core/src/test-files/solr/collection1/conf/schema_codec.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/schema_codec.xml	(revision 1608975)
+++ solr/core/src/test-files/solr/collection1/conf/schema_codec.xml	(working copy)
@@ -17,9 +17,9 @@
 -->
 <schema name="codec" version="1.2">
  <types>
-  <fieldType name="string_pulsing" class="solr.StrField" postingsFormat="Pulsing41"/>
+  <fieldType name="string_pulsing" class="solr.StrField" postingsFormat="Pulsing410"/>
   <fieldType name="string_simpletext" class="solr.StrField" postingsFormat="SimpleText"/>
-  <fieldType name="string_standard" class="solr.StrField" postingsFormat="Lucene41"/>
+  <fieldType name="string_standard" class="solr.StrField" postingsFormat="Lucene410"/>
 
   <fieldType name="string_disk" class="solr.StrField" docValuesFormat="Lucene49" />
   <fieldType name="string_memory" class="solr.StrField" docValuesFormat="Memory" />
